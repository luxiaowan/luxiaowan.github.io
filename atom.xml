<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>串一串</title>
  
  <subtitle>断舍离</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://luxiaowan.github.io/"/>
  <updated>2020-04-22T16:09:55.197Z</updated>
  <id>http://luxiaowan.github.io/</id>
  
  <author>
    <name>cc</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>说说SpringBoot是如何实现自动装配的</title>
    <link href="http://luxiaowan.github.io/2020/04/21/%E8%AF%B4%E8%AF%B4SpringBoot%E6%98%AF%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D%E7%9A%84/"/>
    <id>http://luxiaowan.github.io/2020/04/21/说说SpringBoot是如何实现自动装配的/</id>
    <published>2020-04-21T12:02:00.000Z</published>
    <updated>2020-04-22T16:09:55.197Z</updated>
    
    <content type="html"><![CDATA[<p>Spring Boot是Spring家族中的新宠，它不仅继承了Spring框架原有的优秀特性，还通过简化配置来进一步简化Spring应用程序的创建和开发过程。SpringBoot框架中有两个最主要的策略：开箱即用和约定优于配置。</p><ul><li>开箱即用：在开发过程中，通过引入maven依赖包，然后使用注解来代替繁琐的XML配置文件来管理对象的生命周期，这让开发人员摆脱了复杂的配置和包依赖管理的工作，更加专注于业务逻辑。</li><li>约定优于配置：按约定编程是一种软件设计范式，系统、类库、框架应该假定合理的默认值，而非要求提供不必要的配置，从而既能获得配置简单的好处，而又不失灵活性。</li></ul><p>有关SpringBoot的概念就不说太多了，可以查看一下官方文档。</p><h3 id="自动配置"><a class="markdownIt-Anchor" href="#自动配置"></a> 自动配置</h3><p>SpringBoot的自动配置乍一看很神奇，其实原理非常简单，实现自动配置的核心就是@Conditional注解。</p><h4 id="一-condition是什么"><a class="markdownIt-Anchor" href="#一-condition是什么"></a> 一、@Condition是什么</h4><p>@Condition是Spring4的一个新特性，注解的注释第一句写到“表明仅当所有组件都符合注册条件时，该组件才具有注册资格”，所以我们可以根据这个注解动态的决定需要加载的Bean。</p><p>例如我们想要根据不同的环境加载不同的类，我们可以通过<code>spring.profiles.active=dev</code>指定当前环境，创建类的代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AppConfig</span> </span>&#123;</span><br><span class="line"><span class="meta">@Bean</span></span><br><span class="line"><span class="meta">@Profile</span>(<span class="string">"dev"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> PropConfig <span class="title">devDataSource</span><span class="params">()</span> </span>&#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Bean</span></span><br><span class="line"><span class="meta">@Profile</span>(<span class="string">"pre"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> PropConfig <span class="title">preDataSource</span><span class="params">()</span> </span>&#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Bean</span></span><br><span class="line"><span class="meta">@Profile</span>(<span class="string">"prd"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> PropConfig <span class="title">prdDataSource</span><span class="params">()</span> </span>&#123;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们这里用到了@Profile注解，这个注解是spring3.1之后版本中提供的，从注解的定义上我们可以看到它也是一个Conditional，可以通过<code>ConfigurableEnvironment#setActiveProfiles</code>方法和<code>spring.profiles.active</code>配置完成设置，当然还有其他方法，这里就不一一写出了，详情可以查看类<code>org.springframework.core.env.AbstractEnvironment</code>。</p><p>在业务复杂的情况下，可以使用@Conditional注解来提供更加灵活的条件判断，在SpringBoot中的的很多CccConfiguration类上都设置了很多的Conditional，整理后发现大致有以下几种：</p><ol><li><code>@ConditionalOnClass</code>：当classpath下存在指定的类时，加载被注解的类，使用方法<code>@ConditionalOnClass({A.class, B.class, C.class})</code></li><li><code>@ConditionalOnBean</code>：当Spring容器中存在指定的Bean实例时，加载被注解的类，使用方法<code>@ConditionalOnBean({A.class, B.class})</code></li><li><code>@ConditionalOnMissingBean</code>：当Spring容器中不存在指定的Bean实例时，加载被注解的类，使用方法<code>@ConditionalOnMissingBean({A.class, B.class, C.class})</code></li><li><code>@ConditionalOnMissingClass</code>：当classpath下不存在指定的类时，加载被注解的类，使用方法<code>@ConditionalOnMissingClass({&quot;cc.lu.A&quot;, &quot;cc.lu.B&quot;})</code></li><li><code>@ConditionalOnProperty</code>：控制某个configuration是否生效。具体操作是通过其两个属性name以及havingValue来实现的，其中name用来从application.properties中读取某个属性值，如果该值为空，则返回false;如果值不为空，则将该值与havingValue指定的值进行比较，如果一样则返回true;否则返回false。如果返回值为false，则该configuration不生效；为true则生效，使用方法<code>@ConditionalOnProperty(prefix=&quot;cc.lu.config&quot;, name=&quot;enable&quot;, havingValue=&quot;true&quot;)</code>，上面的<code>@Profile(&quot;dev&quot;)</code>对等于<code>@ConditionalOnProperty(name=&quot;spring.profiles.active&quot;, havingValue=&quot;dev&quot;)</code></li></ol><p>还有很多常用到的注解，可以到<code>org.springframework.autoconfigure.condition</code>包内了解一下，每一个注解单独拿出来都可以讨论半天。下面我们写一个简单的例子：当classpath路径中存在<code>cc.lu.A</code>类、容器中不存在<code>cc.lu.B</code>类且存在配置<code>cc.lu.config.auto=true</code>时加载<code>cc.lu.Cc</code>类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cc.lu;</span><br><span class="line"></span><br><span class="line"><span class="meta">@ConditionalOnClass</span>(A.class)</span><br><span class="line"><span class="meta">@ConditionalOnMissingBean</span>(B.class)</span><br><span class="line"><span class="meta">@ConditionalOnProperty</span>(prefix=<span class="string">"cc.lu.config"</span>, name=<span class="string">"auto"</span>, havingValue=<span class="string">"true"</span>, matchIfMissing=<span class="keyword">false</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Cc</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">Cc</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 在构造器中打印一句话来校验构造器是否被调用</span></span><br><span class="line">    System.out.println(<span class="string">"Cc init......"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="二-cccautoconfiguration分析"><a class="markdownIt-Anchor" href="#二-cccautoconfiguration分析"></a> 二、CccAutoConfiguration分析</h4><p>上面了解了<code>@Conditional</code>注解的机制，也写了一个简单的例子，灵的同学应该已经能猜到SpringBoot是如何来实现自动配置的了，我们现在基于2.2.6版本的源码来粗略的看一下。</p><p>我们创建一个SpringBoot工程，idea可以通过Spring Initializr来快速的创建一个项目，然后我们看整个工程的入口，也就是Application.java(新创建的SpringBoot应用一般只有一个启动类)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CcApplication</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SpringApplication.run(CcApplication.class, args);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>既然过程只有这么一个类，那么关键点就是<code>@SpringBootApplication</code>和<code>SpringApplication#run</code>了，我们先来看下注解<code>@SpringBootApplication</code>，这玩意儿放在启动类上是想要干啥。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Target</span>(ElementType.TYPE)</span><br><span class="line"><span class="meta">@Retention</span>(RetentionPolicy.RUNTIME)</span><br><span class="line"><span class="meta">@Documented</span></span><br><span class="line"><span class="meta">@Inherited</span></span><br><span class="line"><span class="meta">@SpringBootConfiguration</span></span><br><span class="line"><span class="meta">@EnableAutoConfiguration</span></span><br><span class="line"><span class="meta">@ComponentScan</span>(excludeFilters = &#123; <span class="meta">@Filter</span>(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class),</span><br><span class="line"><span class="meta">@Filter</span>(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) &#125;)</span><br><span class="line"><span class="keyword">public</span> <span class="meta">@interface</span> SpringBootApplication &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意到这个注解上有一个<code>@EnableAutoConfiguration</code>，这个注解的目的是启用Spring应用程序上下文的自动配置，尝试猜测和配置可能需要的bean。再来瞜一眼这个注解的定义</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Target</span>(ElementType.TYPE)</span><br><span class="line"><span class="meta">@Retention</span>(RetentionPolicy.RUNTIME)</span><br><span class="line"><span class="meta">@Documented</span></span><br><span class="line"><span class="meta">@Inherited</span></span><br><span class="line"><span class="meta">@AutoConfigurationPackage</span></span><br><span class="line"><span class="meta">@Import</span>(AutoConfigurationImportSelector.class)</span><br><span class="line"><span class="keyword">public</span> <span class="meta">@interface</span> EnableAutoConfiguration &#123;</span><br><span class="line">String ENABLED_OVERRIDE_PROPERTY = <span class="string">"spring.boot.enableautoconfiguration"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>哎，这个<code>@Import</code>注解把<code>AutoConfigurationImportSelector</code>这个类导入了进来，也就是说我们使用<code>@EnableAutoConfiguration</code>的时候，<code>AutoConfigurationImportSelector</code>类会自动被加载，那么是不是核心代码就是在这个类中了？(这样写貌似有点尬！)</p><p><code>AutoConfigurationImportSelector</code>实现于<code>ImportSelector</code>，关键方法就是<code>ImportSelector#selectImports</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> String[] selectImports(AnnotationMetadata annotationMetadata) &#123;</span><br><span class="line">  <span class="keyword">if</span> (!isEnabled(annotationMetadata)) &#123;</span><br><span class="line">    <span class="keyword">return</span> NO_IMPORTS;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 加载META-INF/additional-spring-configuration-metadata.json文件，将配置信息加载到环境中，这里就是为什么配置有默认值的关键，可用到jar包里面查看一下</span></span><br><span class="line">  AutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader</span><br><span class="line">    .loadMetadata(<span class="keyword">this</span>.beanClassLoader);</span><br><span class="line">  <span class="comment">// 加载META-INF/spring.factories文件，并将org.springframework.boot.autoconfigure.EnableAutoConfiguration的值以列表的形式返回</span></span><br><span class="line">  AutoConfigurationEntry autoConfigurationEntry = getAutoConfigurationEntry(autoConfigurationMetadata,annotationMetadata);</span><br><span class="line">  <span class="comment">// 将需要加载的AutoConfiguration返回</span></span><br><span class="line">  <span class="keyword">return</span> StringUtils.toStringArray(autoConfigurationEntry.getConfigurations());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 校验是否开启了自动配置</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">isEnabled</span><span class="params">(AnnotationMetadata metadata)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (getClass() == AutoConfigurationImportSelector.class) &#123;</span><br><span class="line">    <span class="keyword">return</span> getEnvironment().getProperty(EnableAutoConfiguration.ENABLED_OVERRIDE_PROPERTY, Boolean.class, <span class="keyword">true</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从<code>selectImports</code>的源码可以看到它只做了两件事：加载默认的配置属性和返回所有的AutoConfiguration的类信息，通过方法调用链找到最终加载的方法是<code>SpringFactoriesLoader#loadSpringFactories</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(<span class="meta">@Nullable</span> ClassLoader classLoader) &#123;</span><br><span class="line">  MultiValueMap&lt;String, String&gt; result = cache.get(classLoader);</span><br><span class="line">  <span class="keyword">if</span> (result != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 获取classpath下所有的META-INF/spring.factories文件</span></span><br><span class="line">    Enumeration&lt;URL&gt; urls = (classLoader != <span class="keyword">null</span> ?classLoader.getResources(FACTORIES_RESOURCE_LOCATION):ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION));</span><br><span class="line">    result = <span class="keyword">new</span> LinkedMultiValueMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">while</span> (urls.hasMoreElements()) &#123;</span><br><span class="line">      URL url = urls.nextElement();</span><br><span class="line">      UrlResource resource = <span class="keyword">new</span> UrlResource(url);</span><br><span class="line">      <span class="comment">// 加载文件内容</span></span><br><span class="line">      Properties properties = PropertiesLoaderUtils.loadProperties(resource);</span><br><span class="line">      <span class="comment">// 将文件内容存在到Map中</span></span><br><span class="line">      <span class="keyword">for</span> (Map.Entry&lt;?, ?&gt; entry : properties.entrySet()) &#123;</span><br><span class="line">        String factoryTypeName = ((String) entry.getKey()).trim();</span><br><span class="line">        <span class="comment">// value是使用逗号分隔的，所以这里转换成数组，也就是把一碗米饭分成一粒粒的</span></span><br><span class="line">        <span class="keyword">for</span> (String factoryImplementationName : StringUtils.commaDelimitedListToStringArray((String) entry.getValue())) &#123;</span><br><span class="line">          result.add(factoryTypeName, factoryImplementationName.trim());</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cache.put(classLoader, result);</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">catch</span> (IOException ex) &#123;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>SpringBoot为我们提供的配置类有一二百个，但是我们不可能每个工程都把它们全部引入。所以在自动装配的时候，会去classpath下面寻找，是否有对应的配置类。如果有配置类，则按条件注解 @Conditional或者@ConditionalOnProperty等相关注解进行判断，决定是否需要装配。如果classpath下面没有对应的字节码，则不进行任何处理。</p><p>我们到spring.factories文件中随便找一个AutoConfiguration类，比如<code>org.springframework.boot.autoconfigure.data.redis.RedisAutoConfiguration</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Configuration</span>(proxyBeanMethods = <span class="keyword">false</span>)</span><br><span class="line"><span class="meta">@ConditionalOnClass</span>(RedisOperations.class)</span><br><span class="line"><span class="meta">@EnableConfigurationProperties</span>(RedisProperties.class)</span><br><span class="line"><span class="meta">@Import</span>(&#123; LettuceConnectionConfiguration.class, JedisConnectionConfiguration.class &#125;)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RedisAutoConfiguration</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Bean</span></span><br><span class="line"><span class="meta">@ConditionalOnMissingBean</span>(name = <span class="string">"redisTemplate"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> RedisTemplate&lt;Object, Object&gt; <span class="title">redisTemplate</span><span class="params">(RedisConnectionFactory redisConnectionFactory)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> UnknownHostException </span>&#123;</span><br><span class="line">RedisTemplate&lt;Object, Object&gt; template = <span class="keyword">new</span> RedisTemplate&lt;&gt;();</span><br><span class="line">template.setConnectionFactory(redisConnectionFactory);</span><br><span class="line"><span class="keyword">return</span> template;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Bean</span></span><br><span class="line"><span class="meta">@ConditionalOnMissingBean</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StringRedisTemplate <span class="title">stringRedisTemplate</span><span class="params">(RedisConnectionFactory redisConnectionFactory)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> UnknownHostException </span>&#123;</span><br><span class="line">StringRedisTemplate template = <span class="keyword">new</span> StringRedisTemplate();</span><br><span class="line">template.setConnectionFactory(redisConnectionFactory);</span><br><span class="line"><span class="keyword">return</span> template;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个类被<code>@ConditionalOnClass</code>和<code>@EnableConfigurationProperties</code>两个注解修饰。</p><ul><li><code>@ConditionalOnClass(RedisOperations.class)</code>的意思是当classpath路径下存在<code>RedisOperations</code>这个类的时候加载<code>RedisAutoConfiguration</code>，类<code>RedisOperations</code>在spring-data-redis.jar包中，这个包通过<code>spring-boot-starter-data-redis</code>的starter引入，所以在我们引入这个starter的时候就自动去加载了RedisAutoConfiguration，然后再类中又创建了两个Bean，创建的前提是容器中不存在这两个类的实例，如果我们自定义一个RedisTemplate的实例，RedisAutoConfiguration#redisTemplate方法就会失效。</li><li><code>@EnableConfigurationProperties(RedisProperties.class)</code>：在加载RedisAutoConfiguration的时候同步加载RedisProperties，RedisProperties中通过注解<code>@ConfigurationProperties(prefix = &quot;spring.redis&quot;)</code>指定关联的配置信息，若没有配置则使用类中属性的默认值。</li></ul><p>至此，容器中有了RedisTemplate的实例和StringRedisTemplate的实例，并且还使用了配置文件中我们设置的Redis相关配置。</p><h3 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h3><p>整个SpringBoot中，都是通过@Conditional注解的各种扩展来实现自动配置的，我们也可以完全利用这些注解去实现我们自己的starter。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Spring Boot是Spring家族中的新宠，它不仅继承了Spring框架原有的优秀特性，还通过简化配置来进一步简化Spring应用程序的创建和开发过程。SpringBoot框架中有两个最主要的策略：开箱即用和约定优于配置。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;开箱即用：在开发过
      
    
    </summary>
    
    
      <category term="SpringBoot" scheme="http://luxiaowan.github.io/categories/SpringBoot/"/>
    
    
  </entry>
  
  <entry>
    <title>服务监控之SpringBoot Admin</title>
    <link href="http://luxiaowan.github.io/2020/04/21/%E6%9C%8D%E5%8A%A1%E7%9B%91%E6%8E%A7%E4%B9%8BSpringBoot-Admin/"/>
    <id>http://luxiaowan.github.io/2020/04/21/服务监控之SpringBoot-Admin/</id>
    <published>2020-04-21T07:28:00.000Z</published>
    <updated>2020-04-21T09:16:12.509Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简介"><a class="markdownIt-Anchor" href="#简介"></a> 简介</h3><p>Spring Boot Admin是一个管理和健康SpringBoot应用的应用，有点绕口，其实就是用来监控SpringBoot应用的，这些应用可以通过Spring Boot Admin Client或Spring Cloud自动发现的方式注册到Admin Server。</p><h3 id="一-admin-client方式注册"><a class="markdownIt-Anchor" href="#一-admin-client方式注册"></a> 一、Admin Client方式注册</h3><h4 id="admin-server搭建"><a class="markdownIt-Anchor" href="#admin-server搭建"></a> Admin Server搭建</h4><ul><li><p>引入server依赖的包</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>de.codecentric<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-admin-starter-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>启动类加上<code>@EnableAdminServer</code>注解</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.springframework.boot.SpringApplication;</span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.autoconfigure.SpringBootApplication;</span><br><span class="line"><span class="keyword">import</span> de.codecentric.boot.admin.server.config.EnableAdminServer;</span><br><span class="line"></span><br><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="meta">@EnableAdminServer</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SpringBootAdminApplication</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SpringApplication.run(SpringBootAdminApplication.class, args);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p>启动项目，然后访问<code>http://localhost:8080/</code>，到此Admin Server就搭建好了，跟Eureka Server似的，一包一注释，仗势走天涯。</p><p><img src="/images/image-20200421161507822.png" alt="image-20200421161507822"></p><h4 id="admin-client注册"><a class="markdownIt-Anchor" href="#admin-client注册"></a> Admin Client注册</h4><ul><li><p>引入client依赖包</p><p>Spring Boot Admin是使用actuator实现的服务监控，所以在client应用中需要引入actuator的包，并开放相关的接口，否则监控的信息不完整。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>de.codecentric<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-admin-starter-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-actuator<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>配置</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">server:</span></span><br><span class="line"><span class="attr">  port:</span> <span class="number">8888</span></span><br><span class="line"><span class="attr">spring:</span></span><br><span class="line"><span class="attr">  application:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">boot-log</span></span><br><span class="line"><span class="attr">  boot:</span></span><br><span class="line"><span class="attr">    admin:</span></span><br><span class="line"><span class="attr">      client:</span></span><br><span class="line"><span class="attr">        url:</span> <span class="attr">http://127.0.0.1:8080</span> <span class="comment"># 指定admin-server注册地址，和Eureka更像了</span></span><br><span class="line"><span class="attr">management:</span></span><br><span class="line"><span class="attr">  endpoints:</span></span><br><span class="line"><span class="attr">    web:</span></span><br><span class="line"><span class="attr">      exposure:</span></span><br><span class="line"><span class="attr">        include:</span> <span class="string">"*"</span> <span class="comment"># 暴露actuator所有接口</span></span><br></pre></td></tr></table></figure></li></ul><p>至此就完成了Admin Client端的配置，可以启动服务了，查看admin页面</p><p><img src="/images/image-20200421163624584.png" alt="image-20200421163624584"></p><h3 id="二-eureka注册中心自动发现"><a class="markdownIt-Anchor" href="#二-eureka注册中心自动发现"></a> 二、Eureka注册中心自动发现</h3><p>使用Eureka可以解放Client端的配置，不再需要给Client端配置任何东西(除actuator暴露接口列表)</p><h4 id="eureka-server搭建"><a class="markdownIt-Anchor" href="#eureka-server搭建"></a> Eureka Server搭建</h4><p>搭建Eureka Server就不做特殊解释了，直接贴配置代码。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.cloud<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-cloud-starter-netflix-eureka-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">server:</span></span><br><span class="line"><span class="attr">  port:</span> <span class="number">9001</span> <span class="comment">#服务端口</span></span><br><span class="line"><span class="attr">eureka:</span></span><br><span class="line"><span class="attr">  instance:</span></span><br><span class="line"><span class="attr">    hostname:</span> <span class="string">localhost</span></span><br><span class="line"><span class="attr">  client:</span></span><br><span class="line"><span class="attr">    register-with-eureka:</span> <span class="literal">false</span> <span class="comment">#是否将eureka自身作为应用注册到eureka注册中心</span></span><br><span class="line"><span class="attr">    fetch-registry:</span> <span class="literal">false</span> <span class="comment">#为true时，可以启动，但报异常：Cannot execute request on any known server</span></span><br><span class="line"><span class="attr">    serviceUrl:</span></span><br><span class="line"><span class="attr">      defaultZone:</span> <span class="attr">http://localhost:9001/eureka/</span></span><br><span class="line"><span class="attr">  server:</span></span><br><span class="line"><span class="attr">    enable-self-preservation:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.springframework.boot.SpringApplication;</span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.autoconfigure.SpringBootApplication;</span><br><span class="line"><span class="keyword">import</span> org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;</span><br><span class="line"></span><br><span class="line"><span class="meta">@EnableEurekaServer</span></span><br><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SpringCloudEurekaApplication</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SpringApplication.run(SpringCloudEurekaApplication.class, args);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>启动之后访问<code>http://127.0.0.1:9001/</code>进入Eureka控制台</p><h4 id="admin-server搭建-2"><a class="markdownIt-Anchor" href="#admin-server搭建-2"></a> Admin Server搭建</h4><p>Admin Server的搭建方式和第一种相差无几，唯一的差别就是需要把Server应用注册到Eureka中</p><ul><li>在启动类加上<code>@EnableEurekaClient</code>注解</li><li>在配置文件中加上<code>eureka.client.service-url.defaultZone=http://127.0.0.1:9001/eureka</code>配置</li></ul><h4 id="admin-client搭建"><a class="markdownIt-Anchor" href="#admin-client搭建"></a> Admin Client搭建</h4><p>Admin Client正常配置注册到Eureka，然后暴露actuator的相关接口<code>management.endpoints.web.exposure.include=*</code>。</p><p>启动client，查看Eureka和Admin控制台</p><ul><li><p>Eureka Server</p><p><img src="/images/image-20200421171038407.png" alt="image-20200421171038407"></p></li><li><p>Admin Server</p><p><img src="/images/image-20200421171100096.png" alt="image-20200421171100096"></p></li></ul><h3 id="三-spring-boot-admin提供了哪些功能"><a class="markdownIt-Anchor" href="#三-spring-boot-admin提供了哪些功能"></a> 三、Spring Boot Admin提供了哪些功能</h3><p><img src="/images/image-20200421171313326.png" alt="image-20200421171313326"></p><p>就不一一赘述了，搭建了玩玩就都明白了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;简介&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#简介&quot;&gt;&lt;/a&gt; 简介&lt;/h3&gt;
&lt;p&gt;Spring Boot Admin是一个管理和健康SpringBoot应用的应用，有点绕口，其实就是用来监控SpringBoot应用的，这些应用
      
    
    </summary>
    
    
      <category term="分布式" scheme="http://luxiaowan.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    
  </entry>
  
  <entry>
    <title>简单的看一下服务治理是什么</title>
    <link href="http://luxiaowan.github.io/2020/04/20/%E7%AE%80%E5%8D%95%E7%9A%84%E7%9C%8B%E4%B8%80%E4%B8%8B%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86%E6%98%AF%E4%BB%80%E4%B9%88/"/>
    <id>http://luxiaowan.github.io/2020/04/20/简单的看一下服务治理是什么/</id>
    <published>2020-04-20T14:35:00.000Z</published>
    <updated>2020-04-21T07:22:59.739Z</updated>
    
    <content type="html"><![CDATA[<h3 id="引文"><a class="markdownIt-Anchor" href="#引文"></a> 引文</h3><p>开始看文章之前不妨先思考几个问题：服务治理是什么，什么样子的服务需要治理，为什么需要治理服务，应该怎样治理服务，治理服务的哪些方面，不治理的话服务会怎样？这些问题基本就是服务治理的关键思想了，搞懂这些问题，就相当于是掌握了服务治理的基本概念。</p><h3 id="服务演变过程"><a class="markdownIt-Anchor" href="#服务演变过程"></a> 服务演变过程</h3><p>我们在解决上述问题之前，先来看一下服务架构的发展过程。</p><h4 id="单体服务"><a class="markdownIt-Anchor" href="#单体服务"></a> 单体服务</h4><p>对于单体服务来说，应用结构一般比较简单，一般不需要特别的服务治理手段，但随着服务承载的业务越来越庞大，服务内部逻辑变得复杂起来，扩展性也越来越差，这时最好的治理办法就是将其进行拆分，除此之外的治理手段都是徒劳，反而会让应用变得越来越臃肿。</p><h4 id="集群服务"><a class="markdownIt-Anchor" href="#集群服务"></a> 集群服务</h4><p>单体架构在流量较少时能够满足基本需求，随着业务的发展，产品流量越来越多，一秒钟的服务宕机都可能造成很大的损失，基于此，将由单体架构延伸到集群架构。集群中每台服务器都提供相同的服务，使用负载均衡器来实现每台服务器的负载分配，这其实也能算是一种治理，在分配请求时需要选择出最适合的健康的服务器，将请求发送过去，并且要时刻的检查集群中节点的健康状态，及时将故障节点从健康服务列表中剔除。</p><h4 id="分布式服务"><a class="markdownIt-Anchor" href="#分布式服务"></a> 分布式服务</h4><p>当单体应用变得臃肿之后，整个团队维护同一套代码，所有的业务都集中在一个应用中，服务的健壮性会随着业务的发展变得越来越脆弱，当维护一个服务变得困难的时候，就需要考虑将此服务拆分成若干个小而美的服务了，即微服务化，将单一服务架构向微服务架构演进。</p><p>当服务演进到微服务架构之后，会出现新的问题，比如之前所有的业务逻辑都在一个进程中执行，日志查看、问题排查等都很方便，也不存在进程之间的通信，更不存在依赖服务状态的监控等情况。那么既然拆分成微服务之后会遇到这么多问题，为什么还要拆呢？其实大部分由单体服务拆分成微服务的初衷都是想分而治之，也就是单一职责的服务更方便优化和维护。</p><ul><li><p>服务定义：服务拆分要根据一定的规则进行，定义拆分后每个服务的业务范围和边界，不能为了拆而拆，微服务的提出者Matin Fowler在首提微服务的时候是以业务为拆分基准，后续发展过程中，越来越多的条件都可以作为拆分的理由了，比如业务、流量大小等。</p></li><li><p>服务注册与发现：微服务架构体系中，必定会出现服务之间的调用依赖，这就需要调用方知道目标服务的地址，从而发起请求。如果将服务端的地址预先告知调用方，其实也是可行的，缺陷就是当服务越来越多时，服务URL的配置管理变得非常困难，所以在这个过程中我们需要借助一个中介来提供一个接口让我们可以随时获取健康的服务列表，所以服务注册中心应运而生(Zookeeper、Etcd、Consul、Eureka、Dubbo等)，它的理念是由服务端将可提供的服务注册到服务中心，并由服务中心来维护每个服务节点的健康状态，客户端从服务中心获取健康的服务列表用于发送请求。</p></li><li><p>接口调用链监控：微服务架构体系中以服务多著称，服务都具有单一职责，服务之间需要互相调用才能完成一次请求的处理。在单体应用架构中，调用链都是在同一个进程中，只要通过线程ID或者MDC即可查询到请求的整个调用链，定位很轻松；但是在分布式架构中，想要跟踪一次请求的整个调用链就比较麻烦了，一般需要通过第三方组件来完成，比如Twitter的Zipkin。</p></li><li><p>应用服务监控：SpringBootAdmin提供了一套开箱即用的监控工具箱，可以看到监控的每个微服务实例的运行情况，具体操作稍后再嗷嗷</p></li><li><p>负载均衡：在服务拆分设计过程中，每一个服务都需要保证是高可用的，也就是每一个微服务又都是高可用的。</p></li><li><p>服务降级熔断：单体服务拆分后导致服务变多了，调用链出故障的概率就更大了，我们在保证了服务的高可用的同时，仍然需要保证接口的可用性，如果接口出现故障，要能够将故障快速转移并响应调用端，否则可能会造成大量的请求积压，进而拖垮整个系统。断路器和服务降级为接口的快速故障转移提供了保障。</p></li><li><p>服务调用：在分布式架构中，服务之间可以通过http、rpc等方式互相调用，比如Feign是一种http方式的声明式调用，在SpringCloud中有广泛的使用。</p></li><li><p>服务安全：服务自身需要保证接口、数据和服务的安全性，不能出现被随意调用的现象。</p></li><li><p>服务版本：服务在升级过程中，需要考虑到对老版本的兼容性，避免造成因升级带来的系统故障。</p></li></ul><h3 id="回看"><a class="markdownIt-Anchor" href="#回看"></a> 回看</h3><p>简单的描述了一下服务架构的演进路线，我们来看看前面说的几个问题：</p><ol><li><p>服务治理是什么</p><blockquote><p>服务治理是通过一系列手段来更好的管理服务，确保系统能够顺利、安全、稳定的运行。</p></blockquote></li><li><p>什么样子的服务需要治理</p><blockquote><p>任何服务都需要治理</p></blockquote></li><li><p>为什么需要治理服务</p><blockquote><ol><li>为了提升系统的稳定性</li><li>保证服务的可用性</li></ol></blockquote></li><li><p>应该怎样治理服务</p><blockquote><ol><li>建立授权的责任链</li><li>评估服务的有效性</li><li>服务监控告警</li><li>故障转移</li></ol></blockquote></li><li><p>治理服务的哪些方面</p><blockquote><ol><li>服务定义</li><li>服务注册与发现</li><li>接口调用链</li><li>应用服务监控</li><li>负载均衡</li><li>服务降级熔断</li><li>服务调用协议</li><li>服务安全</li><li>服务版本</li></ol></blockquote></li><li><p>不治理的话服务会怎样</p><blockquote><p>场景一：周末你正在逛街，然后老板在群里艾特你，说系统服务器宕机了、机房停电了、光缆被施工队挖断了，系统没办法使用了。这个时候你是不是需要立刻赶到公司去搬砖，直到系统恢复正常。</p><p>场景二：正在与周公下棋，然后一阵滴滴声，发现某个服务节点的故障告警邮件快撑爆收件箱了，并且还不断的有流量打到这个服务节点，如果没有治理的话，你要立刻起床打开电脑尽快恢复这个节点。有了服务治理，你打开服务管理器，一键熔断降级，搞定。然后可以起床吃个早饭再来修复了(呸~当然立刻修复，哪还有心思吃早饭)。</p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;引文&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#引文&quot;&gt;&lt;/a&gt; 引文&lt;/h3&gt;
&lt;p&gt;开始看文章之前不妨先思考几个问题：服务治理是什么，什么样子的服务需要治理，为什么需要治理服务，应该怎样治理服务，治理服务的哪些方面，不治理的话服
      
    
    </summary>
    
    
      <category term="分布式" scheme="http://luxiaowan.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    
  </entry>
  
  <entry>
    <title>聊一聊负载均衡</title>
    <link href="http://luxiaowan.github.io/2020/04/20/%E8%81%8A%E4%B8%80%E8%81%8A%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"/>
    <id>http://luxiaowan.github.io/2020/04/20/聊一聊负载均衡/</id>
    <published>2020-04-20T03:53:00.000Z</published>
    <updated>2020-04-20T10:58:55.067Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h3><p>负载均衡是实现服务高可用的一个关键性技术，在集群环境中，常常会将一个应用部署在多台服务器上同时提供服务，负载均衡将任务均衡的分配给不同的服务器，减少单一服务器的负载，达到水平扩容的目的，再者如果集群中某个节点的服务宕机了，负载均衡器会及时发现不可用的节点，并将其从集群服务节点中逻辑移除，此后的流量不会再转发到这台服务器上。</p><h3 id="单点服务"><a class="markdownIt-Anchor" href="#单点服务"></a> 单点服务</h3><p>没有使用负载均衡的服务架构一般如图：</p><img src="/images/image-20200420122319035.png" alt="image-20200420122319035" style="zoom:60%;"><p>如果Server出现故障导致服务不可用，则整个系统都会无法使用，这种故障的危害对于一个产品而言是非常巨大的。在微服务流行之前，一些中小型公司和传统IT公司大多采用单点部署方案。</p><p>既然单点服务无法保证故障的自动切换，所以为了避免单节点故障而导致的服务不可用，就需要将服务多节点部署，服务之间以主备的方式提供服务，这就延伸到了另外一个方案：多节点主备服务。</p><h3 id="多节点主备"><a class="markdownIt-Anchor" href="#多节点主备"></a> 多节点主备</h3><p>多节点主备不算是负载均衡的一种实现，这种方案只是为服务提供了一个备份，虽然服务部署在多台服务器上，但同时只有一台服务器提供服务，当正在提供服务不可用时，自动将请求切换到备份的服务器上。</p><img src="/images/image-20200420144949473.png" alt="image-20200420144949473" style="zoom:60%;"><p>多节点主备方案保证了服务的高可用，但并没有保证负载的均衡分配，由于同一时间只有一台机器提供服务，所有的流量全部都会透传到这一台服务器上，当流量激增的时候可能会很快的压垮这台服务器，然后不断的在主备机器之间来回切换，每隔一段时间压垮一台服务器。</p><p>我们通常解决这种情况的办法就是增加每台服务器的配置，也可以理解为F5负载均衡（压缩数据、连接聚合、页面缓存、浏览器缓存），但是这种方案的成本非常高，一般的中小企业都难以承担这个费用。所以智者又提出能否让多台服务器同时对外提供服务，然后根据一定的规则将流量分配到每一台机器上，这样既能保证服务的高可用，也能缓解每台服务器的压力，因此Nginx这类负载均衡组件应运而生。</p><h3 id="负载均衡策略"><a class="markdownIt-Anchor" href="#负载均衡策略"></a> 负载均衡策略</h3><p>既然多台服务器可以同时提供相同的服务，那么就需要指定响应的规则进行流量分配，并且要确保服务是可用的，不然请求过来之后不知道该往哪转发，所以在做负载均衡服务配置之前需要先确定均衡策略。</p><img src="/images/image-20200420151415778.png" alt="image-20200420151415778" style="zoom:50%;"><ul><li>轮询策略：轮询也就意味着服务器会被按顺序的选择，从1到N然后重新开始，比如由两台服务器，请求1分配给Server1，请求2分配给Server2，请求3分配给Server1…，所有的服务器都会被分配数量相同的流量。这种策略适合用于各服务器处理能力相同并且每个业务处理量差不多的情况。</li><li>随机策略：请求随机发送到各个节点，每台服务器处理的请求数量可能会有很大的差异。一般不使用随机策略。</li><li>最少连接策略：客户端的每次请求所消耗的时长可能会有很大差异，每台服务器上的连接线程可能会因此产生较大的不同，长此以往并不能达到真正的负载均衡。最少连接策略是让负载均衡器记录每台服务器正在处理的请求数，新的请求打过来之后会分配给当前正在处理请求数最少的那台机器，使每台服务器处理的请求数更加均衡。这种策略适合长时处理的请求服务。</li><li>权重策略：事先为每台服务器分配不同的权重，比如Server1和Server2分别设置为3和7，也就意味着Server2将承担70%的请求，Server1则承担30%的请求，保证性能更优的服务器能够承担更多的请求处理任务。权重策略适用于服务器性能不同的情况。</li><li>IP-Hash策略：负载均衡器根据请求来源的IP计算Hash值，然后决定分配给哪一台服务器，当用户IP和Hash计算方式不发生变化的情况下，他发出的所有请求最终都会落在一台机器上。该策略适用于想简单解决session问题的情况。</li></ul><h3 id="健康检查"><a class="markdownIt-Anchor" href="#健康检查"></a> 健康检查</h3><p>使用负载均衡的目的既然是提升服务的高可用，那么前提自然是要确保集群中的每个服务都是健康可用的，负载均衡器会通过健康检查的方式来识别服务是否可用。</p><p>常用的负载均衡器基本属Nginx莫属了，使用Nginx的好处是它自带健康检查模块<code>ngx_http_upstream_module</code>，可用做到最基本的健康检查。但是Nginx是被动的进行健康检查，也就是健康检查是依赖于请求的，如果服务1出现异常，则需要再将请求转发给服务2，直到遇到能够成功返回的接口，每一次的失败请求都会被记录下来，若失败节点达到了预设的最大次数，则将其从健康服务列表中移除，效率不高。</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">upstream</span> backend&#123;</span><br><span class="line">  <span class="comment"># max_fails表示失败次数，整体的意思是在fail_timeout时间内若失败次数达到了max_fails，则认为该节点的服务异常，移除健康服务列表</span></span><br><span class="line">    <span class="attribute">server</span> <span class="number">127.0.0.1:8080</span>  max_fails=<span class="number">1</span> fail_timeout=<span class="number">40s</span>;</span><br><span class="line">    <span class="attribute">server</span> <span class="number">127.0.0.1:8090</span>  max_fails=<span class="number">1</span> fail_timeout=<span class="number">40s</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">listen</span> <span class="number">80</span>;</span><br><span class="line">    <span class="attribute">server_name</span> d.kv.cc; </span><br><span class="line">    <span class="attribute">location</span> / &#123;</span><br><span class="line">      <span class="attribute">proxy_pass</span>http://backend;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="负载均衡session共享问题"><a class="markdownIt-Anchor" href="#负载均衡session共享问题"></a> 负载均衡session共享问题</h3><p>在使用IP-Hash做负载均衡策略时，因为同一个用户的请求都会转发到固定的一台机器上，所以在服务稳定的情况下，不用考虑session共享的问题，但是为了防止服务器突然宕机而引起的请求被转发到其他机器造成的session丢失的情况出现，所以不管哪一种策略，最好都做好session共享的解决方案。</p><ol><li>Redis：可以使用spring-session+Redis来实现session共享，很简单，请看<a href="https://luxiaowan.github.io/2019/10/24/Spring-Session%E5%92%8CRedis%E5%AE%9E%E7%8E%B0Session%E5%85%B1%E4%BA%AB/">Spring-Session和Redis实现Session共享</a></li><li>基于token认证：JWT认证</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前言&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#前言&quot;&gt;&lt;/a&gt; 前言&lt;/h3&gt;
&lt;p&gt;负载均衡是实现服务高可用的一个关键性技术，在集群环境中，常常会将一个应用部署在多台服务器上同时提供服务，负载均衡将任务均衡的分配给不同的服务器，
      
    
    </summary>
    
    
      <category term="分布式" scheme="http://luxiaowan.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    
  </entry>
  
  <entry>
    <title>软件开发中的幂等是什么</title>
    <link href="http://luxiaowan.github.io/2020/04/20/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%AD%E7%9A%84%E5%B9%82%E7%AD%89%E6%98%AF%E4%BB%80%E4%B9%88/"/>
    <id>http://luxiaowan.github.io/2020/04/20/软件开发中的幂等是什么/</id>
    <published>2020-04-19T22:06:00.000Z</published>
    <updated>2020-04-19T23:31:00.914Z</updated>
    
    <content type="html"><![CDATA[<p>工作中经常会遇到一个事务型接口或者一个事务型方法被调用方重复调用，实际上每次调用的数据都是相同的，有可能是因为网络延迟，客户端点击了多次，也可能是调用方故意而为之，不论是哪种情况，我们都不能让这种重复的操作对我们自己的系统造成不必要的影响。因此，我们需要对于接口和方法做幂等校验。其实幂等过滤一直都贯穿IT行业，只不过现在随着互联网行业的发展，行业内的造词能力和词语引用能力越来越强，幂等被计算机行业正式引用。</p><h3 id="幂等性"><a class="markdownIt-Anchor" href="#幂等性"></a> 幂等性</h3><p>在数学概念里，幂等是指一次变换和N次变换的结果都相同。而在计算机概念中，幂等是指某一操作执行n次和执行一次所产生的影响是一样的，也就是重复执行这项操作也不会对系统造成改变，比如抢优惠券，每人限制抢一张优惠券，可以使用用户ID+优惠券ID做幂等条件，这样就可以保证每个用户只能抢一张此优惠券。</p><h3 id="restful幂等"><a class="markdownIt-Anchor" href="#restful幂等"></a> RESTFul幂等</h3><ul><li>GET请求：GET请求属于是非事务型请求，每次请求都不会对系统造成任何影响，所以这类接口本身就符合了幂等性（这里说的是对系统造成影响，而不是说每次调用获取到的数据相同，幂等是针对于系统而非接口）</li><li>DELETE请求：DELETE请求是一个特殊的事务型请求，在调用第一次的时候就已经把数据删除了，所以不论后续再次调用多少次，对系统产生的影响都是一样的，所以DELETE类型的请求本身也是符合幂等性的</li><li>PUT请求：PUT请求是对数据进行修改，理论上对同一URI进行多次PUT操作对整个系统的影响和一次PUT是相同的，但是这要结合接口具体代码实现来考量<ul><li>直接将PUT的数值替换掉原有数据：幂等</li><li>每次调用都对某个数据递增：非幂等，需要手动幂等</li></ul></li><li>POST请求：POST是创建资源的事务型请求，每次请求都会创建一份资源，所以多次调用对整个系统产生的影响是不同的，该类型的请求是非幂等的</li></ul><h3 id="常用幂等方法"><a class="markdownIt-Anchor" href="#常用幂等方法"></a> 常用幂等方法</h3><ul><li><p>数据库唯一索引去重</p><p>利用数据库唯一索引的特性达到幂等的效果，不会往数据表中插入两条幂等字段相同的数据。例如文章点赞功能，为了防止用户重复点赞，可以在点赞表中以用户ID+文章ID为唯一索引，这样就可以有效的防止用户给一篇文章重复点赞，最终达到幂等操作的效果。</p></li><li><p>版本锁控制</p><p>版本锁属于是乐观锁的一种实现，MySQL中的MVCC就是版本锁的一种应用方式，通过每次更新都通过版本号控制权限来达到幂等操作的效果</p></li><li><p>token缓存机制</p><p>token缓存机制是一种比较常见的幂等处理方式，应用范围较广，不限定场景和语言。核心思想就是为每个请求分配一个全局唯一标识(token)，一个token在一条业务线的每个阶段只能执行一次，执行之后将这个阶段的结果缓存起来，每次执行前先校验是否已有缓存，没有缓存则执行流程，已有缓存则直接返回结果。</p></li></ul><h3 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h3><p>不同的业务需求会面临不同的幂等要求，也就会有不同的解决方案，所以幂等的实现难度是不同的。在做系统设计时，一定要将幂等性考虑周全，否则可能会给系统带来不必要的潜在隐患。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;工作中经常会遇到一个事务型接口或者一个事务型方法被调用方重复调用，实际上每次调用的数据都是相同的，有可能是因为网络延迟，客户端点击了多次，也可能是调用方故意而为之，不论是哪种情况，我们都不能让这种重复的操作对我们自己的系统造成不必要的影响。因此，我们需要对于接口和方法做幂等
      
    
    </summary>
    
    
      <category term="分布式" scheme="http://luxiaowan.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    
  </entry>
  
  <entry>
    <title>Git分支管理</title>
    <link href="http://luxiaowan.github.io/2020/04/20/Git%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86/"/>
    <id>http://luxiaowan.github.io/2020/04/20/Git分支管理/</id>
    <published>2020-04-19T18:18:00.000Z</published>
    <updated>2020-04-19T18:53:22.831Z</updated>
    
    <content type="html"><![CDATA[<p>目前最流行的Git分支管理规范应该属git-flow莫属了，它的核心概念是版本发布，适用于项目的持续集成和频繁发布，这也是近些年使用Git的过程中一直使用的规范。</p><p>git-flow流程中有5类分支，分别是master、develop、relaese(发布版本)、feature(新功能分支)、hotfix(修复bug版本)，这些分支都有各自的作用和生命周期，master是最稳定的分支，develop是一个新代码集成分支，所有发布后的代码都合并到develop，然后由develop合并到master，开发完成后在发布的时候创建release分支进行发布，在开发新功能的时候创建一个feature分支，生产上遇到需要修复的紧急bug时，创建一个hotfix分支。</p><p>分支名称规则：<code>release/feature/hotfix-分支创建日期-分支版本号</code></p><table><thead><tr><th style="text-align:left">分支类型</th><th style="text-align:left">命名规范</th><th style="text-align:left">创建自</th><th style="text-align:left">合并到</th><th style="text-align:left">说明</th></tr></thead><tbody><tr><td style="text-align:left">feature</td><td style="text-align:left">feature/*</td><td style="text-align:left">develop</td><td style="text-align:left">develop</td><td style="text-align:left">新功能</td></tr><tr><td style="text-align:left">release</td><td style="text-align:left">release/*</td><td style="text-align:left">develop</td><td style="text-align:left">develop 和 master</td><td style="text-align:left">一次新版本的发布</td></tr><tr><td style="text-align:left">hotfix</td><td style="text-align:left">hotfix/*</td><td style="text-align:left">master</td><td style="text-align:left">develop 和 master</td><td style="text-align:left">生产环境中发现的紧急 bug 的修复</td></tr></tbody></table><blockquote><p>Git分支关系图</p></blockquote><img src="/images/git-model.png" alt="img" style="zoom: 67%;"><ul><li><p>对于不同的开发任务，需要在不同的分支上完成开发，开发完成后合并到develop分支进行测试，基本流程如下：</p><ol><li><p>从develop分支创建一个feature分支：feature/cc-20200420-v1.0.1</p></li><li><p>在feature/cc分支上进行开发</p></li><li><p>开发完成后将feature/cc合并到develop分支</p></li></ol></li><li><p>在进行版本发布时的基本流程如下：</p><ol><li>从develop创建release分支：release/cc-20200420-v1.0.1</li><li>将release/cc分支发布到测试环境进行测试，测试中出现的bug直接在release/cc分支上修改并提交</li><li>测试完成后合并到develop和master，并在master上打一个Tag</li></ol></li><li><p>在进行生产bug紧急修复时的基本流程如下：</p><ol><li>从master创建hotfix分支：hotfix/cc-20200420-v1.0.1</li><li>验证完成后将hotfix/cc分支合并到develop和master，并在master上打一个Tag</li></ol></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;目前最流行的Git分支管理规范应该属git-flow莫属了，它的核心概念是版本发布，适用于项目的持续集成和频繁发布，这也是近些年使用Git的过程中一直使用的规范。&lt;/p&gt;
&lt;p&gt;git-flow流程中有5类分支，分别是master、develop、relaese(发布版本)
      
    
    </summary>
    
    
      <category term="Git" scheme="http://luxiaowan.github.io/categories/Git/"/>
    
    
  </entry>
  
  <entry>
    <title>Git本地仓库关联远程仓库</title>
    <link href="http://luxiaowan.github.io/2020/04/20/Git%E6%9C%AC%E5%9C%B0%E4%BB%93%E5%BA%93%E5%85%B3%E8%81%94%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93/"/>
    <id>http://luxiaowan.github.io/2020/04/20/Git本地仓库关联远程仓库/</id>
    <published>2020-04-19T17:39:00.000Z</published>
    <updated>2020-04-19T18:11:05.431Z</updated>
    
    <content type="html"><![CDATA[<h3 id="情景"><a class="markdownIt-Anchor" href="#情景"></a> 情景</h3><p>本地创建了一个工程，开发完成后想要提交到github/gitlab上</p><ul><li>本地</li></ul><p><img src="/images/image-20200420014905633.png" alt="image-20200420014905633"></p><ul><li>远程</li></ul><p><img src="/images/image-20200420014219180.png" alt="image-20200420014219180"></p><h3 id="步骤"><a class="markdownIt-Anchor" href="#步骤"></a> 步骤</h3><ol><li><p>进入到工程目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd git-demo</span><br></pre></td></tr></table></figure></li><li><p>初始化本地仓库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure><p>初始化之后，可以通过<code>ls -al</code>查看本地仓库文件，发现此时工程内多出了一个<code>.git</code>目录</p><p><img src="/images/image-20200420015451562.png" alt="image-20200420015451562"></p></li><li><p>设置关联远程仓库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin http://gitlab.xxx.cc/utils/demo.git</span><br></pre></td></tr></table></figure><p>命令执行之后，可以通过<code>git remote -v</code>查看是否关联成功，fetch是从远程仓库同步，push是推送到远程仓库</p><p><img src="/images/image-20200420015810083.png" alt="image-20200420015810083"></p></li><li><p>将本地文件add之后commit</p><p>在执行add命令之前，使用<code>git branch</code>查看本地分支会发现本地当前尚无分支，在add和commit之前，先确定一下本地仓库的账号是否可以连通远程仓库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看仓库所有配置</span></span><br><span class="line">git config -l</span><br></pre></td></tr></table></figure><p><img src="/images/image-20200420020611199.png" alt="image-20200420020611199"></p><p>如果账号不对，则可以通过<code>git config user.name</code>等命令修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add . &amp;&amp; git commit -m "初始化"</span><br></pre></td></tr></table></figure><p>命令执行完后再使用<code>git branch</code>会看到本地已经有了master分支</p><p><img src="/images/image-20200420020224634.png" alt="image-20200420020224634"></p></li><li><p>将本地仓库的文件推送到远程仓库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin master</span><br></pre></td></tr></table></figure><p>推送之后查看远程仓库，会发现远程仓库已经初始化好了</p><p><img src="/images/image-20200420020848287.png" alt="image-20200420020848287"></p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;情景&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#情景&quot;&gt;&lt;/a&gt; 情景&lt;/h3&gt;
&lt;p&gt;本地创建了一个工程，开发完成后想要提交到github/gitlab上&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本地&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img sr
      
    
    </summary>
    
    
      <category term="Git" scheme="http://luxiaowan.github.io/categories/Git/"/>
    
    
  </entry>
  
  <entry>
    <title>Kafka在数据传递上的场景分析</title>
    <link href="http://luxiaowan.github.io/2020/04/19/Kafka%E5%9C%A8%E6%95%B0%E6%8D%AE%E4%BC%A0%E9%80%92%E4%B8%8A%E7%9A%84%E5%9C%BA%E6%99%AF%E5%88%86%E6%9E%90/"/>
    <id>http://luxiaowan.github.io/2020/04/19/Kafka在数据传递上的场景分析/</id>
    <published>2020-04-19T14:38:00.000Z</published>
    <updated>2020-04-19T17:00:39.827Z</updated>
    
    <content type="html"><![CDATA[<h3 id="场景"><a class="markdownIt-Anchor" href="#场景"></a> 场景</h3><p>前段时间遇到一个场景，有一个用户社区模块，用户可以在社区的帖子专区发布帖子，或评论帖子，帖子和评论信息需要依据产生时间依次发送给第三方，消息内容为帖子和帖子评论，接收方必须先接收帖子再接收帖子评论，且需要按照发生时间顺序依次接收。第三接收方随时可能新增或减少，新增的接收方需要获取加入时间前30天之内的历史数据。</p><h3 id="消息中间件选择"><a class="markdownIt-Anchor" href="#消息中间件选择"></a> 消息中间件选择</h3><p>这是一个纯粹的消息传递功能，不涉及数据的整合和流处理，但是因为新增的消费者(第三方)要能获取到前30天之内的历史数据，所以我们需要一个能够将消息持久化的消息组件，并且要能自动清理30天之前的历史消息数据，选来选取，目前最方便的就是kafka了，我们来看一下kafka的特性：</p><ul><li>持久化：kafka默认就支持将消息持久化到磁盘，被消费的消息不会被删除</li><li>历史数据：kafka可以通过<code>log.retention.hours=720</code>指定只保存30天的数据</li><li>帖子顺序发送：kafka的partition是有序的</li><li>效率：kafka的topic可以通过多分区提升效率，为分区设置副本提升高可用</li></ul><h3 id="具体实现"><a class="markdownIt-Anchor" href="#具体实现"></a> 具体实现</h3><blockquote><p>Kafka配置</p></blockquote><p>设置kafka持久化日志过期时间：<code>log.retention.hours=720</code></p><blockquote><p>Topic分区数量</p></blockquote><p>依据帖子和评论的数量估算Topic的分区数量</p><blockquote><p>伪代码</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">push</span><span class="params">(T t)</span></span>&#123;</span><br><span class="line">  queryPostFromDB();</span><br><span class="line">  queryTopicPartitionSize();</span><br><span class="line">  calcPartitionForPostByPostId();</span><br><span class="line">  sendMessageToPartition();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>先从DB中查询帖子和帖子评论</li><li>查询topic在kafka中的分区数</li><li>根据帖子ID和分区数取模，将帖子和评论均衡分配到各个分区中</li><li>将消息发送给对应分区，帖子和对应评论需要发送到同一分区，并且要按照时间发送，这样才能保证同一个帖子的发送顺序</li></ol><blockquote><p>消费者</p></blockquote><p>每一个第三方的消费者都归属于唯一的消费者组，且必须给所有的消费者都设置一个消费者组</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;场景&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#场景&quot;&gt;&lt;/a&gt; 场景&lt;/h3&gt;
&lt;p&gt;前段时间遇到一个场景，有一个用户社区模块，用户可以在社区的帖子专区发布帖子，或评论帖子，帖子和评论信息需要依据产生时间依次发送给第三方，消息内容
      
    
    </summary>
    
    
      <category term="Kafka" scheme="http://luxiaowan.github.io/categories/Kafka/"/>
    
    
  </entry>
  
  <entry>
    <title>Kafka应用场景</title>
    <link href="http://luxiaowan.github.io/2020/04/18/Kafka%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/"/>
    <id>http://luxiaowan.github.io/2020/04/18/Kafka应用场景/</id>
    <published>2020-04-18T15:35:00.000Z</published>
    <updated>2020-04-19T11:34:27.904Z</updated>
    
    <content type="html"><![CDATA[<h3 id="序"><a class="markdownIt-Anchor" href="#序"></a> 序</h3><p>在学习一门新技术之前，我们需要先去了解一下这门技术的具体应用场景，使用它能够做什么，能够达到什么目的，学习kafka的初衷是用作消息队列；但是还可以使用Kafka Stream进行一些实时的流计算，多用于大数据处理；也可以做日志收集汇总、网站活动跟踪等任务。</p><h3 id="消息队列"><a class="markdownIt-Anchor" href="#消息队列"></a> 消息队列</h3><p>kafka可以很好的替代一些传统的消息系统，kafka具有更好的吞吐量，内置的分区使kafka具有更好的容错和伸缩性，这些特性使它可以替代传统的消息系统，成为大型消息处理应用的首选方案。</p><blockquote><p>场景：异步、解耦、削峰填谷</p><ol><li>生成订单：给不同的产品业务线分配同一个topic的不同partition，用户下单后根据订单类型发送到对应的partition</li><li>消息通知：用户登录后计算积分</li></ol></blockquote><ul><li><p>消息生产者</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">  prop.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"127.0.0.1:9092"</span>);</span><br><span class="line">  prop.put(<span class="string">"acks"</span>, <span class="string">"all"</span>);</span><br><span class="line">  prop.put(<span class="string">"retries"</span>, <span class="string">"0"</span>);</span><br><span class="line">  <span class="comment">// 缓冲区大小</span></span><br><span class="line">  prop.put(<span class="string">"batch.size"</span>, <span class="string">"10"</span>);</span><br><span class="line">  prop.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">  prop.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">  KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(prop);</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">101</span>; i++) &#123;</span><br><span class="line">    ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"my_topics"</span>, <span class="string">"value_"</span> + i);</span><br><span class="line">    <span class="comment">// 阻塞到消息发送完成</span></span><br><span class="line">    producer.send(record).get();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 刷新缓冲区，发送到分区，并清空缓冲区</span></span><br><span class="line">  <span class="comment">// producer.flush();</span></span><br><span class="line">  <span class="comment">// 关闭生产者，会阻塞到缓冲区内的数据发送完</span></span><br><span class="line">  producer.close();</span><br><span class="line">  <span class="comment">// producer.close(Duration.ofMillis(1000));</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>生产者发送消息是先将消息放到缓冲区，当缓冲区存满之后会自动flush，或者手动调用flush()方法</p></li><li><p>消息消费者</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">  Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">  properties.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"127.0.0.1:9092"</span>);</span><br><span class="line">  properties.put(<span class="string">"group.id"</span>, <span class="string">"cc_consumer"</span>);</span><br><span class="line">  properties.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">  properties.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">  KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line">  <span class="comment">// 指定topic</span></span><br><span class="line">  consumer.subscribe(Arrays.asList(<span class="string">"my_topics"</span>));</span><br><span class="line">  <span class="comment">// 指定topic的partition</span></span><br><span class="line">  <span class="comment">// TopicPartition partition0 = new TopicPartition("my_topics", 10);</span></span><br><span class="line">  <span class="comment">// consumer.assign(Arrays.asList(partition0));</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">      ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">      <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">        System.out.println(record.toString());</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    consumer.close(Duration.ofMillis(<span class="number">2000</span>));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h3 id="流计算"><a class="markdownIt-Anchor" href="#流计算"></a> 流计算</h3><p>​<em><font color="gray">[todo]</font></em></p><h3 id="日志收集"><a class="markdownIt-Anchor" href="#日志收集"></a> 日志收集</h3><p>应用程序的日志可以通过log4j收集日志信息，并将日志直接打到kafka中：客户端—&gt;应用—&gt;kafka</p><p>SpringBoot中默认使用的是logback，所以要在引入SpringBoot的jar包时排除掉logback的jar包</p><blockquote><p>日志消息发送有同步和异步两种方式，由KafkaAppender中的<code>syncSend</code>属性决定，默认为true(同步)</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="tag">&lt;<span class="name">Kafka</span> <span class="attr">name</span>=<span class="string">"KAFKA-LOGGER"</span> <span class="attr">topic</span>=<span class="string">"cc_log_test"</span> <span class="attr">syncSend</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><ul><li>pom.xml</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-web<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-logging<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- springboot 1.3.x之前版本是log4j，之后版本都是log4j2 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-log4j2<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>log4j2.xml</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">Configuration</span> <span class="attr">status</span>=<span class="string">"off"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">Properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">Properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">Appenders</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Console</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">target</span>=<span class="string">"SYSTEM_OUT"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">PatternLayout</span> <span class="attr">pattern</span>=<span class="string">"%d %p %c&#123;1.&#125; %t %m%n"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Console</span>&gt;</span></span><br><span class="line">      <span class="comment">&lt;!--kafka topic--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Kafka</span> <span class="attr">name</span>=<span class="string">"KAFKA-LOGGER"</span> <span class="attr">topic</span>=<span class="string">"my_topics"</span>&gt;</span></span><br><span class="line">          <span class="comment">&lt;!--JsonLayout：日志格式为json,方便在ES中处理--&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">JsonLayout</span>/&gt;</span></span><br><span class="line">          <span class="comment">&lt;!--kafka server的ip:port--&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">Property</span> <span class="attr">name</span>=<span class="string">"bootstrap.servers"</span>&gt;</span>127.0.0.1:9092<span class="tag">&lt;/<span class="name">Property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">Property</span> <span class="attr">name</span>=<span class="string">"retries"</span>&gt;</span>3<span class="tag">&lt;/<span class="name">Property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">Property</span> <span class="attr">name</span>=<span class="string">"linger.ms"</span>&gt;</span>1000<span class="tag">&lt;/<span class="name">Property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">Property</span> <span class="attr">name</span>=<span class="string">"buffer.memory"</span>&gt;</span>10485760<span class="tag">&lt;/<span class="name">Property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Kafka</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Async</span> <span class="attr">name</span>=<span class="string">"ASYNC-KAFKA-LOGGER"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">AppenderRef</span> <span class="attr">ref</span>=<span class="string">"KAFKA-LOGGER"</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">LinkedTransferQueue</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Async</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">Appenders</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">Loggers</span>&gt;</span></span><br><span class="line">      <span class="comment">&lt;!--日志级别大于info都会被记录到Kafka--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Logger</span> <span class="attr">name</span>=<span class="string">"cc.kevinlu.springbootkafka.controller.MessageController"</span> <span class="attr">level</span>=<span class="string">"info"</span></span></span><br><span class="line"><span class="tag">                <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">AppenderRef</span> <span class="attr">ref</span>=<span class="string">"KAFKA-LOGGER"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Logger</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- Root表示所有Logger用Root中的Appender打印日志  --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Root</span> <span class="attr">level</span>=<span class="string">"info"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">AppenderRef</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Root</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">Loggers</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">Configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>code</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@GetMapping</span>(<span class="string">"/log"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">sendLog</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">    log.info(<span class="string">"kafka log i = "</span> + i);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="string">"success"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>consumer视图</li></ul><img src="/images/image-20200419032218971.png" alt="image-20200419032218971" style="zoom: 50%;"><h3 id="网站活动跟踪"><a class="markdownIt-Anchor" href="#网站活动跟踪"></a> 网站活动跟踪</h3><ol><li><p>前端Nodejs控制</p><p>Node接入kafka需要使用kafka-node库，下面是网上的例子</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> kafka = <span class="built_in">require</span>(<span class="string">'kafka-node'</span>),</span><br><span class="line">    Producer = kafka.Producer,</span><br><span class="line">    client = <span class="keyword">new</span> kafka.KafkaClient(&#123;<span class="attr">kafkaHost</span>: <span class="string">'localhost:9092'</span>&#125;);</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 定义生产类</span></span><br><span class="line"><span class="comment"> * partitionerType 定义</span></span><br><span class="line"><span class="comment"> * 0:默认模式 只产生数据在第一个分区</span></span><br><span class="line"><span class="comment"> * 1:随机分配，在分区个数内，随机产生消息到各分区</span></span><br><span class="line"><span class="comment"> * 2:循环分配，在分区个数内，按顺序循环产生消息到各分区</span></span><br><span class="line"><span class="comment">*/</span>   </span><br><span class="line"><span class="keyword">var</span> producerOption = &#123;</span><br><span class="line">    requireAcks: <span class="number">1</span>,</span><br><span class="line">    ackTimeoutMs: <span class="number">100</span>,</span><br><span class="line">    partitionerType: <span class="number">0</span> <span class="comment">//默认为第一个分区</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">var</span> producer = <span class="keyword">new</span> Producer(client,producerOption);</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * TOPIC的创建需要在命令行进行创建，以便指定分区个数以及备份个数</span></span><br><span class="line"><span class="comment"> * PS：kafka-node的创建topic不行，不能创建分区</span></span><br><span class="line"><span class="comment"> * 产生消息,如果不指定partition</span></span><br><span class="line"><span class="comment"> * 则根据 partitionerType 的值来指定发送数据到哪个分区</span></span><br><span class="line"><span class="comment"> * 我们创建的topic-test-one只有一个分区，所以只能产生数据到第1个分区（下标0），否则不会生产数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">getPayloads</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> [</span><br><span class="line">        &#123;<span class="attr">topic</span>:<span class="string">"topic-test-one"</span>,<span class="attr">messages</span>:<span class="built_in">JSON</span>.stringify(&#123;<span class="string">"name"</span>:<span class="string">"jack"</span>,<span class="string">"age"</span>:<span class="string">"120"</span>&#125;),<span class="attr">partition</span>:<span class="number">0</span>&#125;</span><br><span class="line">    ];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">producer.on(<span class="string">"ready"</span>,<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    setInterval(<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">        producer.send(getPayloads(),<span class="function"><span class="keyword">function</span>(<span class="params">err,data</span>)</span>&#123;</span><br><span class="line">            <span class="keyword">if</span>(!err)&#123;</span><br><span class="line">                <span class="built_in">console</span>.log(<span class="string">"send message complete!data:"</span>+<span class="built_in">JSON</span>.stringify(data),<span class="keyword">new</span> <span class="built_in">Date</span>());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">     &#125;,<span class="number">1000</span>);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">producer.on(<span class="string">'error'</span>, <span class="function"><span class="keyword">function</span> (<span class="params">err</span>) </span>&#123;<span class="built_in">console</span>.log(<span class="string">"send message error!\r\n"</span>+err);&#125;)</span><br></pre></td></tr></table></figure></li><li><p>后端日志控制</p><p>后端也可以使用log4j的日志系统来完成，拦截所有需要监控的api请求，使用log4j输出日志到kafka队列中，和上述日志收集方法相同。若同一个应用中需要通过日志输出到kafka的多个topic中，可以使用log4j的Marker标记来区分，配置如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">Configuration</span> <span class="attr">status</span>=<span class="string">"off"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">Properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">Properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">Appenders</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Console</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">target</span>=<span class="string">"SYSTEM_OUT"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">PatternLayout</span> <span class="attr">pattern</span>=<span class="string">"%d %p %c&#123;1.&#125; %t %m%n"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Console</span>&gt;</span></span><br><span class="line">      <span class="comment">&lt;!-- 日志收集 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Kafka</span> <span class="attr">name</span>=<span class="string">"KAFKA-LOGGER"</span> <span class="attr">topic</span>=<span class="string">"cc_log_test"</span> <span class="attr">syncSend</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">JsonLayout</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">Property</span> <span class="attr">name</span>=<span class="string">"bootstrap.servers"</span>&gt;</span>127.0.0.1:9092<span class="tag">&lt;/<span class="name">Property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">Property</span> <span class="attr">name</span>=<span class="string">"retries"</span>&gt;</span>3<span class="tag">&lt;/<span class="name">Property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">Property</span> <span class="attr">name</span>=<span class="string">"linger.ms"</span>&gt;</span>1000<span class="tag">&lt;/<span class="name">Property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">Property</span> <span class="attr">name</span>=<span class="string">"buffer.memory"</span>&gt;</span>10485760<span class="tag">&lt;/<span class="name">Property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">Filters</span>&gt;</span></span><br><span class="line">              <span class="comment">&lt;!-- 通过Marker过滤消息 --&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">MarkerFilter</span> <span class="attr">marker</span>=<span class="string">"Kafka"</span> <span class="attr">onMatch</span>=<span class="string">"ACCEPT"</span> <span class="attr">onMismatch</span>=<span class="string">"DENY"</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">Filters</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Kafka</span>&gt;</span></span><br><span class="line">      <span class="comment">&lt;!-- 轨迹跟踪 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Kafka</span> <span class="attr">name</span>=<span class="string">"KAFKA-TRACK-LOGGER"</span> <span class="attr">topic</span>=<span class="string">"cc_test1"</span> <span class="attr">syncSend</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">JsonLayout</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">Property</span> <span class="attr">name</span>=<span class="string">"bootstrap.servers"</span>&gt;</span>127.0.0.1:9092<span class="tag">&lt;/<span class="name">Property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">Property</span> <span class="attr">name</span>=<span class="string">"retries"</span>&gt;</span>3<span class="tag">&lt;/<span class="name">Property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">Property</span> <span class="attr">name</span>=<span class="string">"linger.ms"</span>&gt;</span>1000<span class="tag">&lt;/<span class="name">Property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">Property</span> <span class="attr">name</span>=<span class="string">"buffer.memory"</span>&gt;</span>10485760<span class="tag">&lt;/<span class="name">Property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">Filters</span>&gt;</span></span><br><span class="line">              <span class="comment">&lt;!-- 通过Marker过滤消息 --&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">MarkerFilter</span> <span class="attr">marker</span>=<span class="string">"Track"</span> <span class="attr">onMatch</span>=<span class="string">"ACCEPT"</span> <span class="attr">onMismatch</span>=<span class="string">"DENY"</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">Filters</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Kafka</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Async</span> <span class="attr">name</span>=<span class="string">"ASYNC-KAFKA-LOGGER"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">AppenderRef</span> <span class="attr">ref</span>=<span class="string">"KAFKA-LOGGER"</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">AppenderRef</span> <span class="attr">ref</span>=<span class="string">"KAFKA-TRACK-LOGGER"</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">LinkedTransferQueue</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Async</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">Appenders</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">Loggers</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Logger</span> <span class="attr">name</span>=<span class="string">"cc.kevinlu.springbootkafka.controller"</span> <span class="attr">level</span>=<span class="string">"info"</span></span></span><br><span class="line"><span class="tag">                <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">AppenderRef</span> <span class="attr">ref</span>=<span class="string">"KAFKA-LOGGER"</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">AppenderRef</span> <span class="attr">ref</span>=<span class="string">"KAFKA-TRACK-LOGGER"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Logger</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Root</span> <span class="attr">level</span>=<span class="string">"info"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">AppenderRef</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Root</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">Loggers</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">Configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> Marker KAFKA_MARKER       = MarkerManager.getMarker(<span class="string">"Kafka"</span>);</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> Marker KAFKA_TRACK_MARKER = MarkerManager.getMarker(<span class="string">"Track"</span>);</span><br><span class="line"></span><br><span class="line"><span class="meta">@GetMapping</span>(<span class="string">"/log"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">sendLog</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 轨迹跟踪</span></span><br><span class="line">  log.info(KAFKA_TRACK_MARKER, <span class="string">"send async message!"</span>);</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">    <span class="comment">// 日志收集</span></span><br><span class="line">    log.info(KAFKA_MARKER, <span class="string">"kafka log i = &#123;&#125;"</span>, i);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="string">"success"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>前端+后端组合</p><p>后端提供API供前端传递轨迹，后端接收到请求之后将消息同步到kafka中。</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;序&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#序&quot;&gt;&lt;/a&gt; 序&lt;/h3&gt;
&lt;p&gt;在学习一门新技术之前，我们需要先去了解一下这门技术的具体应用场景，使用它能够做什么，能够达到什么目的，学习kafka的初衷是用作消息队列；但是还可以使
      
    
    </summary>
    
    
      <category term="Kafka" scheme="http://luxiaowan.github.io/categories/Kafka/"/>
    
    
  </entry>
  
  <entry>
    <title>Kafka消费者</title>
    <link href="http://luxiaowan.github.io/2020/04/17/Kafka%E6%B6%88%E8%B4%B9%E8%80%85/"/>
    <id>http://luxiaowan.github.io/2020/04/17/Kafka消费者/</id>
    <published>2020-04-17T06:46:00.000Z</published>
    <updated>2020-04-17T14:40:13.306Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简"><a class="markdownIt-Anchor" href="#简"></a> 简</h3><p>消息由生产者产出，产出后push到partition中，但是既然有生产了，那肯定就要有消费，不然我们生产出来的消息岂不成了垃圾数据，所以在kafka中有一个与生产者对应的玩意儿：消费者。生产者是往partition中push数据，而消费者是从partition中pull消息，有些MQ中是由服务端push消息给消费者，并且是阅后即焚的模式，但是kafka是支持将消息持久化到磁盘上，并且并不会因为消息被消费了而删除，每一个消费者都可以自由的消费partition中的历史消息，即使是一个新加入的Consumer，也可以通过指定offset将partition中所有的历史消息都从头消费一次，而服务端只负责为消息设定offset，Consumer从哪一条消息开始消费完全由自己决定，每一个Consumer都会在本地维护自己的offset，Consumer之间的offset互不干扰。</p><p>每一个消费者都归属于一个消费者群组，一个partition只能被同一个消费者群组内的一个消费者实例消费，但可以被不同消费者群组同时消费，每个群组内的所有消费者订阅的都是同一个topic，每个消费者接收一个topic中一部分partition的消息，若消费者数量多于partition的数量，则会出现闲置的消费者，而若消费者数量小于partition的数量，则会出现某个消费者消费多个partition中的数据。</p><h3 id="消费者群组consumer-group"><a class="markdownIt-Anchor" href="#消费者群组consumer-group"></a> 消费者群组(Consumer Group)</h3><p>消费者群组是由一个或多个消费者实例组成的群组，具有可扩展性和容错性。每一个消费者群组都拥有全局唯一的group_id，群组内的所有的消费者实例共享这个group_id，群组内的所有消费者订阅同一个topic，组内的一个消费者实例只能消费topic中的一个Partition的消息。</p><p>消费类型：</p><ol><li>点对点：一个topic对应一个消费者群组</li><li>广播（发布-订阅）：一个topic对应多个消费者群组</li></ol><h3 id="消费者和分区"><a class="markdownIt-Anchor" href="#消费者和分区"></a> 消费者和分区</h3><ol><li><p>Consumer数量 &lt; Partition数量</p><p>当Partition的数量多于Consumer的时候，一个Consumer会消费来自多个Partition的消息，此种情形下会加大Consumer的压力，单Consumer实例的吞吐量决定了消息消费的效率，当消息井喷的时候，会造成大多数消息都积压在Partition中，严重的时候会影响服务性能。若想提升消息消费效率，可以通过增加Consumer实例解决。</p><img src="/images/image-20200417151054543.png" alt="image-20200417151054543" style="zoom:50%;"></li><li><p>Consumer数量 = Partition数量</p><p>当Partition和Consumer数量相等的时候属于是最平衡的状态，一个Partition对应一个Consumer，消息的消费效率完全取决于Consumer实例的吞吐量</p><img src="/images/image-20200417151007055.png" alt="image-20200417151007055" style="zoom:50%;"></li><li><p>Consumer数量 &gt; Partition数量</p><p>因为一个Partition只能被同一个消费者组内的一个Consumer实例消费，所以当Consumer的数量大于Partition的数量时，将会有多个Consumer实例空闲着，无消息可消费。比如下图中，Topic0有4个Partition，而Consumer Group中有5个消费者实例，由消费关系来看，Consumer4这个实例会一直空闲着。</p><img src="/images/image-20200417150940246.png" alt="image-20200417150940246" style="zoom: 50%;"></li><li><p>多元化消费消息</p><p>一个Partition只能被同一个消费者群组内的一个Consumer实例消费，但是我们在实际生产中可能会遇到需要将一条消息发送给多端的情况，比如订单生成佣金的时候，不同的项目组都有各自的佣金策略，这种情况可以给每一个项目组分配一个group的形式让所有的项目组共享消息</p><img src="/images/image-20200417165151251.png" alt="image-20200417165151251" style="zoom:50%;"></li></ol><h3 id="消费者重平衡rebalance"><a class="markdownIt-Anchor" href="#消费者重平衡rebalance"></a> 消费者重平衡(ReBalance)</h3><p>当消费者群组内的消费者实例发生变化时，消息的消费情况会是什么样子？用户群组初始有两个Consumer实例，每一个都要负责消费两个Partition的消息，然后群组内新增了两个Consumer实例，这样Consumer和Partition的数量一致了，新增加的消费者会均衡的替原有消费者分摊处理Partition的消息，最终达到一对一的平衡状态，这个过程就叫作ReBalance。</p><p><img src="/images/image-20200417172313104.png" alt="image-20200417172313104"></p><p>kafka通过消费者群组的ReBalance实现高可用和伸缩性，在群组做ReBalance期间，整个群组的消费者都无法消费消息，所以一般情况下我们都会预先估算好消费者群组内的消费者数量，估算的依据是消息量和topic下的Partition数。且当一个Partition重新分配给另一个消费者实例时，会造成当前正在消费的消息状态丢失。</p><p>消费者群组内的消费者实例需要定期向broker发送心跳来维护自己在群组内的地位，比如消费者进行poll和commit的同时，会发送一次心跳。如果broker长时间未接收到来自Consumer的心跳请求，则认为该Consumer实例已宕机，自动将其从群组中移除，并做一次ReBalance，Consumer之前对应的Partition的消息会暂时不再被消费。</p><h3 id="偏移量"><a class="markdownIt-Anchor" href="#偏移量"></a> 偏移量</h3><p>偏移量可以分为生产者偏移量和消费者偏移量，生产者偏移量实际上就是每一个Partition的偏移量，Partition之间是相互隔离的，也可以理解为Partition中的每一个消息都一个offset属性，每一个Producer针对topic-Partition都在自身维护一个offset；同时也可以理解为生产者偏移量只是一个虚拟的offset，实际上就是消息队列的size。消费者偏移量是切切实实和消费者相关的一个最重要的元素，使用偏移量，Consumer才能知道要从哪条消息开始读取，偏移量主要和topic的Partition相关，尤其是在消费者群组发生ReBalance的时候，调整后的Consumer从新的Partition接收消息之前需要知道这个Partition有多少消息已经被消费了，当前的消费者要从哪一条消息开始消费，否则就会出现重复消费或者遗漏消息的情况。</p><p>kafka中默认一个topic：<code>__consumer_offsets</code>，消费者每消费一次就会像该topic发送一次消息，消息中包含每一个Topic-Partition的offset，在发生ReBalance之后，消费者开始负责另外一个Partition的消息，这个时候会向<code>__consumer_offsets</code>获取该Partition最后一次被消费的偏移量，然后从该偏移量开始继续消费数据。</p><p>通过topic、partition、offset三个元素可以定位一条消息。</p><p>消费者在获取一批消息之后，需要将最后一条消息的偏移量提交给服务器，也就是将偏移量commit给topic<code>__consumer_offsets</code>，提交方式有自动提交和手动提交两种，通过参数<code>enable.auto.commit</code>来控制，默认值为true，也就是默认自动提交；而将参数值改为false，就是手动提交。</p><ul><li>自动提交<ol><li><code>enable.auto.commit=true</code>，设置为自动提交</li><li><code>auto.commit.interval.ms=3000</code>，设置自动提交时间间隔，单位：毫秒，默认5秒</li></ol></li><li>手动提交<ol><li><code>enable.auto.commit=false</code>，设置为手动提交</li><li>同步提交：<code>consumer.commitSync()</code></li><li>异步提交：<code>consumer.commitAsync()</code></li></ol></li></ul><h3 id="配置"><a class="markdownIt-Anchor" href="#配置"></a> 配置</h3><ul><li><code>fetch.min.bytes</code>：指定消费者每次获取记录的最小字节数，默认为1字节，若本次poll时数据量大小不满足条件，则会等到有足够的数据时再拉取。当消息不用非常实时的时候，可以将消费者的此值设置略大一些，以此降低消费者和broker的工作负载。</li><li><code>fetch.max.bytes</code>：指定消费者每次获取记录的最大字节数，默认为50M</li><li><code>max.partition.fetch.bytes</code>：指定每个分区每次拉取消息的最大字节数，默认为1M</li><li><code>auto.offset.reset</code>：指定消费者在读取一个没有偏移量的分区或者偏移量无效的情况下的处理方式，默认值是latest，也就是从最新的记录开始读取，也可以设置为earliest，让消费者从第一条消息开始读取</li><li><code>enable.auto.commit</code>：设置消费者是否自动提交偏移量，默认为true</li><li><code>heartbeat.interval.ms</code>：消费者每次心跳间隔，单位：毫秒，默认为3s</li><li><code>bootstrap.servers</code>：指定消费者连接的kafka集群地址，通过逗号分隔，格式：<code>host1:port1,host2:port2,...</code></li><li><code>group.id</code>：消费者所属群组id，默认为默认分组</li><li><code>key.deserializer</code>：key反序列化方式</li><li><code>value.deserializer</code>：value反序列化方式</li></ul><h3 id="案例"><a class="markdownIt-Anchor" href="#案例"></a> 案例</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">receiver</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">  <span class="comment">// kafka集群地址</span></span><br><span class="line">  properties.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"127.0.0.1:9092"</span>);</span><br><span class="line">  <span class="comment">// 消费者群组id</span></span><br><span class="line">  properties.put(<span class="string">"group.id"</span>, <span class="string">"cc_consumer"</span>);</span><br><span class="line">  <span class="comment">// key反序列化</span></span><br><span class="line">  properties.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">  <span class="comment">// value反序列化</span></span><br><span class="line">  properties.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">  <span class="comment">// 创建消费者</span></span><br><span class="line">  KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line">  <span class="comment">// 指定消费者监听的topic</span></span><br><span class="line">  consumer.subscribe(Arrays.asList(<span class="string">"my_topics"</span>));</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">      <span class="comment">// 拉取topic</span></span><br><span class="line">      ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">      <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">        System.out.println(record.toString());</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    consumer.close(Duration.ofMillis(<span class="number">2000</span>));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;简&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#简&quot;&gt;&lt;/a&gt; 简&lt;/h3&gt;
&lt;p&gt;消息由生产者产出，产出后push到partition中，但是既然有生产了，那肯定就要有消费，不然我们生产出来的消息岂不成了垃圾数据，所以在kafka中
      
    
    </summary>
    
    
      <category term="Kafka" scheme="http://luxiaowan.github.io/categories/Kafka/"/>
    
    
  </entry>
  
  <entry>
    <title>Kafka生产者</title>
    <link href="http://luxiaowan.github.io/2020/04/17/Kafka%E7%94%9F%E4%BA%A7%E8%80%85/"/>
    <id>http://luxiaowan.github.io/2020/04/17/Kafka生产者/</id>
    <published>2020-04-16T16:46:00.000Z</published>
    <updated>2020-04-17T05:06:52.429Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简"><a class="markdownIt-Anchor" href="#简"></a> 简</h3><p>在kafka中把产生消息的一方称为生产者（Producer），尽管消息的产生非常简单，但是消息的发送过程比较复杂</p><p><img src="/images/kafka-p-send.png" alt="img"></p><p>发送消息从创建一个ProducerRecord对象开始，此类是kafka中的一个核心类，表示kafka需要发送的K-V键值对，记录了要发送的topic、partition、key、value、timestamp等</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerRecord</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String topic;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Integer partition;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Headers headers;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> K key;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> V value;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Long timestamp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在发送ProducerRecord的时候需要将对象序列化为字节数组，便于在网络上传输，之后消息达到分区器，若发送过程中指定了分区号，也就是partition，则在发送消息的时候将使用指定的分区，若发送过程中未制定分区，则根据topic和cluster中的partition数量顺序选择一个分区进行发送，分区选择器由接接口<code>org.apache.kafka.clients.producer.Partitioner</code>的实现类指定。</p><blockquote><p>org.apache.kafka.clients.producer.KafkaProducer</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(ProducerRecord&lt;K, V&gt; record, <span class="keyword">byte</span>[] serializedKey, <span class="keyword">byte</span>[] serializedValue, Cluster cluster)</span> </span>&#123;</span><br><span class="line">  Integer partition = record.partition();</span><br><span class="line">  <span class="keyword">return</span> partition != <span class="keyword">null</span> ?</span><br><span class="line">    partition :</span><br><span class="line">  partitioner.partition(</span><br><span class="line">    record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>org.apache.kafka.clients.producer.internals.DefaultPartitioner</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 选取分区</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line">  List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line">  <span class="keyword">int</span> numPartitions = partitions.size();</span><br><span class="line">  <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="comment">// 顺序index</span></span><br><span class="line">    <span class="keyword">int</span> nextValue = nextValue(topic);</span><br><span class="line">    List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span><br><span class="line">    <span class="keyword">if</span> (availablePartitions.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// 取模</span></span><br><span class="line">      <span class="keyword">int</span> part = Utils.toPositive(nextValue) % availablePartitions.size();</span><br><span class="line">      <span class="keyword">return</span> availablePartitions.get(part).partition();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// no partitions are available, give a non-available partition</span></span><br><span class="line">      <span class="keyword">return</span> Utils.toPositive(nextValue) % numPartitions;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// hash the keyBytes to choose a partition</span></span><br><span class="line">    <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ProducerRecord内关联的时间戳timestamp，如果用户未指定，则使用KafkaProducer内的time的时间作为时间戳，但是kafka最终使用的时间戳取决于topic配置的时间戳类型：</p><ul><li>topic为CreateTime，则消息记录中的时间戳由broker使用</li><li>topic为LogAppendTime，则消息记录中的时间戳会在追加到日志中时由broker重写</li></ul><p><img src="/images/kafka_log.png" alt="img"></p><p>消息被放在一个记录批次里<code>ProducerBatch</code>，这个批次的所有消息都会被发送到相同的topic和partition上，由一个FutureRecordMetadata负责发送。</p><p>broker收到消息后会返回一个响应，如果发送正常的话，会返回一个<code>RecordAppendResult</code>对象，包含了topic、partition、offset、时间戳等信息，发送失败则会将失败的消息记录下来，然后后续重试发送。</p><blockquote><p>org.apache.kafka.clients.producer.KafkaProducer</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> Future&lt;RecordMetadata&gt; <span class="title">doSend</span><span class="params">(ProducerRecord&lt;K, V&gt; record, Callback callback)</span> </span>&#123;</span><br><span class="line">  TopicPartition tp = <span class="keyword">null</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    throwIfProducerClosed();</span><br><span class="line">    <span class="comment">// first make sure the metadata for the topic is available</span></span><br><span class="line">    ClusterAndWaitTime clusterAndWaitTime;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (KafkaException e) &#123;</span><br><span class="line">      <span class="keyword">if</span> (metadata.isClosed())</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Producer closed while send in progress"</span>, e);</span><br><span class="line">      <span class="keyword">throw</span> e;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">long</span> remainingWaitMs = Math.max(<span class="number">0</span>, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);</span><br><span class="line">    Cluster cluster = clusterAndWaitTime.cluster;</span><br><span class="line">    <span class="comment">// 序列化key</span></span><br><span class="line">    <span class="keyword">byte</span>[] serializedKey;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (ClassCastException cce) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"Can't convert key of class "</span> + record.key().getClass().getName() +</span><br><span class="line">                                       <span class="string">" to class "</span> + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +</span><br><span class="line">                                       <span class="string">" specified in key.serializer"</span>, cce);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 序列化value</span></span><br><span class="line">    <span class="keyword">byte</span>[] serializedValue;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (ClassCastException cce) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"Can't convert value of class "</span> + record.value().getClass().getName() +</span><br><span class="line">                                       <span class="string">" to class "</span> + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +</span><br><span class="line">                                       <span class="string">" specified in value.serializer"</span>, cce);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 决定要发送的partition</span></span><br><span class="line">    <span class="keyword">int</span> partition = partition(record, serializedKey, serializedValue, cluster);</span><br><span class="line">    tp = <span class="keyword">new</span> TopicPartition(record.topic(), partition);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置header</span></span><br><span class="line">    setReadOnly(record.headers());</span><br><span class="line">    Header[] headers = record.headers().toArray();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> serializedSize = AbstractRecords.estimateSizeInBytesUpperBound(apiVersions.maxUsableProduceMagic(),</span><br><span class="line">                                                                       compressionType, serializedKey, serializedValue, headers);</span><br><span class="line">    ensureValidRecordSize(serializedSize);</span><br><span class="line">    <span class="comment">// 设置消息时间戳</span></span><br><span class="line">    <span class="keyword">long</span> timestamp = record.timestamp() == <span class="keyword">null</span> ? time.milliseconds() : record.timestamp();</span><br><span class="line">    log.trace(<span class="string">"Sending record &#123;&#125; with callback &#123;&#125; to topic &#123;&#125; partition &#123;&#125;"</span>, record, callback, record.topic(), partition);</span><br><span class="line">    <span class="comment">// producer callback will make sure to call both 'callback' and interceptor callback</span></span><br><span class="line">    Callback interceptCallback = <span class="keyword">new</span> InterceptorCallback&lt;&gt;(callback, <span class="keyword">this</span>.interceptors, tp);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 事务</span></span><br><span class="line">    <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span> &amp;&amp; transactionManager.isTransactional())</span><br><span class="line">      transactionManager.maybeAddPartitionToTransaction(tp);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 发送消息，见下方代码</span></span><br><span class="line">    RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey,</span><br><span class="line">                                                                     serializedValue, headers, interceptCallback, remainingWaitMs);</span><br><span class="line">    <span class="keyword">if</span> (result.batchIsFull || result.newBatchCreated) &#123;</span><br><span class="line">      log.trace(<span class="string">"Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch"</span>, record.topic(), partition);</span><br><span class="line">      <span class="keyword">this</span>.sender.wakeup();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result.future;</span><br><span class="line">  &#125; <span class="keyword">catch</span> (ApiException e) &#123;</span><br><span class="line">    log.debug(<span class="string">"Exception occurred during message send:"</span>, e);</span><br><span class="line">    <span class="keyword">if</span> (callback != <span class="keyword">null</span>)</span><br><span class="line">      callback.onCompletion(<span class="keyword">null</span>, e);</span><br><span class="line">    <span class="comment">// 记录错误信息</span></span><br><span class="line">    <span class="keyword">this</span>.errors.record();</span><br><span class="line">    <span class="keyword">this</span>.interceptors.onSendError(record, tp, e);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> FutureFailure(e);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">    <span class="keyword">this</span>.errors.record();</span><br><span class="line">    <span class="keyword">this</span>.interceptors.onSendError(record, tp, e);</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> InterruptException(e);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (BufferExhaustedException e) &#123;</span><br><span class="line">    <span class="keyword">this</span>.errors.record();</span><br><span class="line">    <span class="keyword">this</span>.metrics.sensor(<span class="string">"buffer-exhausted-records"</span>).record();</span><br><span class="line">    <span class="keyword">this</span>.interceptors.onSendError(record, tp, e);</span><br><span class="line">    <span class="keyword">throw</span> e;</span><br><span class="line">  &#125; <span class="keyword">catch</span> (KafkaException e) &#123;</span><br><span class="line">    <span class="keyword">this</span>.errors.record();</span><br><span class="line">    <span class="keyword">this</span>.interceptors.onSendError(record, tp, e);</span><br><span class="line">    <span class="keyword">throw</span> e;</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">    <span class="keyword">this</span>.interceptors.onSendError(record, tp, e);</span><br><span class="line">    <span class="keyword">throw</span> e;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>org.apache.kafka.clients.producer.internals.RecordAccumulator</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> RecordAppendResult <span class="title">append</span><span class="params">(TopicPartition tp,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">long</span> timestamp,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">byte</span>[] key,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">byte</span>[] value,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     Header[] headers,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     Callback callback,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">long</span> maxTimeToBlock)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">  appendsInProgress.incrementAndGet();</span><br><span class="line">  ByteBuffer buffer = <span class="keyword">null</span>;</span><br><span class="line">  <span class="keyword">if</span> (headers == <span class="keyword">null</span>) headers = Record.EMPTY_HEADERS;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    Deque&lt;ProducerBatch&gt; dq = getOrCreateDeque(tp);</span><br><span class="line">    <span class="keyword">synchronized</span> (dq) &#123;</span><br><span class="line">      <span class="keyword">if</span> (closed)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Producer closed while send in progress"</span>);</span><br><span class="line">      RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq);</span><br><span class="line">      <span class="keyword">if</span> (appendResult != <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">return</span> appendResult;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">byte</span> maxUsableMagic = apiVersions.maxUsableProduceMagic();</span><br><span class="line">    <span class="keyword">int</span> size = Math.max(<span class="keyword">this</span>.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(maxUsableMagic, compression, key, value, headers));</span><br><span class="line">    log.trace(<span class="string">"Allocating a new &#123;&#125; byte message buffer for topic &#123;&#125; partition &#123;&#125;"</span>, size, tp.topic(), tp.partition());</span><br><span class="line">    <span class="comment">// 申请一个缓冲区，将消息数据写入到缓冲区中</span></span><br><span class="line">    buffer = free.allocate(size, maxTimeToBlock);</span><br><span class="line">    <span class="keyword">synchronized</span> (dq) &#123;</span><br><span class="line">      <span class="keyword">if</span> (closed)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Producer closed while send in progress"</span>);</span><br><span class="line"></span><br><span class="line">      RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq);</span><br><span class="line">      <span class="keyword">if</span> (appendResult != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> appendResult;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic);</span><br><span class="line">      <span class="comment">// 将消息分批处理</span></span><br><span class="line">      ProducerBatch batch = <span class="keyword">new</span> ProducerBatch(tp, recordsBuilder, time.milliseconds());</span><br><span class="line">      FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, headers, callback, time.milliseconds()));</span><br><span class="line"></span><br><span class="line">      dq.addLast(batch);</span><br><span class="line">      incomplete.add(batch);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 清空缓冲区</span></span><br><span class="line">      buffer = <span class="keyword">null</span>;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> RecordAppendResult(future, dq.size() &gt; <span class="number">1</span> || batch.isFull(), <span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (buffer != <span class="keyword">null</span>)</span><br><span class="line">      free.deallocate(buffer);</span><br><span class="line">    appendsInProgress.decrementAndGet();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="消息发送类型"><a class="markdownIt-Anchor" href="#消息发送类型"></a> 消息发送类型</h3><ol><li><p>简单发送</p><p>kafka最简单的消息发送是只指定topic和key及value，分区及时间戳均使用默认值，send()方法会返回一个<code>Future&lt;RecordMetadata&gt;</code>对象，如果不需要关心返回值，则可以忽略这个返回值，否则必须关注此值，方法返回的异常信息可能有<code>InterruptedException(发送线程中断异常)</code>，<code>BufferExhaustedException(缓冲区已满)</code>，<code>SerializationException(序列化异常)</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ProducerRecord&lt;String,String&gt; record =</span><br><span class="line">                <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"cc_test"</span>,<span class="string">"cc"</span>,<span class="string">"chuanchuan"</span>);</span><br><span class="line">producer.send(record);</span><br></pre></td></tr></table></figure></li><li><p>同步发送</p><p>第一种简单发送方式的前提是我们不在意发送的结果，但是我们在正常的情况下都会等待broker的反馈。我们从发送的源码中看到send()方法返回的<code>Future&lt;RecordMetadata&gt;</code>对象，我们可以调用Future的get()方法阻塞主线程等待broker的响应，如果返回错误，则我们调用get()方法的时候会抛出异常，如果没发生异常，则顺利获取到<code>RecordMetadata</code>对象，使用该对象查看消息的详细信息：topic、key和value的序列化后的大小、offset、partition。</p><p>生产者发送过程中一般会出现两类错误：一类可以通过重试解决，一类无法通过重试解决。比如连接错误、无Leader错误等都可以通过重试来实现，而消息过大这类错误KafkaProducer会直接抛出异常，不会重试，因为不管重试多少次都是消息过大。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"cc_test"</span>, <span class="string">"cc"</span>, <span class="string">"chuanchuan"</span>);</span><br><span class="line"><span class="keyword">try</span>&#123;</span><br><span class="line">  RecordMetadata rm = producer.send(record).get();</span><br><span class="line">  System.out.println(rm.offset());</span><br><span class="line">&#125; <span class="keyword">catch</span>(Exception e) &#123;</span><br><span class="line">  log.error(<span class="string">"occur error"</span>, e);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>异步发送</p><p>消息同步发送会造成同一时间只能有一条消息在发送中，在其有返回之前，其他的消息都需要一直等待，这样会造成消息堵塞滞后，无法让kafka发挥更大的效益，若一个消息发送需要20ms，发送五十条消息就需要1s，如果我们使用异步这种方式，那么发送五十条可能只需要30ms，甚至更少。异步发送的原理是在我们调用send()方法时传入一个接口<code>org.apache.kafka.clients.producer.Callback</code>的实现类的对象，由ProducerBatch的私有方法<code>completeFutureAndFireCallbacks</code>完成回调</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"cc_test"</span>, <span class="string">"cc"</span>, <span class="string">"chuanchuan"</span>);</span><br><span class="line">producer.send(record, );</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CcProducerCallback</span> <span class="keyword">implements</span> <span class="title">Callback</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata,Exception exception)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(exception != <span class="keyword">null</span>)&#123;</span><br><span class="line">      exception.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>org.apache.kafka.clients.producer.internals.ProducerBatch</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">completeFutureAndFireCallbacks</span><span class="params">(<span class="keyword">long</span> baseOffset, <span class="keyword">long</span> logAppendTime, RuntimeException exception)</span> </span>&#123;</span><br><span class="line">  produceFuture.set(baseOffset, logAppendTime, exception);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// execute callbacks</span></span><br><span class="line">  <span class="keyword">for</span> (Thunk thunk : thunks) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 发生异常</span></span><br><span class="line">      <span class="keyword">if</span> (exception == <span class="keyword">null</span>) &#123;</span><br><span class="line">        RecordMetadata metadata = thunk.future.value();</span><br><span class="line">        <span class="keyword">if</span> (thunk.callback != <span class="keyword">null</span>)</span><br><span class="line">          thunk.callback.onCompletion(metadata, <span class="keyword">null</span>);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 正常</span></span><br><span class="line">        <span class="keyword">if</span> (thunk.callback != <span class="keyword">null</span>)</span><br><span class="line">          thunk.callback.onCompletion(<span class="keyword">null</span>, exception);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      log.error(<span class="string">"Error executing user-provided callback on message for topic-partition '&#123;&#125;'"</span>, topicPartition, e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  produceFuture.done();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h3 id="分区机制"><a class="markdownIt-Anchor" href="#分区机制"></a> 分区机制</h3><p>kafka对于数据的读写是以partition为粒度的，partition可以分布在不同的broker上，每个节点都可以独立的实现消息的读写，并且能够通过新增新的broker来提升kafka集群的吞吐量，partition部署在多个broker来实现负载均衡。</p><p>kafka的分区策略其实指的就是Producer将消息发送到哪个分区的算法，kafka提供了默认的分区策略，同时也支持我们自定义分区策略，所有的策略都实现于接口<code>org.apache.kafka.clients.producer.Partitioner</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Partitioner</span> <span class="keyword">extends</span> <span class="title">Configurable</span>, <span class="title">Closeable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 提供消息信息计算partition</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topic topic名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key key名称</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> keyBytes key序列化字节数组</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value value值</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> valueBytes value序列化字节数组</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> cluster 集群</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 关闭partitioner</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>消息发送到哪一个partition上涉及到分区选择机制，主要有顺序、随机、按key分配、自定义分配等方式，具体的实现方法就是<code>public int partition()</code>。</p><ol><li><p>顺序轮询</p><p>顺序分配就是消息均匀的发送给每一个partition，每个partition存储一次消息，kafka的默认策略。</p><img src="/images/image-20200417033430230.png" alt="image-20200417033430230" style="zoom: 67%;"></li><li><p>随机策略</p><p>随机策略可以先计算出topic的总的partition数，然后使用<code>ThreadLocalRandom.current().nextInt()</code>方法来获取一个小于分区总数的随机值，随机策略会导致消息分布不均匀。虽然是随机的，但是单个分区内也是有序的。</p><img src="/images/image-20200417035130292.png" alt="image-20200417035130292" style="zoom:67%;"><blockquote><p>策略代码</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line"><span class="keyword">return</span> ThreadLocalRandom.current().nextInt(partitions.size());</span><br></pre></td></tr></table></figure></li><li><p>key分配策略</p><p>这个策略也叫做 key-ordering策略，kafka中每条消息都会有自己的key，一旦消息被定义了 key，那么你就可以保证同一个key的所有消息都进入到相同的partition里面，因为每个partition下的消息处理都是有顺序的，所以这个策略也被称为按消息键保序策略</p><img src="/images/image-20200417035625713.png" alt="image-20200417035625713" style="zoom:60%;"><blockquote><p>策略代码</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line"><span class="comment">// Math.abs()的原因是hashCode可能是负数</span></span><br><span class="line"><span class="keyword">return</span> Math.abs(key.hashCode()) % partitions.size();</span><br></pre></td></tr></table></figure></li><li><p>自定义分配策略</p><p>自由发挥吧，只要实现Partitioner接口就成了</p><blockquote><p>application.properties</p></blockquote><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># org.apache.kafka.clients.producer.ProducerConfig类中定义了各类参数配置信息</span></span><br><span class="line"><span class="meta">spring.kafka.properties.partitioner.class</span>=<span class="string">cc.kevinlu.springboot.kafka.partitioners.CcPartitioner</span></span><br></pre></td></tr></table></figure><blockquote><p>CcPartitioner</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cc.kevinlu.springboot.kafka.partitioners;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Partitioner;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.Cluster;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.extern.slf4j.Slf4j;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Slf</span>4j</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CcPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (log.isDebugEnabled()) &#123;</span><br><span class="line">            log.debug(<span class="string">"&#123;&#125;------------&#123;&#125;"</span>, topic, cluster.availablePartitionsForTopic(topic).size());</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="comment">// 永远都打到partition 0上</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h3 id="producer-property"><a class="markdownIt-Anchor" href="#producer-property"></a> Producer Property</h3><ol><li>retries：消息重试次数，若消息发送过程中出现错误，但是可通过重新发送来弥补错误，比如Leader缺失，则生产者会不断的重发消息，直到重发次数达到此参数指定的值后放弃重试并返回错误，默认情况下每次重试间隔100ms，通过参数<code>retry.backoff.ms</code>指定</li><li>acks：指定要有多少个partition副本接收消息，生产者才认为消息是成功写入，acks能够控制消息丢失概率。<ul><li>acks=0：表示生产者只管发不管服务器是否接收了，非常容易丢消息</li><li>acks=1：只要集群的Leader收到了消息就立刻反馈给生产者，消息可能会丢失</li><li>acks=all：只有当所有的参与复制的节点都接收到消息时，broker才会反馈给生产者，能够保证消息绝不丢失，但是延迟更高</li></ul></li><li>key.serializer：key的序列化类，需是接口<code>org.apache.kafka.common.serialization.Serializer</code>的实现类</li><li>value.serializer：value的序列化类，需是接口<code>org.apache.kafka.common.serialization.Serializer</code>的实现类</li><li>compression.type：消息压缩类型，默认为none, 可选值有none、gzip、snappy、lz4、zstd</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;简&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#简&quot;&gt;&lt;/a&gt; 简&lt;/h3&gt;
&lt;p&gt;在kafka中把产生消息的一方称为生产者（Producer），尽管消息的产生非常简单，但是消息的发送过程比较复杂&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/
      
    
    </summary>
    
    
      <category term="Kafka" scheme="http://luxiaowan.github.io/categories/Kafka/"/>
    
    
  </entry>
  
  <entry>
    <title>Git切换回某个commit</title>
    <link href="http://luxiaowan.github.io/2020/04/16/Git%E5%88%87%E6%8D%A2%E5%9B%9E%E6%9F%90%E4%B8%AAcommit/"/>
    <id>http://luxiaowan.github.io/2020/04/16/Git切换回某个commit/</id>
    <published>2020-04-16T15:45:00.000Z</published>
    <updated>2020-04-16T16:00:43.558Z</updated>
    
    <content type="html"><![CDATA[<h3 id="原由"><a class="markdownIt-Anchor" href="#原由"></a> 原由</h3><p>commit之后忘了push，然后就revert HEAD了，导致本地的代码丢失了刚修改的内容</p><h3 id="第一步"><a class="markdownIt-Anchor" href="#第一步"></a> 第一步</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git log</span><br></pre></td></tr></table></figure><p>使用该命令查看commit记录</p><img src="/images/image-20200416235542502.png" alt="git commit log" style="zoom:50%;"><p>格式<code>commit commit_id</code>，比如<code>commit bc208f03c3bb341dfc56533d9ea196b6d347ff34</code>中，bc208f03c3bb341dfc56533d9ea196b6d347ff34就是commit_id，每一次commit的id都是全局唯一的</p><h3 id="第二步"><a class="markdownIt-Anchor" href="#第二步"></a> 第二步</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset --hard $&#123;commit_id&#125;</span><br></pre></td></tr></table></figure><p>若想切换回jmm这次的commit，则语句为<code>git reset --hard 81fc9404e8186d132c799ffaf62e652a4c8c98f0</code></p><h3 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h3><p>git操作要慎重，不过即使出了问题也有恢复的小技巧</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;原由&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#原由&quot;&gt;&lt;/a&gt; 原由&lt;/h3&gt;
&lt;p&gt;commit之后忘了push，然后就revert HEAD了，导致本地的代码丢失了刚修改的内容&lt;/p&gt;
&lt;h3 id=&quot;第一步&quot;&gt;&lt;a clas
      
    
    </summary>
    
    
      <category term="Git" scheme="http://luxiaowan.github.io/categories/Git/"/>
    
    
  </entry>
  
  <entry>
    <title>Kafka初探</title>
    <link href="http://luxiaowan.github.io/2020/04/16/Kafka%E5%88%9D%E6%8E%A2/"/>
    <id>http://luxiaowan.github.io/2020/04/16/Kafka初探/</id>
    <published>2020-04-15T18:51:00.000Z</published>
    <updated>2020-04-16T16:00:26.695Z</updated>
    
    <content type="html"><![CDATA[<h3 id="基本名词"><a class="markdownIt-Anchor" href="#基本名词"></a> 基本名词</h3><ul><li>消息：kafka中的数据单元称为消息，也可以叫记录，相当于MySQL表中的一条记录</li><li>批次：为了提高效率，kafka可以一次性写入一批数据(消息)，批次指的就是一组消息</li><li>主题：相当于MySQL的表，一个主题(Topic)代表着一类消息，kafka使用主题对消息分类</li><li>分区：分区(partition)归属于主题，一个主题可以划分为若干个分区，分区可以分布在不同的broker上，也可以在同一个broker上，使用分区来实现kafka的伸缩性。主题的单个分区上的消息是有序的，但是不同分区上的消息无法保证有序。</li><li>生产者：向主题发布消息的客户端称为生产者，生产者用于不断的向主题发送消息</li><li>消费者：订阅主题消息的客户端称为消费者，消费者用于处理生产者生产的消息</li><li>消费者群组：生产者与消费者的关系是一对多，比如一个客服对应多个咨询者，消费者群组就是由一批消费者组成的</li><li>偏移量：偏移量(Consumer Offset)是一种源数据，是一个单向递增的整数标识，用于记录消费者发生重平衡时的位置，以便用来恢复数据</li><li>broker：一个独立的服务器被称为broker，broker接收来自生产者的消息，并为消息设置偏移量，并提交消息持久化到磁盘</li><li>broker集群：多个broker组成一个集群，保证kafka的高可用，每个集群中都有一个broker充当集群Leader的角色</li><li>副本：kafka中消息的备份又称为副本(Replica)，副本的数量是可配置的，类型有Leader和Follower两种，Leader对外提供服务，Follower辅助</li><li>重(chong)平衡(ReBalance)：若消费者组内某个消费者宕了，其他存活的消费者自动重新分配订阅主题分区，kafka高可用的必备能力</li></ul><h3 id="关系介绍"><a class="markdownIt-Anchor" href="#关系介绍"></a> 关系介绍</h3><ul><li><p>Topic&amp;Partition</p><ol><li><p>Topic是kafka中给消息分类的标记，一个消息必定属于一个Topic，一个Topic可以包括一个或多个Partition，Partition又可以有多个副本，副本又可以分配在不同的broker上。</p></li><li><p>Partition内部是有序的，Partition之间是无序的</p><img src="/images/log_anatomy.png" alt="img" style="zoom: 130%;"></li><li><p>内部存储是以append-log的方式不断进行log文件尾部追加，文件读写是在磁盘上是顺序的，效率极高，媲美内存操作，每一条log对应一个offset，可以把offset理解为一个数组的下标，通过这个下标就可以读取对应的消息数据，Partition只负责为消息分配offset，消费者具体由哪个offset开始消费消息完全由消费者自己控制，也就是kafka服务端只负责提供数据，消费者自己控制消息消费进度。</p><img src="/images/log_consumer.png" alt="img" style="zoom: 25%;"></li><li><p>kafka虽然是可以持久化消息，并且不删除已经被消费过的消息，但消息也不是被永久存储在磁盘上的，为了防止磁盘长期被消息写入数据日积月累，kafka提供两种旧数据淘汰策略：</p></li></ol><ul><li><p>开启数据清理：<code>log.cleaner.enable=true</code>，默认关闭状态</p></li><li><p>基于时间：<code>log.retention.hours=168</code>，单位：小时；<code>log.retention.ms=100</code>，单位：毫秒；<code>log.retention.minutes</code>，单位：分钟</p></li><li><p>基于文件大小：<code>log.retention.bytes=1073741824</code>，单位：字节</p></li></ul></li><li><p>Consumer&amp;Consumer Group</p><ol><li><p>一个消费者组由一个或多个消费者组合而成，每一条消息只会被同一个group中的一个消费者消费，但是不同group中的消费者可以同时消费同一条消息，保证了 消息队列中的消息只被消费一次；kafka是发布订阅模式的消息队列，这里订阅的是消费者组，而不是特定的一个消费者实例。</p><p><img src="/images/consumer-groups.png" alt="img"></p><p>kafka支持离线处理和实时处理，所以我们可以使用Hadoop进行离线处理，也可以使用Storm这种实时流处理系统进行实时处理，还可以将数据实时的同步到其他的数据中心，前提是这些消费者处于不同的消费者组。</p><p>可以测试一下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1. 创建一个topic</span></span><br><span class="line">kafka-topics --create --zookeeper localhost:2181 --replication-factor 3 --partition 1 --topic cc_topic</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2. 启动一个Producer</span></span><br><span class="line">kafka-console-producer --broker-list localhost:9092 --topic cc_topic</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3. 启动五个Consumer，1~3号放cc_group_1，4~5放cc_group_2</span></span><br><span class="line">kafka-console-consumer --bootstrap-server localhost:9092 --group cc_group_1 --from-beginning --topic cc_topic</span><br><span class="line"></span><br><span class="line">kafka-console-consumer --bootstrap-server localhost:9092 --group cc_group_1 --from-beginning --topic cc_topic</span><br><span class="line"></span><br><span class="line">kafka-console-consumer --bootstrap-server localhost:9092 --group cc_group_1 --from-beginning --topic cc_topic</span><br><span class="line"></span><br><span class="line">kafka-console-consumer --bootstrap-server localhost:9092 --group cc_group_2 --from-beginning --topic cc_topic</span><br><span class="line"></span><br><span class="line">kafka-console-consumer --bootstrap-server localhost:9092 --group cc_group_2 --from-beginning --topic cc_topic</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4. 发送一条消息</span></span><br><span class="line">123</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 5. 查看消费者</span></span><br><span class="line">cc_group_1和cc_group_2各自收到123这条消息一次</span><br></pre></td></tr></table></figure></li></ol></li><li><p>Consumer ReBalance</p><p>Consumer ReBalance是通过Zookeeper实现，kafka保证了同一个消费者组中只能有一个消费者消费某条消息，其实kafka保证的是在稳定状态下每一个消费者都只会消费一个或多个Partition的消息，而某一Partition的消息仅会被一个消费者消费，这样设计的优势是每个消费者不用跟所有的broker进行通信，减少了通信开销，劣势是同一个消费组内的消费者不能均匀消费，而且单个Partition内部的数据是有序的，所以对于单个消费者来说，其消费的消息是有序的。</p><ol><li>Consumer &lt; Partition：会出现某些Consumer消费多个Partition的数据</li><li>Consumer &gt; Partition：会出现某些Consumer没有可消费的Partition</li><li>Consumer = Partition：一个Consumer消费一个Partition，均匀</li></ol></li></ul><h3 id="特性"><a class="markdownIt-Anchor" href="#特性"></a> 特性</h3><ul><li>高吞吐、低延迟：kafka处理消息的速度非常快，每秒几乎可以处理几十万条消息，并且最低延迟只有几毫秒</li><li>高伸缩性：每个topic都能有多个partition，每个partition又可以分布在不同的broker上</li><li>高并发：能够同时支持数千个客户端进行读写</li><li>容错性：允许集群中的某些节点失败，某个节点宕机，kafka仍然可用继续提供服务</li><li>持久性、可靠性：kafka的消息存储是基于Zookeeper的，Zookeeper是可以将消息持久化到磁盘上，并且支持数据备份，所以kafka是一个非常可靠的可持久化消息中间件</li><li>速度快：kafka采用零拷贝的模式实现数据的快速移动，避免了内核空间和用户空间的频繁切换，kafka可以批量发送数据，从生产者到文件系统到消费者；数据压缩可以通过有效的数据压缩减少IO次数，并且采用顺序读写的方式避免寻址造成的消耗。总结起来就是零拷贝、顺序读写、数据压缩、分批发送。</li></ul><h3 id="消息队列"><a class="markdownIt-Anchor" href="#消息队列"></a> 消息队列</h3><ul><li><p>点对点（一对一）：一个生产者所生产的消息只会被一个消费者进行消费，不会同时被多个消费者消费</p><img src="/images/image-20200416173858298.png" alt="image-20200416173858298" style="zoom:50%;"></li><li><p>发布订阅（一对多、多对多）：一个或多个生产者所生产的消息会被多个消费者同时消费</p><img src="/images/image-20200416173919621.png" alt="image-20200416173919621" style="zoom:50%;"></li></ul><h3 id="架构体系"><a class="markdownIt-Anchor" href="#架构体系"></a> 架构体系</h3><img src="/images/image-20200416220029587.png" alt="image-20200416220029587" style="zoom:45%;"><p>一个kafka集群包含若干个Producer、若干Consumer group、若干broker和Zookeeper集群组成，kafka通过Zookeeper管理Partition，选举Leader，以及在Consumer发生变化时通过Zookeeper进行ReBalance。Producer将消息push到Partition，Consumer通过pull将消息从Partition拉取到本地。</p><h3 id="api"><a class="markdownIt-Anchor" href="#api"></a> API</h3><p>kafka目前提供五类常用的API，主要有Producer、Consumer、Stream、Connect、Admin API：</p><ul><li>Producer API：允许App作为Producer将消息发送到kafka集群的一个或多个topic上</li><li>Consumer API：允许App作为Consumer从kafka集群上的一个或多个topic拉取消息</li><li>Stream API：允许App作为流处理器，从一个或多个topic中消费输入流并转化为输出流</li><li>Connector API：允许将现有的应用程序或存储系统连接到kafka的topic，充当Producer或Consumer</li><li>Admin API：允许管理和检查topic、broker和kafka的其他内容</li></ul><img src="/images/kafka-apis.png" alt="img" style="zoom:50%;"><h3 id="重要配置参数"><a class="markdownIt-Anchor" href="#重要配置参数"></a> 重要配置参数</h3><p>kafka的参数配置文件是server.properties</p><ul><li><a href="http://broker.id" target="_blank" rel="noopener">broker.id</a>：每个broker都有一个唯一标识，就像是MySQL表中的主键ID，默认值是0，这个值在kafka集群中必须是唯一不可重复的，值随意设置。</li><li>port：kafka broker的默认端口是9092，若未指定port参数，则就是9092，修改port参数可以是任意端口，但是最好不要低于1024，不然就需要管理员权限启动了</li><li>zookeeper.connect：设置broker源数据的Zookeeper地址，参数的值可以设置一个或多个，多个zk通过逗号分隔，比如<code>zk1:port,zk2:port,zk3:port</code>，不同的kafka集群可以使用同一个zk集群，可以通过指定zk的具体path来区分每个kafka的使用，比如kafka cluster1使用<code>zk:port/path1</code>，kafka cluster2使用<code>zk:port/path2</code>。</li><li><a href="http://zookeeper.connection.timeout.ms" target="_blank" rel="noopener">zookeeper.connection.timeout.ms</a>：设置broker连接Zookeeper的超时时间，单位是毫秒</li><li>log.dirs：kafka把所有的消息都保存在本地磁盘上，保存的日志地址通过该参数指定，可以指定多个存储目录，通过逗号分隔，例如<code>/home/kafka/1,/home/kafka/2,/home/kafka/3</code></li><li>auto.create.topic.enable：默认为true，允许随意的创建topic，参数为true时，使用Producer往一个不存在的topic发送消息时会自动创建topic、使用Consumer从一个不存在的topic拉取消息时自动创建topic、主动创建topic、当任意一个客户端向topic发送元数据请求时。此值建议在生产上设置为false，topic由人工进行分配，防止生产环境出现各种乱七八糟的topic。</li><li>topic相关参数<ul><li>num.partitions：主题拥有的Partition数量，若在创建topic的时候未指定分区数量，则使用该参数的值，默认为1。在运行过程中，分区数量可以增加不能减少，在创建时可以通过<code>--partition</code>指定个数</li><li>default.replication.factor：kafka消息的默认副本数，默认为1，只有在自动创建topic的时候才有效，在创建时可以通过<code>--replication-factor</code>指定</li><li>log.cleaner.enable：是否开启日志清理功能，默认为true，清理方式有时间和日志文件大小两种方式</li><li>log.retention.hours：设置kafka消息保存的时间，默认为168个小时，还可以通过<code>log.retention.ms</code>和<code>log.retention.minutes</code>来设置清理时间的毫秒和分钟时间</li><li>log.retention.bytes：设置topic的每个Partition所能保存的数据量，比如若一个topic有10个Partition，此参数的值为1G，那么该topic的最大存储容量为8G，topic的容量随着Partition的增加而增加。</li><li>log.segment.bytes：设置日志文件的最大的容量大小。当消息到达broker时，会被追加到日志文件中，但是如果日志片段的当前大小加上新接收消息的打小后超过了该参数设置的值，则将新消息和后续的消息写入到一个新的日志文件中。该参数的值越小，分割的文件就越多，磁盘的写入效率就越低。</li><li><a href="http://log.segment.ms" target="_blank" rel="noopener">log.segment.ms</a>：除了待日志文件大小超值后重新分配新文件之外，还可以通过日志创建时间来控制消息日志文件的生命周期，可以和<code>log.segment.bytes</code>同时设置，哪一个先达标使用哪一个策略，比如bytes设置为1G，ms设置为1小时，若30分钟内文件容量已达1G，则后续消息写入到新的日志文件中，若1小时内日志文件尚未达到1G，则也分配新的日志文件记录后续的消息。</li><li><a href="http://log.retention.check.interval.ms" target="_blank" rel="noopener">log.retention.check.interval.ms</a>：检查日志段以查看是否可以根据保留策略删除它们的时间间隔，单位：毫秒</li><li>message.max.bytes：该参数限定broker可接收的单个消息的大小，默认是1MB，如果Producer发送的消息大于此值，则broker会直接拒绝并返回错误。该参数指定的是压缩后的消息大小，消息的实际大小可能大于此值。</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;基本名词&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#基本名词&quot;&gt;&lt;/a&gt; 基本名词&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;消息：kafka中的数据单元称为消息，也可以叫记录，相当于MySQL表中的一条记录&lt;/li&gt;
&lt;li&gt;批次：为了提高效率
      
    
    </summary>
    
    
      <category term="Kafka" scheme="http://luxiaowan.github.io/categories/Kafka/"/>
    
    
  </entry>
  
  <entry>
    <title>Redis的穿透、击穿和雪崩</title>
    <link href="http://luxiaowan.github.io/2020/04/13/Redis%E7%9A%84%E7%A9%BF%E9%80%8F%E3%80%81%E5%87%BB%E7%A9%BF%E5%92%8C%E9%9B%AA%E5%B4%A9/"/>
    <id>http://luxiaowan.github.io/2020/04/13/Redis的穿透、击穿和雪崩/</id>
    <published>2020-04-13T14:23:00.000Z</published>
    <updated>2020-04-16T15:41:38.711Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h3><p>不论是我们在使用Redis还是准备面试，都逃不掉一块我们必须要考虑到的内容，也是使用Redis不精细的话必定会遇到的问题，就是缓存穿透、击穿和雪崩，这三个问题严重情况下，会使服务无法继续正常使用。从名字上来看，好像雪崩最严重，眼前突然浮现出电影《攀登者》里面的画面=。=</p><h3 id="缓存穿透"><a class="markdownIt-Anchor" href="#缓存穿透"></a> 缓存穿透</h3><p>缓存穿透是指访问一个DB和Redis中必定不存在的key，如果不对这类请求进行过滤拦截的话，请求每次都会穿过Redis直接打到DB上，并且我们一般是缓存中没数据的时候去DB中取，取出来之后再放到缓存中，但这类请求所需要的数据在DB中也不存在，所以即使请求打到DB上，最终缓存中还是没有数据，在这类请求特别多的情况下，DB很快就会被拖垮，引起服务异常。</p><blockquote><p>解决方案</p></blockquote><ol><li>布隆过滤器：我们可以在做事务型处理之后，将需要缓存的key放到布隆过滤器中，但是由于布隆过滤器只能保证可能存在，所以在使用过程中还是会有穿透的可能性存在，但概率极小，所以不用过多担心</li><li>短期null：此方案是在DB中查询不到数据的时候，就往Redis中设置一个短时间内就会过期的null值，比如30秒，1分钟等，不过时间还是要根据自己的业务性质来定。为什么要给不存在的key在缓存中设置一个null值？其实不一定是null，只要团队约定一个特殊字符即可，因为我们到数据库里取不出来数据，缓存里取个null（或者nil），也就代表了这个key不存在与数据库中</li></ol><h3 id="缓存雪崩"><a class="markdownIt-Anchor" href="#缓存雪崩"></a> 缓存雪崩</h3><p>大量的key在同一时刻同时失效，这些key并不一定是设置了相同的时间，也可能是凑巧时间累计在一起了，恰巧大量的针对这些失效的key的请求在同一时间大量的打了进来，这时缓存全部未命中，所有的请求都透传到DB上，引起DB压力瞬间扩大数倍，极易导致DB因负载过高而崩溃，危害极大。</p><blockquote><p>解决方案</p></blockquote><ol><li>加锁：对访问的key进行加锁，同时只放一个请求透传到DB，从一定程度上缓解了DB的压力，这也是缓存击穿的一种解决方案</li><li>队列：所有的请求全部塞入到队列中，依次打到DB上，这种方式能解决DB的压力，但是会给请求处理效率带来一些延迟</li><li>随机过期时间：给key设置过期时间时，在原过期时间的基础上加一个随机时间，比如3000毫秒以内的随机数，这样过期时间重复或者累计重复的可能性降低了很多，不太容易引起大量的key同时失效，并且成本较低。</li></ol><h3 id="缓存击穿"><a class="markdownIt-Anchor" href="#缓存击穿"></a> 缓存击穿</h3><p>缓存雪崩是说的大量的key，缓存击穿说的是某一个热点key，也就是在某些时间点会被超高的并发访问。key在某个时间点过期的时候，恰好在对这个Key有大量的并发请求过来，缓存中无法命中则会把请求全部打到DB端，如此大量的请求可能会瞬间把DB压爆。</p><blockquote><p>解决方案</p></blockquote><ol><li><p>临时加锁：对key加上互斥锁，若缓存中命中不了的时候，先给这个key设置一个锁（SETNX），锁的过期时间要非常简短，只有加锁成功的线程才透传到DB，加载完数据后set到缓存中，并释放锁</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">function get(key):</span><br><span class="line">  var v = redis.get(key)</span><br><span class="line">  <span class="keyword">if</span> v eq null:</span><br><span class="line">    <span class="keyword">if</span> setnx(key_lock, val, timeout):</span><br><span class="line">      var v = db.get(key)</span><br><span class="line">      redis.set(key, v, timeout)</span><br><span class="line">      release(key_lock)</span><br><span class="line">      <span class="keyword">return</span> v</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      sleep(<span class="number">30</span>)</span><br><span class="line">      <span class="keyword">return</span> get(key)</span><br><span class="line"> <span class="keyword">else</span>:</span><br><span class="line">  <span class="keyword">return</span> v;</span><br></pre></td></tr></table></figure></li><li><p>同步锁：对于热点key，set缓存的时候同时set一个针对这个key的监视key，监视key的过期时间一定要小于被监视的key，每次获取缓存数据的时候都获取一下这个监视key，并判断监视key是否过期了，如果过期了，则重置一下key的过期时间，并重新设置这个监视key</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">function get(key):</span><br><span class="line">  var v = redis.get(key)</span><br><span class="line">  <span class="keyword">if</span> v eq null:</span><br><span class="line">    <span class="keyword">if</span> setnx(key_lock, val, timeout):</span><br><span class="line">      var v = db.get(key)</span><br><span class="line">      redis.set(key, v, timeout)</span><br><span class="line">      redis.set(key_monitor, now() + timeout - time, timeout - time)</span><br><span class="line">      release(key_lock)</span><br><span class="line">      <span class="keyword">return</span> v</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      sleep(<span class="number">30</span>)</span><br><span class="line">      <span class="keyword">return</span> get(key)</span><br><span class="line"> <span class="keyword">else</span>:</span><br><span class="line">  var vm = redis.get(key_monitor)</span><br><span class="line">  <span class="keyword">if</span> now() - vm lt <span class="number">10</span>:</span><br><span class="line">    redis.expire(key ,timeout)</span><br><span class="line">    reids.expire(key_monitor, now() + timeout - time)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure><h3 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h3><p>若是想更好的使用Redis，那么穿透、击穿、雪崩必定是要慎重考虑的东西，解决方案有多种，应根据自己的实际业务做更优选择</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前言&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#前言&quot;&gt;&lt;/a&gt; 前言&lt;/h3&gt;
&lt;p&gt;不论是我们在使用Redis还是准备面试，都逃不掉一块我们必须要考虑到的内容，也是使用Redis不精细的话必定会遇到的问题，就是缓存穿透、击穿和雪
      
    
    </summary>
    
    
      <category term="Redis" scheme="http://luxiaowan.github.io/categories/Redis/"/>
    
    
  </entry>
  
  <entry>
    <title>Java内存模型JMM</title>
    <link href="http://luxiaowan.github.io/2020/04/13/Java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8BJMM/"/>
    <id>http://luxiaowan.github.io/2020/04/13/Java内存模型JMM/</id>
    <published>2020-04-12T17:11:00.000Z</published>
    <updated>2020-04-12T17:13:19.959Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/jmm.png" alt="JMM"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/images/jmm.png&quot; alt=&quot;JMM&quot;&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Java" scheme="http://luxiaowan.github.io/categories/Java/"/>
    
    
  </entry>
  
  <entry>
    <title>查看字节码Javap</title>
    <link href="http://luxiaowan.github.io/2020/04/13/%E6%9F%A5%E7%9C%8B%E5%AD%97%E8%8A%82%E7%A0%81javap/"/>
    <id>http://luxiaowan.github.io/2020/04/13/查看字节码javap/</id>
    <published>2020-04-12T16:45:00.000Z</published>
    <updated>2020-04-12T16:51:46.838Z</updated>
    
    <content type="html"><![CDATA[<h3 id="代码"><a class="markdownIt-Anchor" href="#代码"></a> 代码</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JavaP</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    String s1 = <span class="string">"Hello"</span>;</span><br><span class="line">    String s2 = <span class="string">"Hel"</span> + <span class="string">"lo"</span>;</span><br><span class="line">    String s3 = <span class="keyword">new</span> String(<span class="string">"Hello"</span>);</span><br><span class="line">    String s4 = <span class="string">"Hel"</span> + <span class="keyword">new</span> String(<span class="string">"lo"</span>);</span><br><span class="line">    String s5 = s3.intern();</span><br><span class="line">    <span class="keyword">int</span> i1 = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> i2 = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    System.out.println(s1 == s2);</span><br><span class="line">    System.out.println(s1 == s3);</span><br><span class="line">    System.out.println(s3 == s4);</span><br><span class="line">    System.out.println(s2 == s4);</span><br><span class="line">    System.out.println(s1 == s2);</span><br><span class="line">    System.out.println(s2 == s5);</span><br><span class="line">    System.out.println(i1 == i2);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="查看字节码"><a class="markdownIt-Anchor" href="#查看字节码"></a> 查看字节码</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">javap -v JavaP.class</span><br></pre></td></tr></table></figure><h3 id="字节码"><a class="markdownIt-Anchor" href="#字节码"></a> 字节码</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">Constant pool:（常量池）</span><br><span class="line"><span class="meta">#</span><span class="bash">2 = String <span class="comment">#36 // Hello</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">3 = Class <span class="comment">#37 // java/lang/String</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">5 = Class <span class="comment">#39 // java/lang/StringBuilder</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">7 = String <span class="comment">#40 // Hel</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">9 = String <span class="comment">#42 // lo</span></span></span><br><span class="line">&#123;</span><br><span class="line">stack=4, locals=8, args_size=1</span><br><span class="line">0: ldc #2 // String Hello （加载常量池的#2号字符串压入栈）</span><br><span class="line">2: astore_1 (将上一步加载的引用变量赋值给变量1，即s1【参照LocalVariableTable的slot数值】)</span><br><span class="line">3: ldc #2 // String Hello （加载常量池的#2号字符串压入栈）</span><br><span class="line">5: astore_2 (将上一步加载的引用变量赋值给变量2，即s2【参照LocalVariableTable的slot数值】)</span><br><span class="line">6: new #3 // class java/lang/String (创建#3号的实例：java/lang/String)</span><br><span class="line">9: dup （复制上一步创建的对象的引用压入栈）</span><br><span class="line">10: ldc #2 // String Hello （加载常量池的#2号字符串压入栈）</span><br><span class="line">12: invokespecial #4 // Method java/lang/String."&lt;init&gt;":(Ljava/lang/String;)V (调用String的init方法，将上一步加载入栈的引用作为参数传入init方法)</span><br><span class="line">15: astore_3 （将上一步返回的引用变量赋值给变量3，即s3【参照LocalVariableTable的slot数值】)</span><br><span class="line">16: new #5 // class java/lang/StringBuilder（创建#5号的实例：java/lang/StringBuilder）</span><br><span class="line">19: dup （复制上一步创建的对象引用压入栈）</span><br><span class="line">20: invokespecial #6 // Method java/lang/StringBuilder."&lt;init&gt;":()V （调用StringBuilder的init方法）</span><br><span class="line">23: ldc #7 // String Hel （加载常量池#7号字符串压入栈）</span><br><span class="line">25: invokevirtual #8 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder;（调用StringBuilder的append方法，将上一步加载的引用作为参数传入append方法）</span><br><span class="line">28: new #3 // class java/lang/String （创建#3号的实例：java/lang/String）</span><br><span class="line">31: dup （复制上一步创建的对象的引用压入栈）</span><br><span class="line">32: ldc #9 // String lo （加载常量池#9号字符串压入栈）</span><br><span class="line">34: invokespecial #4 // Method java/lang/String."&lt;init&gt;":(Ljava/lang/String;)V （调用String的init方法，将上一步加载入栈的引用作为参数传入init方法）</span><br><span class="line">37: invokevirtual #8 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder;（调用StringBuilder的append方法，将上一步加载的引用作为参数传入append方法）</span><br><span class="line">40: invokevirtual #10 // Method java/lang/StringBuilder.toString:()Ljava/lang/String;（调用StringBuilder的toString方法）</span><br><span class="line">43: astore 4 （将上一步返回的引用赋值给变量4，即s4【参照LocalVariableTable的slot数值】）</span><br><span class="line">45: aload_3 （加载变量3压入栈）</span><br><span class="line">46: invokevirtual #11 // Method java/lang/String.intern:()Ljava/lang/String; （调用String的intern方法）</span><br><span class="line">49: astore 5 （将上一步返回的引用赋值给变量5，即s5【参照LocalVariableTable的slot数值】）</span><br><span class="line">51: iconst_0 （将数字0压入栈）</span><br><span class="line">52: istore 6 （将上一步加载的数字赋值给变量6，即i1【参照LocalVariableTable的slot数值】）</span><br><span class="line">54: iconst_1 （将数字1压入栈）</span><br><span class="line">55: istore 7 （将上一步加载的数字赋值给变量7，即i2【参照LocalVariableTable的slot数值】）</span><br><span class="line">57: getstatic #12 // Field java/lang/System.out:Ljava/io/PrintStream; （调用System的静态方法out）</span><br><span class="line">60: aload_1 （加载变量1，即s1）</span><br><span class="line">61: aload_2 （加载变量2，即s2）</span><br><span class="line">62: if_acmpne 69（比较s1和s2是否不相等，如果不相等，跳转到63行指令，如果相等则继续下一步，if_acmpne和if_icmpne，a表示对象引用比较，I表示数字比较；if_acmpeq和if_acmpne，eq【equal】表示相等，ne【not equal】）</span><br><span class="line">65: iconst_1 （将数字1压入栈）</span><br><span class="line">66: goto 70 （跳转到64行指令）</span><br><span class="line">69: iconst_0 （将数字0压入栈）</span><br><span class="line">70: invokevirtual #13 // Method java/io/PrintStream.println:(Z)V</span><br><span class="line">……</span><br><span class="line">156: getstatic #12 // Field java/lang/System.out:Ljava/io/PrintStream;</span><br><span class="line">159: iload 6 （加载变量6，即i1）</span><br><span class="line">161: iload 7 （加载变量7，即i2）</span><br><span class="line">163: if_icmpne 170（比较i1和i2）</span><br><span class="line">166: iconst_1</span><br><span class="line">167: goto 171</span><br><span class="line">170: iconst_0</span><br><span class="line">171: invokevirtual #13 // Method java/io/PrintStream.println:(Z)V</span><br><span class="line">……</span><br><span class="line">150: return</span><br><span class="line">LocalVariableTable:</span><br><span class="line">Start Length Slot Name Signature</span><br><span class="line">0 151 0 args [Ljava/lang/String;</span><br><span class="line">3 148 1 s1 Ljava/lang/String;</span><br><span class="line">6 145 2 s2 Ljava/lang/String;</span><br><span class="line">16 135 3 s3 Ljava/lang/String;</span><br><span class="line">45 106 4 s4 Ljava/lang/String;</span><br><span class="line">51 100 5 s5 Ljava/lang/String;</span><br><span class="line">54 121 6 i1 I</span><br><span class="line">57 118 7 i2 I</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;代码&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#代码&quot;&gt;&lt;/a&gt; 代码&lt;/h3&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span cla
      
    
    </summary>
    
    
      <category term="Java" scheme="http://luxiaowan.github.io/categories/Java/"/>
    
    
  </entry>
  
  <entry>
    <title>GC日志分析</title>
    <link href="http://luxiaowan.github.io/2020/04/13/GC%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"/>
    <id>http://luxiaowan.github.io/2020/04/13/GC日志分析/</id>
    <published>2020-04-12T16:40:00.000Z</published>
    <updated>2020-04-12T16:46:14.671Z</updated>
    
    <content type="html"><![CDATA[<h3 id="vm参数"><a class="markdownIt-Anchor" href="#vm参数"></a> VM参数</h3><p>1、在控制台打印出每次GC信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-XX:+PrintGCDetails</span><br></pre></td></tr></table></figure><p>2、在发生OOM时打印出堆栈信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/Users/chuan/</span><br></pre></td></tr></table></figure><h3 id="oom代码"><a class="markdownIt-Anchor" href="#oom代码"></a> OOM代码</h3><p>发生OOM测试程序代码，也可以自行编写，运行VM参数（<code>-Xms1m -Xmx1m -XX:+PrintGCDetails -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/Users/chuan/</code>）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OOMDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> K = <span class="number">1024</span>;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> M = K * K;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> G = K * M;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> ALIVE_OBJECT_SIZE = <span class="number">32</span> * M;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> length = ALIVE_OBJECT_SIZE / <span class="number">64</span>;</span><br><span class="line">    ObjectOf64Bytes[] array = <span class="keyword">new</span> ObjectOf64Bytes[length];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">long</span> i = <span class="number">0</span>; i &lt; G; i++) &#123;</span><br><span class="line">    array[(<span class="keyword">int</span>) (i % length)] = <span class="keyword">new</span> ObjectOf64Bytes();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ObjectOf64Bytes</span> </span>&#123;</span><br><span class="line">  <span class="keyword">long</span> placeholder0;</span><br><span class="line">  <span class="keyword">long</span> placeholder1;</span><br><span class="line">  <span class="keyword">long</span> placeholder2;</span><br><span class="line">  <span class="keyword">long</span> placeholder3;</span><br><span class="line">  <span class="keyword">long</span> placeholder4;</span><br><span class="line">  <span class="keyword">long</span> placeholder5;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="minor-gc日志"><a class="markdownIt-Anchor" href="#minor-gc日志"></a> Minor GC日志</h3><p><code>[GC --[PSYoungGen: 447K-&gt;447K(1024K)] 709K-&gt;956K(1536K), 0.0031459 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]</code></p><p>下面来解析这一段信息，其实很简单，不用硬记，按顺序从左到右：</p><ul><li><p>GC：表示这是发生了GC</p></li><li><p>PSYoungGen：GC类型，此处表示新生代内存发生了GC；</p></li><li><p>447K-&gt;447K：-&gt;前面的447K表示GC前新生代内存占用，-&gt;后面的447K表示GC后新生代内存占用（此处是分毫未被回收，那你执行个骡子，浪费 0.0031459秒）；</p></li><li><p>(1024K)：1024K表示新生代总共大小，看得出来是1M，可以回头看一下VM参数配置；</p></li><li><p>709K-&gt;956K：709K表示GC前JVM堆内存使用，956表示GC后JVM堆内存使用（硌老子的，GC后还变多了）；</p></li><li><p>(1536K):1536K表示JVM堆内存总大小；</p></li><li><p>0.0031459 secs：本次GC耗时（看得出还是很快的）；</p></li><li><p>user：GC用户耗时；</p></li><li><p>sys：GC系统耗时；</p></li><li><p>real：GC实际耗时。</p></li></ul><h3 id="full-gc日志"><a class="markdownIt-Anchor" href="#full-gc日志"></a> Full GC日志</h3><p><code>[Full GC [PSYoungGen: 447K-&gt;197K(1024K)] [ParOldGen: 508K-&gt;494K(512K)] 956K-&gt;692K(1536K), [Metaspace: 3179K-&gt;3179K(1056768K)], 0.0069504 secs] [Times: user=0.00 sys=0.00, real=0.01 secs]</code></p><ul><li><p>Full GC：GC类型；</p></li><li><p>[PSYoungGen: 447K-&gt;197K(1024K)] ：Young区，447K表示GC前Young区内存占用，197K表示GC后Young区内存占用，1024K表示Young区总大小；</p></li><li><p>[ParOldGen: 508K-&gt;494K(512K)] ：Old区，508K表示GC前Old区内存占用，494K表示GC后Old区内存占用，512K表示Old区总大小；</p></li><li><p>956K-&gt;692K(1536K)：956K表示GC前JVM堆内存使用，692K表示GC后JVM堆内存使用，1536K表示JVM堆内存总大小；</p></li><li><p>[Metaspace: 3179K-&gt;3179K(1056768K)]：元空间，3179K表示GC前后元空间的使用，1056768K表示元空间的总大小（使用VM参数 -XX:-UseCompressedClassPointers -XX:MetaspaceSize=50M指定元空间的总大小）；</p></li><li><p>0.0069504 secs：本次GC耗时（看得出还是很快的）；</p></li><li><p>user：GC用户耗时；</p></li><li><p>sys：GC系统耗时；</p></li><li><p>real：GC实际耗时。</p></li></ul><blockquote><p>规律</p><p>[名称:GC前内存-&gt;GC后内存(总内存)]</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Heap</span><br><span class="line">PSYoungGen total 2560K, used 60K [0x00000000ffd00000, 0x0000000100000000, 0x0000000100000000)</span><br><span class="line">eden space 2048K, 2% used [0x00000000ffd00000,0x00000000ffd0f038,0x00000000fff00000)</span><br><span class="line">from space 512K, 0% used [0x00000000fff00000,0x00000000fff00000,0x00000000fff80000)</span><br><span class="line">to space 512K, 0% used [0x00000000fff80000,0x00000000fff80000,0x0000000100000000)</span><br><span class="line">ParOldGen total 7168K, used 736K [0x00000000ff600000, 0x00000000ffd00000, 0x00000000ffd00000)</span><br><span class="line">object space 7168K, 10% used [0x00000000ff600000,0x00000000ff6b8128,0x00000000ffd00000)</span><br><span class="line">Metaspace used 3328K, capacity 4112K, committed 4352K, reserved 8192K</span><br></pre></td></tr></table></figure><p>解析上述：</p><p>PSYoungGen total = eden space + from space，内存地址eden+from+to</p><p>ParOldGen GC前后老年代的内存变化</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;vm参数&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#vm参数&quot;&gt;&lt;/a&gt; VM参数&lt;/h3&gt;
&lt;p&gt;1、在控制台打印出每次GC信息：&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;t
      
    
    </summary>
    
    
      <category term="Java" scheme="http://luxiaowan.github.io/categories/Java/"/>
    
    
  </entry>
  
  <entry>
    <title>MySQL连接查询</title>
    <link href="http://luxiaowan.github.io/2020/04/12/MySQL%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2/"/>
    <id>http://luxiaowan.github.io/2020/04/12/MySQL连接查询/</id>
    <published>2020-04-12T15:36:00.000Z</published>
    <updated>2020-04-12T17:08:48.838Z</updated>
    
    <content type="html"><![CDATA[<h3 id="左连接"><a class="markdownIt-Anchor" href="#左连接"></a> 左连接</h3><p>左连接基本格式为<code>A left join B on A.key=B.key</code>，比如以下语句：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from A left join B on A.id=B.id;</span><br></pre></td></tr></table></figure><p>如果A、B表的数据结构为：</p><table><thead><tr><th><code>table A</code></th><th><code>table B</code></th></tr></thead><tbody><tr><td>id, name</td><td>id, name</td></tr><tr><td>1, xiaolu</td><td>1, xiaolu</td></tr><tr><td>2, chuanchuan</td><td>3, chuanchuan</td></tr></tbody></table><p>这时执行上述语句会得出如下结果：</p><table><thead><tr><th><code>A.id</code></th><th><code>A.name</code></th><th><code>B.id</code></th><th><code>B.name</code></th></tr></thead><tbody><tr><td>1</td><td>Xiaolu</td><td>1</td><td>Xiaolu</td></tr><tr><td>2</td><td>chuanchuan</td><td>null</td><td>null</td></tr></tbody></table><p>由上述结果可以看出，左连接是以左表为坐标，首先将A表中所有的数据列出来，然后根据on的匹配条件查出B表中的数据并将数据列在A表数据后面，如果在B表中没有与A表中匹配的数据，则显示为null，查询出的总数据数为A表中的数据条目个数。</p><h3 id="右连接"><a class="markdownIt-Anchor" href="#右连接"></a> 右连接</h3><p>右连接基本格式为<code>A right join B on A.key=B.key</code>，比如以下语句：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from A right join B on A.id=B.id;</span><br></pre></td></tr></table></figure><p>如果A、B表的数据结构为：</p><table><thead><tr><th><code>table A</code></th><th><code>table B</code></th></tr></thead><tbody><tr><td>id, name</td><td>id, name</td></tr><tr><td>1, xiaolu</td><td>1, xiaolu</td></tr><tr><td>2, chuanchuan</td><td>3, chuanchuan</td></tr></tbody></table><p>这时执行上述语句会得出如下结果：</p><table><thead><tr><th><code>B.id</code></th><th><code>B.name</code></th><th><code>A.id</code></th><th><code>A.name</code></th></tr></thead><tbody><tr><td>1</td><td>xiaolu</td><td>1</td><td>xiaolu</td></tr><tr><td>null</td><td>null</td><td>3</td><td>chuanchuan</td></tr></tbody></table><p>由上述结果可以看出，左连接是以左表为坐标，首先将B表中所有的数据列出来，然后根据on的匹配条件查出A表中的数据并将A表的数据列在B表的前面，如果在A表中没有与B表中匹配的数据，则显示为null，查询出的总数据数为B表中的数据条目个数。</p><h3 id="内链接"><a class="markdownIt-Anchor" href="#内链接"></a> 内链接</h3><p>内连接基本格式为<code>A inner join B on A.key=B.key</code>，比如以下语句：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from A inner join B on A.id=B.id;</span><br></pre></td></tr></table></figure><p>如果A、B表的数据结构为：</p><table><thead><tr><th><code>table A</code></th><th><code>table B</code></th></tr></thead><tbody><tr><td>id, name</td><td>id, name</td></tr><tr><td>1, xiaolu</td><td>1, xiaolu</td></tr><tr><td>2, chuanchuan</td><td>3, chuanchuan</td></tr></tbody></table><p>这时执行上述语句会得出如下结果：</p><table><thead><tr><th><code>B.id</code></th><th><code>B.name</code></th><th><code>A.id</code></th><th><code>A.name</code></th></tr></thead><tbody><tr><td>1</td><td>xiaolu</td><td>1</td><td>xiaolu</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;左连接&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#左连接&quot;&gt;&lt;/a&gt; 左连接&lt;/h3&gt;
&lt;p&gt;左连接基本格式为&lt;code&gt;A left join B on A.key=B.key&lt;/code&gt;，比如以下语句：&lt;/p&gt;
&lt;figure
      
    
    </summary>
    
    
      <category term="MySQL" scheme="http://luxiaowan.github.io/categories/MySQL/"/>
    
    
  </entry>
  
  <entry>
    <title>MySQL联合索引底层数据结构</title>
    <link href="http://luxiaowan.github.io/2020/04/12/MySQL%E8%81%94%E5%90%88%E7%B4%A2%E5%BC%95%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    <id>http://luxiaowan.github.io/2020/04/12/MySQL联合索引底层数据结构/</id>
    <published>2020-04-12T07:36:00.000Z</published>
    <updated>2020-04-12T10:39:05.585Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h3><p>了解MySQL索引结构的基本都知道索引BTree类型是用B+树的数据结构，单列索引的结构我们很容易理解，二级索引的每个叶子节点只存储主键关键字外的一个数据，查询起来也很容易在非叶子节点进行大小值判断，最终找到叶子节点</p><p><img src="/images/image-20200409120534155.png" alt="单列索引"></p><p>对于多列组合索引，存储结构也是B+树，那么非叶子节点和叶子节点都存储的是什么内容？</p><h3 id="二级组合索引"><a class="markdownIt-Anchor" href="#二级组合索引"></a> 二级组合索引</h3><p>对于组合索引，需要遵循断桥原则(最左匹配原则)，例如(a, b,)可以满足a，a、b，我们根据这个原则反推一下二级组合索引的存储规则：</p><ol><li>叶子节点应该是线性排列，并且每个节点的数据排列顺序和创建索引字段的顺序一致</li><li>叶子节点排列顺序应该是先按照a进行排序，排序完成后再按照b进行排序，所以应该是a是全局有序，b是a中有序，如果列数更多的情况下，下一列都相对于前列有序。</li><li>非叶子节点存储完整的索引关键字信息，排列规则和叶子节点一致</li><li>整体查询使用二分法</li></ol><p>根据上述推断，我们基本可以判定二级组合索引的数据结构图了</p><p><img src="/images/image-20200412180233754.png" alt="image-20200412180233754"></p><p>上面我们进行了规则反推，也根据反推总结出了简单的组合索引数据结构图，那么我们来验证一下上述推论：</p><ol><li>因为索引遵循断桥原则，B+树是顺序且极限二分查找的方式进行遍历，所以在进行B+树遍历的时候从左到右进行匹配，并且我们创建索引的目的是为了提升查询效率，如果每个节点中数据和索引的字段顺序不一致，各执己见，那么在查询的时候还要判断当前节点的某位数据对应了索引的哪个字段，效率会更低，所以推论1是可信可靠的。</li><li>根据图中叶子节点的数据可以看出，所有的数据都是按照列A进行排序的1、2、3、4，B列的顺序为1、2、1、5、1、5，B列全局是无序的，这就尴尬了，如果我们仅按照列B去查询，在索引中匹配的时候岂不是很麻烦，或者说压根儿就无法匹配，这也就明白了为什么仅使用到列B的时候不会走索引了，那仅使用A的时候可以走索引是因为列A在索引树中相对于全局是有序的，所以可以根据列A进行二分查找和定位。由此可见推论2也是可靠的。</li><li>B+树的非叶子节点存储的是索引关键字的数据信息，并且根据推论2的结果可以验证推论3也是正确的。</li><li>B+树是使用二分法进行查找的，所以推论4是正确的。</li></ol><h3 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h3><p>我们根据B+树的特性、索引的断桥原则和单列索引存储特性三个方面反推组合索引的数据存储结构，并验证了我们的推论，这仅仅是串一串的推论，可能不靠谱，哈哈~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前言&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#前言&quot;&gt;&lt;/a&gt; 前言&lt;/h3&gt;
&lt;p&gt;了解MySQL索引结构的基本都知道索引BTree类型是用B+树的数据结构，单列索引的结构我们很容易理解，二级索引的每个叶子节点只存储主键关键字外
      
    
    </summary>
    
    
      <category term="MySQL" scheme="http://luxiaowan.github.io/categories/MySQL/"/>
    
    
      <category term="索引" scheme="http://luxiaowan.github.io/tags/%E7%B4%A2%E5%BC%95/"/>
    
  </entry>
  
  <entry>
    <title>Redis和MySQL分布式双写一致性</title>
    <link href="http://luxiaowan.github.io/2020/04/12/Redis%E5%92%8CMySQL%E5%88%86%E5%B8%83%E5%BC%8F%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/"/>
    <id>http://luxiaowan.github.io/2020/04/12/Redis和MySQL分布式双写一致性/</id>
    <published>2020-04-12T06:37:00.000Z</published>
    <updated>2020-04-12T14:50:23.244Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h3><p>一个MySQL服务和一个Redis服务，用户的数据存储持久化在MySQL中，缓存在Redis中，有请求的时候从Redis中获取缓存的用户数据，有修改则同时修改MySQL和Redis中的数据。现在问题是：不论是先保存到MySQL还是Redis，都面临着此成功彼失败的情况，那么如何保证MySQL与Redis中的数据一致？</p><h3 id="数据一致性"><a class="markdownIt-Anchor" href="#数据一致性"></a> 数据一致性</h3><p>数据一致性主要出现在使用不同存储组件的情况下，存储组件之间无法直接通信，所以不能相互之间实现数据交换，但是使用第三方来单独操作各存储组建时，有极大的可能造成各存储组件之间数据不一致。</p><p>数据一致性分为强一致性和最终一致性，强一致性的情况是不论何时访问哪一个存储组件，所得到的数据都是一样的，最终一致性的情况是可以在某短时间内访问不同存储组件所得到的数据允许不一样。我们实际的生产开发中，基本都是遵循最终一致性。</p><h3 id="mysql和redis一致性"><a class="markdownIt-Anchor" href="#mysql和redis一致性"></a> MySQL和Redis一致性</h3><p>Redis一般用于存储热点数据，MySQL存储所有数据。但是在更新缓存这方面有多种方案：</p><ul><li>先删除缓存，再更新数据库</li><li>先更新缓存，再更新数据库</li><li>先更新数据库，在更新缓存</li><li>先更新数据库，再删除缓存</li><li>先更新数据库，再给缓存设置过期时间</li><li>使用Canal中间件</li></ul><h4 id="先删除缓存再更新数据库不可取"><a class="markdownIt-Anchor" href="#先删除缓存再更新数据库不可取"></a> 先删除缓存，再更新数据库（不可取）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">method</span><span class="params">()</span>:</span></span><br><span class="line">  deleteRedis()</span><br><span class="line">  updateDB()</span><br></pre></td></tr></table></figure><p>第一步先删除缓存，第二步再更新数据库，如果在第一步之后，其他线程发生了数据库读操作，然后读到的旧数据又set到了Redis中，然后第二步执行，最终Redis和MySQL中的数据出现了不一致</p><h4 id="先更新缓存再更新数据库不可取"><a class="markdownIt-Anchor" href="#先更新缓存再更新数据库不可取"></a> 先更新缓存，再更新数据库（不可取）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">method</span><span class="params">()</span>:</span></span><br><span class="line">  updateRedis()</span><br><span class="line">  updateDB()</span><br></pre></td></tr></table></figure><p>第一步先更新缓存，第二步更新数据库，如果更新Redis成功，但是更新数据库失败，则两者的数据又出现了不一致</p><h4 id="先更新数据库在更新缓存不可取"><a class="markdownIt-Anchor" href="#先更新数据库在更新缓存不可取"></a> 先更新数据库，在更新缓存（不可取）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">method</span><span class="params">()</span>:</span></span><br><span class="line">  updateDB()</span><br><span class="line">  updateRedis()</span><br></pre></td></tr></table></figure><p>两个线程同时对一条数据进行操作，在线程B先于线程A更新缓存成功，则造成了缓存中的数据低于MySQL一个版本</p><p>####先更新数据库，再删除缓存（可取）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">method</span><span class="params">()</span>:</span></span><br><span class="line">  updateDB()</span><br><span class="line">  deleteRedis()</span><br></pre></td></tr></table></figure><p>两个线程同时操作一条数据，甭管哪一个先操作成功，最终都会删除掉缓存中的数据，然后由查询将最新数据set到缓存中，但是在并发量大的情况下，由于缓存被删除了两次，可能会造成缓存击穿</p><h4 id="先更新数据库再给缓存设置过期时间推荐"><a class="markdownIt-Anchor" href="#先更新数据库再给缓存设置过期时间推荐"></a> 先更新数据库，再给缓存设置过期时间（推荐）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">method</span><span class="params">()</span>:</span></span><br><span class="line">  updateDB()</span><br><span class="line">  expireRedis()</span><br></pre></td></tr></table></figure><p>更新完数据库之后，并不立即删除缓存，而是给缓存设置一个过期时间，由缓存自行过期，使用这个方案的前提是能允许数据可以很短时间内的不一致，并且推荐</p><h4 id="使用canal中间件可取"><a class="markdownIt-Anchor" href="#使用canal中间件可取"></a> 使用Canal中间件（可取）</h4><p>Canal是阿里巴巴开源的一款数据库同步工具，他可以将自己伪装成一个MySQL Slave节点，将MySQL的binlog文件同步到Canal，然后由Canal进行下一步处理，比如存入MQ、Redis等，但是可能会存在些许延迟。后续讨论</p><h3 id="强一致性"><a class="markdownIt-Anchor" href="#强一致性"></a> 强一致性</h3><p>如果想要求数据强一致性，就只用MySQL，不要用Redis，以免出现不一致的情况</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前言&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#前言&quot;&gt;&lt;/a&gt; 前言&lt;/h3&gt;
&lt;p&gt;一个MySQL服务和一个Redis服务，用户的数据存储持久化在MySQL中，缓存在Redis中，有请求的时候从Redis中获取缓存的用户数据，有
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
