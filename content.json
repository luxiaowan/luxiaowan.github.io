{"meta":{"title":"串一串","subtitle":"断舍离","description":null,"author":"cc","url":"http://luxiaowan.github.io","root":"/"},"pages":[{"title":"关于本人","date":"2019-10-21T15:26:09.000Z","updated":"2019-10-21T15:28:30.066Z","comments":true,"path":"about/index.html","permalink":"http://luxiaowan.github.io/about/index.html","excerpt":"","text":"串一串 浪人 非常懒，偶尔写一下 文笔很烂，写的不好 凑合看吧"},{"title":"推荐书单","date":"2019-10-21T15:38:29.000Z","updated":"2019-10-21T15:56:47.928Z","comments":true,"path":"books/index.html","permalink":"http://luxiaowan.github.io/books/index.html","excerpt":"","text":"书名 购买地址 JavaScript权威指南（第6版） JavaScript高级程序设计 Java编程思想（第4版） java并发编程实战 Netty权威指南（第2版） Spring Boot实战 Spring微服务实战 Word Excel PPT 2016入门与提高 编程珠玑（第2版） 操作系统真象还原 大型网站系统与Java中间件实践 高性能JavaScript 高性能MySQL（第3版） 机器学习 极简思维 技术运营 人性的弱点 设计模式解析（第2版） 设计模式之禅 深度思维 深入理解Java虚拟机 深入浅出Node.js 深入浅出React和Redux 移动Web前端高效开发实战 亿级流量网站架构核心技术 原则 怎样管精力就怎样过一生 重构改善既有代码的设计 Excel高效办公：数据处理与分析（修订版） 项目管理艺术 周鸿祎自述 我的互联网方法论 奇点临近 高效能人士的七个习惯 三板斧：阿里巴巴管理之道 掘金移动互联：跨境电商如何挑战海外市场 微信思维 系统之美:决策者的系统思考 思考，快与慢 创新者的窘境 微服务设计 Scrum敏捷软件开发 管理的实践"},{"title":"标签","date":"2019-10-21T15:35:35.000Z","updated":"2019-10-21T15:35:54.071Z","comments":true,"path":"tags/index.html","permalink":"http://luxiaowan.github.io/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2019-10-21T15:29:38.000Z","updated":"2019-10-21T15:32:44.295Z","comments":true,"path":"categories/index.html","permalink":"http://luxiaowan.github.io/categories/index.html","excerpt":"","text":""},{"title":"个人项目","date":"2019-10-21T15:54:25.000Z","updated":"2019-10-21T15:57:51.353Z","comments":true,"path":"repository/index.html","permalink":"http://luxiaowan.github.io/repository/index.html","excerpt":"","text":"https://github.com/luxiaowan"},{"title":"友情链接","date":"2019-10-21T15:42:43.000Z","updated":"2019-10-21T15:57:01.607Z","comments":true,"path":"links/index.html","permalink":"http://luxiaowan.github.io/links/index.html","excerpt":"","text":"本人还没友链，孤独小客"}],"posts":[{"title":"Redis哨兵简介","slug":"Redis哨兵简介","date":"2020-03-28T17:05:00.000Z","updated":"2020-03-28T19:11:33.900Z","comments":true,"path":"2020/03/29/Redis哨兵简介/","link":"","permalink":"http://luxiaowan.github.io/2020/03/29/Redis哨兵简介/","excerpt":"","text":"前言 Redis Sentinel是Redis官方建议的高可用(HA)解决方案，在我们搭建Redis集群时，Redis本身并未集成主备切换功能，sentinel本身是独立运行的，能够监控多个Redis集群，发现master宕机后能够自动切换，选举一个slave成为新的master，当原master恢复之后，sentinel会自动将其作为slave加入到集群中，整个过程不需要人工参与，完全自动化。 主要介绍 sentinel主要功能 定期监控Redis服务是否运行正常 定期监控其他sentinel服务是否正常 能够自动切换master节点 sentinel节点不存储数据 sentinel集群 这个不难理解，如果我们用一个非高可用的sentinel去实现Redis的高可用，明显是不科学的，当这一台sentinel宕机之后，Redis显然无法继续保持它的高可用，所以我们在部署sentinel的时候也会采用集群的方式 优势： ​ 即使有sentinel服务宕机，只要还有一台sentinel运行正常，就可以使Redis继续保持高可用 sentinel版本问题 sentinel在Redis2.6版本中引入的，当时是sentinel 1，貌似有蛮多问题，毕竟初版 在Redis2.8版本中升级到sentinel 2，之后就非常稳定了 不过现在Redis已经发展了很久，版本也越来越高，sentinel已经非常值得信赖了 sentinel中的定时任务 每隔10秒向各个Redis服务器(master和slave节点)发送INFO命令，根据回应获取master和slave信息，通过master的回复可以获取到新增的slave节点 每隔02秒向Redis的master服务器发送命令(hello消息)，用于发现和监视其他sentinel，sentinel之间的监控不在额外创建订阅 每隔01秒向Redis和sentinel所有服务发送PING消息(sentinel本身的ip、端口、id等内容)，通过回复PONG判断服务是否在线 下线判断 主观下线：当前sentinel断定master下线 客观下线：满足sentinel配置文件中quorum数量的sentinel均断定master下线 配置文件解读 1234567891011121314151617181920212223242526272829303132333435# sentinel运行的端口，默认为26379port 26377 dir \"/private/tmp\"logfile \"/var/log/redis/sentinel_26377.log\"# 以守护进程执行daemonize yes# 守护进程运行的pid保存文件pidfile \"/var/run/redis-sentinel.pid\"# 格式：sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt;# 该行的意思是：&lt;master-name&gt;：自定义# &lt;ip&gt;：master主机的IP# &lt;redis-port&gt;：master的端口# &lt;quorum&gt;：表示在sentinel集群中，使master由主观下线变为客观下线的sentinel数量。sentinel monitor cc_master 127.0.0.1 6379 2 # 格式：sentinel down-after-milliseconds &lt;master-name&gt; &lt;milliseconds&gt;# sentinel会向master发送心跳PING来确认master是否存活，如果master在&lt;milliseconds&gt;时间内回应的不是PONG，那么这个sentinel会主观地认为这个master下线了。&lt;milliseconds&gt;的单位是毫秒，默认30秒。sentinel down-after-milliseconds cc_master 15000# 格式：sentinel failover-timeout &lt;master-name&gt; &lt;milliseconds&gt;# failover过期时间，当failover开始后，在此时间内仍然没有触发任何failover操作，当前sentinel将会认为此次failoer失败。默认180秒，即3分钟。sentinel failover-timeout cc_master 60000# sentinel parallel-syncs &lt;master-name&gt; &lt;numreplicas&gt;# 在发生failover主备切换时，这个选项指定了最多可以有多少个slave同时对新的master进行同步，这个数字越小，完成failover所需的时间就越长，但是如果这个数字越大，就意味着越多的slave因为replication而不可用。可以通过将这个值设为1来保证每次只有一个slave处于不能处理命令请求的状态。sentinel parallel-syncs cc_master 1# 格式：sentinel auth-pass &lt;master-name&gt; &lt;password&gt;# sentinel连接的master节点的登录密码sentinel auth-pass cc_master redis 故障转移 sentinel使用Raft投票选举出一个leader去执行故障转移 每一个将master标记为主观下线的sentinel节点发起投票 其他sentinel节点收到投票后，若尚未参与投票(也就是尚未投票给其他sentinel)，则同意，否则拒绝 最终收到过半同意的sentinel节点作为leader 若有两个sentinel收到了过半投票，那么就再重新选举 选举新的master节点 选择replica-priority配置数字最高的slave节点为master，默认为100 若replica-priority相同，则选择偏移量最大的slave节点，偏移量是指slave从master同步的进度，偏移量越大说明数据越完整，可以通过Redis的info命令查看(slave_repl_offset)当前slave的偏移量 若偏移量相同，则选择最先启动的slave作为master 更改master后，通知其他slave节点同步为新的master节点的slave节点 原master节点恢复之后自动加入到集群中，成为新master的slave节点 实战 在本机上启动3个Redis实例，采用1主2从的模式，以下只记录redis.conf和sentinel.conf中关键内容 redis.conf redis-master.conf配置 12# 默认端口port 6379 redis-slave1.conf配置 123456# 端口port 63791# 格式：replicaof &lt;masterip&gt; &lt;masterport&gt;# 从节点归属的master节点replicaof 127.0.0.1 6379 redis-slave2.conf配置 123456# 端口port 63792# 格式：replicaof &lt;masterip&gt; &lt;masterport&gt;# 从节点归属的master节点replicaof 127.0.0.1 6379 sentinel.conf sentinel0.conf 1234567# 端口port 26379sentinel myid 842c9102c48eb0cedeb06fe55e7d2258595ac267# 监控mastersentinel monitor cc_master 127.0.0.1 6379 2 sentinel1.conf 1234567# 端口port 26378sentinel myid 842c9102c48eb0cedeb06fe55e7d2258595ac266# 监控mastersentinel monitor cc_master 127.0.0.1 6379 2 sentinel2.conf 1234567# 端口port 26377sentinel myid 842c9102c48eb0cedeb06fe55e7d2258595ac265# 监控mastersentinel monitor cc_master 127.0.0.1 6379 2 启动 启动sentinel 12345redis-sentinel ~/Documents/develop_tools/tools/redis-5.0.5/sentinel0.confredis-sentinel ~/Documents/develop_tools/tools/redis-5.0.5/sentinel1.confredis-sentinel ~/Documents/develop_tools/tools/redis-5.0.5/sentinel2.conf 启动Redis 12345redis-server ~/Documents/develop_tools/tools/redis-5.0.5/redis-master.confredis-server ~/Documents/develop_tools/tools/redis-5.0.5/redis-slave1.confredis-server ~/Documents/develop_tools/tools/redis-5.0.5/redis-slave2.conf Redis通过info查看信息 127.0.0.1:6379&gt;info all 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161# Server服务器信息redis_version:5.0.5 # Redis 服务器版本redis_git_sha1:00000000 # Git SHA1redis_git_dirty:0 # Git dirty flagredis_build_id:6c6e38af7cea0726 # Redis构建IDredis_mode:standalone # Redis运行模式os:Darwin 19.3.0 x86_64 # 运行环境操作系统版本arch_bits:64 # 架构（32 或 64 位）multiplexing_api:kqueue # Redis 所使用的事件处理机制atomicvar_api:atomic-builtingcc_version:4.2.1 # 编译的GCC版本process_id:61985 # 服务器进程的 PIDrun_id:433b78ec513c8b782f3a46ba6b4ade1f12439aca # Redis 服务器的随机标识符(用于Sentinel和集群)tcp_port:6379 # Redis端口uptime_in_seconds:108 # Redis运行时长，秒uptime_in_days:0 # Redis运行市场，天hz:10configured_hz:10lru_clock:8363059 # 以分钟为单位进行自增的时钟，用于 LRU 管理executable:/Users/chuan/redis-server # 运行命令config_file: # 启动使用的配置文件# Clientsconnected_clients:7 # 已连接客户端的数量client_recent_max_input_buffer:2 # 当前连接的客户端当中，最长的输出列表client_recent_max_output_buffer:0 # 当前连接的客户端当中，最大输入缓存blocked_clients:0 # 正在等待阻塞命令（BLPOP、BRPOP、BRPOPLPUSH）的客户端的数量# Memory (太多了，不做解释了)used_memory:2235920 # 由 Redis 分配器分配的内存总量，以字节（byte）为单位used_memory_human:2.13M # 以可读的格式返回 Redis 分配的内存总量used_memory_rss:3153920 # 从操作系统的角度，返回 Redis 已分配的内存总量（俗称常驻集大小）。这个值和top 、 ps 等命令的输出一致。used_memory_rss_human:3.01M # 以可读的格式返回rssused_memory_peak:2317840 # Redis 的内存消耗峰值（以字节为单位）used_memory_peak_human:2.21Mused_memory_peak_perc:96.47%used_memory_overhead:2221526used_memory_startup:987776used_memory_dataset:14394used_memory_dataset_perc:1.15%allocator_allocated:2271808allocator_active:3116032allocator_resident:3116032total_system_memory:17179869184total_system_memory_human:16.00Gused_memory_lua:37888used_memory_lua_human:37.00Kused_memory_scripts:0used_memory_scripts_human:0Bnumber_of_cached_scripts:0maxmemory:0maxmemory_human:0Bmaxmemory_policy:noevictionallocator_frag_ratio:1.37allocator_frag_bytes:844224allocator_rss_ratio:1.00allocator_rss_bytes:0rss_overhead_ratio:1.01rss_overhead_bytes:37888mem_fragmentation_ratio:1.39mem_fragmentation_bytes:882112mem_not_counted_for_evict:0mem_replication_backlog:1048576mem_clients_slaves:33844mem_clients_normal:151226mem_aof_buffer:0mem_allocator:libcactive_defrag_running:0lazyfree_pending_objects:0# Persistenceloading:0 # 服务器是否正在载入持久化文件rdb_changes_since_last_save:1 # 距离最后一次成功创建持久化文件之后，改变了多少个键值rdb_bgsave_in_progress:0 # 服务器是否正在创建RDB文件rdb_last_save_time:1585421263 # 最近一次成功创建RDB文件的UNIX时间rdb_last_bgsave_status:ok # 最后一次创建RDB文件的结果是成功还是失败rdb_last_bgsave_time_sec:0 # 最后一次创建RDB文件耗费的秒数rdb_current_bgsave_time_sec:-1 # 记录当前创建RDB操作已经耗费了多长时间（单位为秒）rdb_last_cow_size:0aof_enabled:0 # AOF是否处于打开状态aof_rewrite_in_progress:0 # 服务器是否正在创建AOF文件aof_rewrite_scheduled:0 # 是否需要执行预约的AOF重写操作aof_last_rewrite_time_sec:-1 # 最后一次重启AOF的秒数aof_current_rewrite_time_sec:-1 # 记录当前正在重写AOF的秒数aof_last_bgrewrite_status:ok # 最后一次重写AOF文件的结果aof_last_write_status:ok # 最后一次写入结果aof_last_cow_size:0# Stats (可以不做了解)total_connections_received:9total_commands_processed:720instantaneous_ops_per_sec:6total_net_input_bytes:34197total_net_output_bytes:229238instantaneous_input_kbps:0.34instantaneous_output_kbps:1.19rejected_connections:0sync_full:2 # 主从完全同步成功次数sync_partial_ok:0 # 主从部分同步成功次数sync_partial_err:0 # 主从部分同步失败次数expired_keys:0 # 运行以来过期的key的数量expired_stale_perc:0.00 # 过期的比率expired_time_cap_reached_count:0 # 过期计数evicted_keys:0keyspace_hits:1keyspace_misses:1pubsub_channels:1pubsub_patterns:0latest_fork_usec:278migrate_cached_sockets:0slave_expires_tracked_keys:0active_defrag_hits:0active_defrag_misses:0active_defrag_key_hits:0active_defrag_key_misses:0# Replication（master节点）role:master # 角色 master和slaveconnected_slaves:2 # slave节点数slave0:ip=127.0.0.1,port=63791,state=online,offset=20163,lag=1 # 从节点1slave1:ip=127.0.0.1,port=63792,state=online,offset=20163,lag=0 # 从节点2master_replid:895f219aa1e7ed5ecda50dcb1f77eea9f1ef9c3d # 主实例启动随机字符串master_replid2:0000000000000000000000000000000000000000 # 主实例启动随机字符串2master_repl_offset:20163 # 主从同步偏移量,此值如果和上面的offset相同说明主从一致没延迟，与master_replid可被用来标识主实例复制流中的位置。second_repl_offset:-1 # 主从同步偏移量2,此值如果和上面的offset相同说明主从一致没延迟repl_backlog_active:1 # 复制积压缓冲区是否开启repl_backlog_size:1048576 # 复制积压缓冲大小repl_backlog_first_byte_offset:1 # 复制缓冲区里偏移量的大小repl_backlog_histlen:20163 # 此值等于 master_repl_offset - repl_backlog_first_byte_offset,该值不会超过repl_backlog_size的大小# Replication（slave节点）role:slave # 角色 master和slavemaster_host:127.0.0.1 # master节点IPmaster_port:6379 # master节点端口master_link_status:up # master通信master_last_io_seconds_ago:1 # 主库多少秒未发送数据到从库master_sync_in_progress:0 # 从服务器是否在与主服务器进行同步slave_repl_offset:42262 # slave复制偏移量slave_priority:100 # slave优先级slave_read_only:1 # 从库是否设置只读connected_slaves:0 # 连接的slave实例个数master_replid:895f219aa1e7ed5ecda50dcb1f77eea9f1ef9c3dmaster_replid2:0000000000000000000000000000000000000000master_repl_offset:42262 # master偏移量，与slave_repl_offset相同则表示同步完整second_repl_offset:-1repl_backlog_active:1 # 复制积压缓冲区是否开启repl_backlog_size:1048576 # 复制积压缓冲大小repl_backlog_first_byte_offset:1 # 复制缓冲区里偏移量的大小repl_backlog_histlen:42262 # 此值等于 master_repl_offset - repl_backlog_first_byte_offset,该值不会超过repl_backlog_size的大小# CPUused_cpu_sys:0.104404 # 将所有redis主进程在核心态所占用的CPU时求和累计起来used_cpu_user:0.079472 # 将所有redis主进程在用户态所占用的CPU时求和累计起来used_cpu_sys_children:0.002037 # 将后台进程在核心态所占用的CPU时求和累计起来used_cpu_user_children:0.000648 # 将后台进程在用户态所占用的CPU时求和累计起来# Clustercluster_enabled:0 # 实例是否启用集群模式# Keyspacedb0:keys=1,expires=0,avg_ttl=0 # db0的key的数量,以及带有生存期的key的数,平均存活时间","categories":[{"name":"Redis","slug":"Redis","permalink":"http://luxiaowan.github.io/categories/Redis/"}],"tags":[{"name":"Sentinel","slug":"Sentinel","permalink":"http://luxiaowan.github.io/tags/Sentinel/"}]},{"title":"线程池运行的线程和队列中等待的线程是同一个吗","slug":"线程池运行的线程和队列中等待的线程是同一个吗","date":"2020-03-27T16:52:00.000Z","updated":"2020-03-27T18:42:31.562Z","comments":true,"path":"2020/03/28/线程池运行的线程和队列中等待的线程是同一个吗/","link":"","permalink":"http://luxiaowan.github.io/2020/03/28/线程池运行的线程和队列中等待的线程是同一个吗/","excerpt":"","text":"线程池 在高并发场景下，线程池是会被频繁使用到的，简单介绍下线程池： 线程池基础参数：核心线程数、最大线程数、线程最大存活时间、时间单位、阻塞队列、线程池工厂、拒绝策略 创建方式： ThreadPoolExecutor类：ThreadPoolExecutor tpe = new ThreadPoolExecutor(1, 1, 0, TimeUtil.SECONDS, new ArrayBlockingQueue(), new DefaultThreadFactory()) Executors类： newFixedThreadPool(1); newSingleThreadExecutor(); newCachedThreadPool(); newScheduledThreadPool(1); 等待队列： ArrayBlockingQueue：由数组结构组成的有界阻塞队列 LinkedBlockingQueue：由链表结构组成的有界阻塞队列 DelayQueue：使用优先级队列实现的无界阻塞队列 PriorityBlockingQueue：支持优先级排序的无界阻塞队列 SynchronousQueue：不存储元素的阻塞队列 拒绝策略： DiscardPolicy：丢弃被拒绝任务 DiscardOldestPolicy：丢弃队列头部的任务 AbortPolicy：抛出RejectedExecutionException CallerRunsPolicy：在调用execute方法的线程中运行被拒绝的任务 工作原理： 正文 线程池创建过程 创建语句： 123456789ThreadPoolExecutor pools = new ThreadPoolExecutor(5, 10, 10, TimeUtil.SECONDS, new LinkedBlockingQueue(), new ThreadPoolExecutor.AbortPolicy());-- 核心线程数：5 最大线程数：10 非核心线程最大存活时间：10秒 阻塞队列：LinkedBlockingQueue 线程池工厂：DefaultThreadFactory 拒绝策略：AbortPolicy 在线程池真正运行之前，核心线程尚未创建，因为默认是在实际使用的时候才会去创建，但是如果我们想要在线程池创建的时候就初始化核心线程，可以调用ThreadPoolExecutor的实例方法prestartAllCoreThreads()，如果我们想要让核心线程在空闲时可以过期，那么我们可以调用ThreadPoolExecutor的实例方法allowCoreThreadTimeOut(boolean value)来设置 1234567891011121314151617181920212223242526/** * 设置核心线程是否允许过期 */public void allowCoreThreadTimeOut(boolean value) &#123; // 若value为true，但是线程最大存活时间不大于0，那么则抛异常 if (value &amp;&amp; keepAliveTime &lt;= 0) throw new IllegalArgumentException(\"Core threads must have nonzero keep alive times\"); // 如果设置的新值和当前值不同，则执行计划 if (value != allowCoreThreadTimeOut) &#123; allowCoreThreadTimeOut = value; // 若value为true，则终止线程池内的所有空闲Worker if (value) interruptIdleWorkers(); &#125;&#125;/** * 初始化核心线程 */public int prestartAllCoreThreads() &#123; int n = 0; // 循环创建工作线程Worker while (addWorker(null, true)) ++n; return n;&#125; 创建线程Worker 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172private boolean addWorker(Runnable firstTask, boolean core) &#123; retry:// goto语法 for (;;) &#123;// 无限循环，循环体内控制退出 int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; !(rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; !workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c);// 当前线程数 // 校验当前正在执行的线程数是否超过了2^29 - 1，或者根据创建的是否为核心线程来与核心线程数和最大线程数做校验，如果已经超过了相关的值，则返回false拒绝创建 if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; // 改变当前运行的线程数，这里使用的CAS来保证线程安全，设置成功则跳出最外层循环 if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl // 当前线程池状态和方法最初的对比，若不等，则重新执行for循环体 if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; // 创建Worker线程实例 w = new Worker(firstTask); // Worker实例的属性，在Worker构造器中通过getThreadFactory().newThread(this);来创建 final Thread t = w.thread; if (t != null) &#123; // 加锁，保证线程安全 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); // 校验rs是否为RUNNING，或者停止且队列中无任务 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); // 将新创建的Worker实例放入HashSet集合中 workers.add(w); int s = workers.size(); // 更新最大线程运行数 if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; // 工作线程创建成功后，调用线程的start()方法开启线程 if (workerAdded) &#123; t.start();// tag-cc307 workerStarted = true; &#125; &#125; &#125; finally &#123; // 创建失败的话，则处理失败计划 if (!workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125; 由addWorker方法我们可以看出，线程池的核心执行器是Worker内部类 123456789101112131415// Worker类定义private final class Worker extends AbstractQueuedSynchronizer implements Runnable-- final修饰：不可被扩展 继承自AQS：保证线程运行的隔离性，线程池的线程安全核心 实现自Runnable，所以Worker也是一个线程类-- 构造器 Worker(Runnable firstTask) &#123; setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; // 使用线程池工厂创建新线程，并将创建的线程赋值给实例属性thread，也就是在我们调用了thread的start()方法之后，会运行Worker类中的run()方法 this.thread = getThreadFactory().newThread(this); &#125; 看到这里，就应该去看Worker类中的run方法了，我们看到在run方法中调用了runWorker方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495// 核心关键方法，final修饰，不允许被overload和overridefinal void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); // 全文搜索tag-cc307 Runnable task = w.firstTask; w.firstTask = null; // 加锁前先释放锁，查看Worker中的tryRelease方法 w.unlock(); // allow interrupts boolean completedAbruptly = true; try &#123; // 无限循环，这里的无限循环的实现方法主要在getTask()方法中，getTask()是从阻塞队列中获取等待的任务，这里我们可以看到阻塞队列中存储的是一个个Runnable实例 while (task != null || (task = getTask()) != null) &#123; // 线程加锁 w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; // 执行前计划 beforeExecute(wt, task); Throwable thrown = null; try &#123; // 直接调用任务的run方法，这里其实就是将队列中Runnable实例当成普通的非线程对象，我们都知道直接调用线程的run方法会以普通方法的形式去执行，这里之所以这样写，是因为我们当前已经处于一个线程中了，没必要再去启用一个线程去执行任务，否则线程池就没有存在的必要了 task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; afterExecute(task, thrown); &#125; &#125; finally &#123; task = null; // 记录线程Worker的成功任务数 w.completedTasks++; w.unlock(); &#125; &#125; completedAbruptly = false; &#125; finally &#123; processWorkerExit(w, completedAbruptly); &#125;&#125;/*** 获取队列中的任务*/private Runnable getTask() &#123; boolean timedOut = false; // Did the last poll() time out? for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); return null; &#125; int wc = workerCountOf(c); // 核心：判断是否允许核心线程过期 或 当前工作线程数是否超过了核心线程数，timed决定了是否回收核心线程 boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c)) return null; continue; &#125; try &#123; // 若需要销毁工作线程，则使用poll方法使阻塞队列消失 // 否则通过take方法继续阻塞，直到队列中有新数据 Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; timedOut = true; &#125; catch (InterruptedException retry) &#123; timedOut = false; &#125; &#125;&#125; 总结 从以上的内容中我们可以看出来：线程池运行的线程和队列中等待的线程不是同一个，线程池中实际运行的线程是Worker实例","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[]},{"title":"Redis相关东西","slug":"Redis相关东西","date":"2020-03-26T14:15:00.000Z","updated":"2020-03-28T16:03:57.189Z","comments":true,"path":"2020/03/26/Redis相关东西/","link":"","permalink":"http://luxiaowan.github.io/2020/03/26/Redis相关东西/","excerpt":"","text":"目录 Redis是什么 五种数据类型 Redis为何这么快 Redis和Memcached的区别 淘汰策略 持久化 主从复制 哨兵 Redis是什么 ​ Redis是由C语言编写的一个开源的高性能键值对的内存数据库，是一款NoSQL(not-only sql | no sql)数据库，可以用作缓存、数据库、消息中间件。 ​ Redis作为一款内存数据库，其优势为： 1. 性能优秀，数据都存于内存中，读写速度快，理论读取速度能达到10W/秒； 2. 单线程操作，线程安全，采用IO多路复用实现垃圾回收和持久化； 3. 支持丰富的数据类型：字符串(string)、列表(list)、散列(hash)、集合(set)、有序集合(zset)； 4. 支持数据持久化，并且提供RDB和AOF两种持久化方式； 5. 可以用于分布式锁，也可以利用Redis的发布-订阅特性实现消息队列； 6. 主从复制，哨兵，高可用。 五种数据类型 字符串(string)类型：Redis的字符串类型是最基本的类型，可以理解为一个key对应一个value，value可以是字符也可以是数字，该类型可以存储图片或者序列化后的对象等二进制数据，支持的value大小最大512M，当value小于44字节(3.2版本+是44，3.0版本-是39)时，字符串编码为embstr，大于的时候字符串编码为raw，常用的命令有set、get等。 散列(hash)类型：hash是一个键值对的集合，特别适合用于存储对象，可以直接获取到对象的某个属性的值，常用的命令有hset、hget、hgetall等。 列表(list)类型： 集合(set)类型： 有序集合(zset)类型： Redis为何那么快 ​ Redis之所以是单线程的还那么快，完全是因为Redis是纯内存操作，没有CPU上下文切换带来的消耗，也没有磁盘寻址等带来的IO开销，官方理论QPS为10W+。 ​ 既然Redis的性能瓶颈是内存和网络带宽，那么就没必要设计成多线程模式，否则会多出CPU切换，且只要涉及到多线程必然会因为资源竞争而衍生出资源锁的使用，频繁的加锁、释放锁还是非常浪费时间的，所以既然多线程会带来那么多问题，还是使用单线程得了，并且Redis本身就是K-V存储，查询时间复杂度限制在O(1)的情况，所以Redis才那么快。 Redis和Memcached的区别 存储方式：Memcache将数据存储在内存中，若服务器出现故障，则数据全部丢失，无法持久化。Redis提供了RDB和AOF两种方式进行数据持久化，就算是服务器宕机，在恢复之后依然可以保证数据的完整性； 数据类型：Memcache仅支持字符串存储，而Redis支持字符串、列表、散列、集合、有序集合等类型，使用起来更方便和多样化； value大小：Redis的字符串类型可以存储512M的内容，而Memcache最高仅能存储1M的内容，不过虽然Redis支持很大的value，但是一般不会那么用； 底层协议不同：Redis拥有自己的VM，Memcache使用系统函数 淘汰策略 Redis目前有6种淘汰策略，据说新版本中有8种 volatile-lru：从设置了过期时间的所有key中将最近最少使用(least recently used)的key淘汰掉 volatile-ttl：从设置了过期时间的所有key中将剩余存活时间最少(time to live)的key淘汰掉 volatile-random：从设置了过期时间的所有key中随机淘汰掉部分key allkeys-lru：从所有的key中将最近最少使用的key淘汰掉 allkeys-random：从所有的key中随机淘汰掉部分key noeviction：不执行数据淘汰，当内存不足时直接拒绝新的插入请求，并返回错误信息 volatile-lfu：从设置了过期时间的所有key中将访问频率最少(least frequently used)的key淘汰掉 allkeys-lfu：从所有的key中将访问频率最少的key淘汰掉 持久化 Redis支持两种持久化方式，持久化的目的是将内存中的数据写入到磁盘中，防止服务出现故障后的数据丢失的情况。 RDB方式：RDB是Redis的默认持久化方式，属于是定时保存，每隔一段时间将fork出一个子进程去将内存中的数据写入到一个临时dump.rdb(名字在配置文件中设置)文件中，待子进程执行完成之后，将这个临时的dump文件替换掉原来的dump文件，这样做的目的是可以实现copy-on-write，子进程运行过程中使用的内存资源与Redis主进程无关 *通过bgsave和save命令可以手动触发执行RDB 配置信息：redis.conf文件中 解读： save 900 1：900秒内若至少有1个key发生变化，则触发备份 save 300 10：300秒内若至少有10个key发生变化，则触发备份 save 60 10000：60秒内若至少有1万个key发生变化，则触发备份 劣势： RDB持久化方式适合于整库备份，dump文件用于故障恢复，但是由于RDB方式并不是实时的整库备份，所以我们拿到的dump文件总是会和内存中的数据不一致，如果你想要避免服务器发生故障的时候丢失数据，那么仅仅使用RDB是万万不行的，需要配合AOF使用。 为了使用子进程在磁盘上持久存储，RDB经常需要fork()。如果数据集很大，Fork()可能很耗时，并且可能导致Redis停止为客户端提供服务几毫秒甚至一秒钟(如果数据集很大，而且CPU性能不是很好)。AOF还需要fork()，但是您可以调整重写日志的频率，而不需要牺牲持久性。 优势： RDB是一个非常紧凑的单文件时间点表示您的Redis数据。RDB文件非常适合备份。例如，您可能希望在最近的24小时内每小时存档一次RDB文件，并在30天内每天保存一次RDB快照。这允许您在发生灾难时轻松地恢复不同版本的数据集。 RDB对于灾难恢复非常有用，它是一个紧凑的文件，可以传输到远程数据中心上。 RDB最大限度地提高了Redis性能，因为为了保持Redis父进程所需做的惟一工作就是创建一个将完成所有其余工作的子进程。父实例永远不会执行磁盘I/O或类似的操作。 与AOF相比，RDB允许对大数据集进行更快的重启。 AOF方式：AOF方式在Redis中是默认未开启的，在开启AOF后，会将内容写入到appendonly.aof文件中，文件的内容是服务器接收到的所有对数据进行修改的命令集合，按照时间顺序追加到文件尾部，并且在故障恢复的时候，会优先读取appendonly.aof文件中的内容，因为aof的默认策略是每秒钟写入一次，所以当采用aof进行持久化的时候，最多也仅仅丢失一秒的数据。 配置信息：redis.conf文件中 劣势： 随着服务运行时间越来越久，内存中的数据变更次数越来越多，会造成aof文件越来越大，当然我们可以在配置文件redis.conf中设置aof文件重写策略，默认当aof文件大小达到64mb且增长比例超过了之前是100%的时候进行重写，重写的规则是将内存中的数据的当前值全部以对应的set命令写入到新的aof文件中，比如当前aof文件100mb，重写之后80mb，那么只有当文件再次达到160mb(160&gt;=80*2&amp;&amp;160&gt;64)的时候才会再次进行重写， AOF文件损坏修复： ​ 如果在AOF文件写入的过程中突然宕机，可能会导致aof文件损坏，我们可以使用redis-check-aof --fix命令来修复 ####### 故障恢复 若同时开启了RDB和AOF，那么在故障恢复的时候先使用AOF文件进行恢复，这样可以保证丢失最少的数据，但是如果我们想尽快的恢复Redis服务，可以允许丢失一部分数据，那么可以禁用AOF，只使用RDB，使用RDB之所以比AOF快，是因为AOF是一条条命令的去执行的，直到最终状态，RDB是一次性把所有数据的最终状态刷到内存的 AOF日志文件的命令通过可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用flushall命令清空了所有数据，只要这个时候后台rewrite还没有发生，那么就可以立即拷贝AOF文件，将最后一条flushall命令给删了，然后再将该AOF文件放回去，就可以通过恢复机制，自动恢复所有数据 主从复制 ​ redis单节点存在单点故障问题，为了解决单点问题，一般都需要对redis配置从节点，然后使用哨兵来监听主节点的存活状态，如果主节点挂掉，从节点能继续提供缓存功能，从节点仅提供读操作，主节点提供写操作。对于读多写少的状况，可给主节点配置多个从节点，从而提高响应效率。 ​ 复制过程 从节点执行slaveof [masterIP] [masterPort]，保存主节点信息 ； 从节点中的定时任务发现主节点信息，建立和主节点的socket连接； 从节点发送Ping信号，主节点返回Pong，两边能互相通信； 连接建立后，主节点将所有数据发送给从节点（数据同步）； 主节点把当前的数据同步给从节点后，便完成了复制的建立过程； 接下来，主节点就会持续的把写命令发送给从节点，保证主从数据一致性。 哨兵 主从复制存在的问题 一旦主节点宕机，从节点晋升为主节点，同时需要修改应用方的主节点地址，还需要命令所有从节点去复制新的主节点，整个过程需要人工干预； 主节点的写能力受到单机的限制； 主节点的存储能力受到单机的限制； 原生复制的弊端在早期的版本中也会比较突出，比如：redis复制中断后，从节点会发起psync。此时如果同步不成功，则会进行全量同步，主库执行全量备份的同时，可能会造成毫秒或秒级的卡顿。 改善方式 哨兵模式是一种特殊的模式，Redis提供了哨兵命令，哨兵是一个独立的进程，原理是通过哨兵发送命令，然后等待Redis服务器的响应，进而实现对Redis实例的监控。 运行方式 通过命令的发送，Redis实例返回监控的运行状态，所有的Redis服务器 当master机器宕机后，会随机选择一个slave节点作为master，然后通过发布订阅模式通知其他slave节点，修改配置信息，更改跟随的主机 单哨兵模式相对来说不太可靠，毕竟会出现一言堂的情况，所以我们在使用哨兵的时候一般会采用多少兵模式，每一个哨兵都监控所有的Redis服务器，哨兵之间互相监控，当一个节点宕机后，只有指定数量的哨兵全部将其标记为下线，才会将节点移除 故障切换过程 主节点服务器宕机 哨兵1检测到主节点宕机，然后将其标记为客观下线，这个时候主节点还是主节点，并未进行failover过程 其他哨兵检测到主节点宕机，全部哨兵都会将主节点标记为客观下线 标记为客观下线的哨兵数量达到指定数量(sentinel.conf中配置，尽量设置为N/2+1)的时候，由一个哨兵(Raft选举)进行投票，根据投票结果决定是否进行主节点切换(选择优先级最高的，优先级可以通过slave-priority来设置，若优先级相同，则以复制偏移量最大的为主，若偏移量也全部相同，则选择服务ID最小的那个) 主节点切换完成之后，通过发布订阅模式让各个哨兵和从服务器更换主服务器配置，这个过程称为主观下线","categories":[{"name":"Redis","slug":"Redis","permalink":"http://luxiaowan.github.io/categories/Redis/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://luxiaowan.github.io/tags/面试/"}]},{"title":"服务治理思考","slug":"服务治理思考","date":"2020-03-24T05:21:00.000Z","updated":"2020-03-24T06:29:57.075Z","comments":true,"path":"2020/03/24/服务治理思考/","link":"","permalink":"http://luxiaowan.github.io/2020/03/24/服务治理思考/","excerpt":"","text":"前言 服务治理概念当下主要针对的是分布式架构下的多服务、微服务等。分布式系统由若干个大大小小的服务组成，服务之间通过HTTP或TCP的方式进行相互通信，促使整个系统活络起来。当分布式系统中的服务随着业务的发展不断变多时，我们就需要对服务进行相关的管理，不能一味的任由其野蛮增长。 思考 1. 节点管理，即服务提供者在注册中心及客户端创建的服务节点。 节点注册于注册中心，缓存于客户端，目的为若注册中心与服务端出现网络连通故障，但客户端与服务端网络连通正常，此时注册中心已将节点移除，但客户端在下一次同步注册中心节点之前仍可通过自身缓存的服务节点发送请求。 注册中心管理：服务提供者定时向注册中心发送心跳通知来证明其是存活状态，每次收到心跳通知均与上一次收到通知的时间进行比较，如果时差超出注册中心允许的最大值，则认为该服务提供者发生故障，将其从注册中心移除，随即通知监听客户端。 客户端管理：若注册中心与服务端网络连通出现问题，但客户端与服务端网络连接正常，直至下一次与注册中心同步之前仍可继续使用该节点。若客户端与服务端网络连通故障，但注册中心与服务端网络连通正常，则客户端会将该节点从缓存中移除直至下一次与注册中心进行同步，周而复始。 2. 负载均衡，顾名思义：平衡所有服务端处理请求的负载，防止某个服务端因接受过多请求导致服务故障。 随机算法：字面意思，简洁明了，就是采用随机数的方式选择本次请求所要转发的服务端，此法非常公平，不会因为服务端配置的优劣而对其另眼相看，绝对的公平！ 加权法：又叫轮询算法。本法则事在人为，完全按照主人的喜好行事，又称拍马屁，就好比食堂打饭，所有人围绕一个圈，如果打饭阿姨看到每个人的长相都一样，那么他对所有人都没有私心，从第一个开始每人给一勺，如此循环下去，谁都不会多谁也不会少，大家都均等，这就是大家的对注册中心来说权重都一样；如果打饭阿姨喜欢帅哥，看到长得帅的（比如我）每次都会多给一勺，其他人仍是一勺，此种情况对于注册中心而言，我的权重大于其他服务提供者，所以每一批请求中都会多分发给权重大的服务端。（此例不太恰当，换为吃饭：胖子和瘦子，胖子多吃，瘦子少吃，好像更好）。 最少活跃算法：这个拿吃饭来说吧，吃得多的碗落不下了，然后就少盛点，吃的少趁机多吃点均衡一下。上面也说了，打饭阿姨因为我长得帅，每次给别人打着饭呢都会不定时的拐到我这边给我加上一碗，递过来一碗饭，我桌子上的碗的数量就+1，等我吃完一碗饭将空碗回收后，桌子上的碗的数量就-1，但是打饭阿姨给的次数太过频繁，导致我面前很多碗，其他人面前的碗则很少，有人就向领导投诉，领导痛斥一顿后，阿姨则给面前碗最少的人开始打饭，这时此人碗的数量+1，然后阿姨重新统计，下一碗给统计后面前碗最少的人，这样大家都不至于被冷落，一旦落后，立刻照顾到。 一致性Hash算法：对每次请求的参数均计算hash，hash值相同的转发到同一个节点。上体育课1234报数排队，报到相同数字的站在一队，若某一队解散，由4队变成3队，则解散的这一队的人重新123报数，归并相关各队。（为什么不用吃饭举例了？因为再吃就撑死了！） 3. 服务路由 灰度访问：类似于单双号限行和不限行。一条马路刚修好，实行为期一个礼拜的单双号限行，一个礼拜之内无故障，则取消限行，大家都可以走。 就近原则：每次请求到达，客户端先关门在自己的局域网内查找可用的服务提供者，若有则直接调用，若未查到则出门浪。 配置分为静态配置和动态配置，这里不做解释了，字面意思！ 4. 服务容错：有容奶大！要有一颗包容的心！没错，是不是没发现奶非乃！😳 failover：拆开来就是fail over，也就是请求服务端a，然后a故障了，那就直接将请求转发给服务端b，结果b也故障了，那就再转发给c，直到成功！当然也可以设置最大转发次数，比如设置最大转发次数是两次，那么（划重点）在服务端2也故障时就不会转发给c了，直接返回给客户端告知失败！此方式为幂等的，也就是每一个服务提供方返回的数据均相等。 failback：遇到请求故障，那么就告知客户端请求失败，不再重试，然后根据返回的指令进行下一步操作。 failcache：遇到故障，就把请求缓存起来，间隔一段时间再发起请求，防止频繁请求影响服务端恢复。 failfast：遇到故障就返回，管他誓言有多真！绝不重试。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://luxiaowan.github.io/categories/分布式/"}],"tags":[{"name":"服务治理","slug":"服务治理","permalink":"http://luxiaowan.github.io/tags/服务治理/"}]},{"title":"OLTP和OLAP区别","slug":"OLTP和OLAP区别","date":"2020-03-24T03:15:00.000Z","updated":"2020-03-24T04:44:39.771Z","comments":true,"path":"2020/03/24/OLTP和OLAP区别/","link":"","permalink":"http://luxiaowan.github.io/2020/03/24/OLTP和OLAP区别/","excerpt":"","text":"我们在《MongoDB和Elasticsearch简单对比》一文中提到了OLAP和OLTP，然后我去问了几个技术人员，基本上知道这两个名词的人少之又少，当然这也情有可原，毕竟IT行业里名词太多了，我们来说一下这两者的区别 词义 OLTP：on-line Transaction Processing，联机(在线)事务处理 OLAP：on-line Analytical Processing，联机(在线)分析处理 从名词上我们就可以看出，OLTP主要是执行日常基本的事务处理，OLAP主要是执行日常的数据分析 特点 OLAP 实时性不高。比如ES中常见的使用日期检索日志 数据量大。ES利用其倒排索引的特点强化全文检索能力，即使有大量的日志打到ES中，我们仍然可以很快的查询出对应数据，效率贼高 动态检索纬度。我们在做数据分析时，数据的检索纬度是非常重要的一个条件，因为我们一般都是需要依据某一纬度做数据分析，这样才能将分析出来的数据提供给决策使用，不同的决策者需要的纬度不同，所以OLAP需要支持动态的检索纬度 OLTP 实时性高。既然是联机事务处理，那么对实时性要求肯定是一个高指标要求，会尽量杜绝出现数据变更不实时的情况 数据量不是很大。数据量过大会影响CRUD的性能 对确定性的数据进行操作。 高并发且满足ACID。 其他 OLTP一般是指我们常说的关系型数据库，或者说是支持频繁CRUD的数据存储媒介。 OLAP一般用于大数据处理和数据仓库，目前OLAP系统内的数据大多是针对OLTP内存储的数据做出进一步分析和应用，然后提供信息支持最终决策，对其大多是查多改少","categories":[{"name":"杂谈","slug":"杂谈","permalink":"http://luxiaowan.github.io/categories/杂谈/"}],"tags":[]},{"title":"MongoDB和Elasticsearch对比","slug":"MongoDB和Elasticsearch简单对比","date":"2020-03-24T02:09:00.000Z","updated":"2020-03-24T03:14:25.705Z","comments":true,"path":"2020/03/24/MongoDB和Elasticsearch简单对比/","link":"","permalink":"http://luxiaowan.github.io/2020/03/24/MongoDB和Elasticsearch简单对比/","excerpt":"","text":"前言 MongoDB和Elasticsearch都属于是NoSQL类型的数据存储媒介，两者有很大的一个相似度，但使用方式和场景还是有所区别的。 使用场景 MongoDB创立的初衷是为了干掉关系型数据库，和RDBMS算是竞争关系。 Elasticsearch起初就是以检索查询为主要应用场景出道，和RDBMS有点互相协助的意思。 相同点 数据存储格式为json 聚合和全文检索 CRUD 分片和复制 简单的join操作 适用于大数据量的处理 不支持事务 不同点 开发语言不同：ES的Java语言(restful)，Mongo是C++语言(driver)，从开发角度来看，ES对Java更方便 分片方式：ES是hash，Mongo是range和hash 分布式：ES的主副分片自动组合和配置，Mongo需要手动配置集群“路由+服务配置+sharding” 索引：ES自建倒排索引，检索力度强，Mongo手动创建索引（B树），不支持倒排索引，这点和RDBMS类似 检索字段：ES全文检索，可用的检索插件较多，Mongo对索引字段个数有限制，全文检索效率低乃至不采用 时效性：ES非实时，有丢数据的风险，Mongo实时，理论上无丢数据的风险 终 ES偏向于检索、查询和数据分析，适用于OLAP（on-line Analytical Processing）系统，Mongo偏向于大数据下的CRUD，适用于OLTP（on-line Transaction Processing）系统","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"http://luxiaowan.github.io/categories/NoSQL/"}],"tags":[]},{"title":"MySQL简单优化技巧","slug":"MySQL简单优化技巧","date":"2020-03-22T18:08:00.000Z","updated":"2020-03-23T14:48:44.504Z","comments":true,"path":"2020/03/23/MySQL简单优化技巧/","link":"","permalink":"http://luxiaowan.github.io/2020/03/23/MySQL简单优化技巧/","excerpt":"","text":"前言 一提到MySQL优化，大多数同学都比较依赖于DBA，但是对于程序员来说，掌握SQL的编写技巧其实很重要。 技巧 比较运算符能用“=”就不要用“&lt;&gt;”，因为“=”能够增大列索引的使用概率 如果只查询一条数据，那么就使用“limit 1”，告知查询游标找到第一个之后就返回，以免进行全表扫描 给列选择合适的类型，比如可以使用TINYINT代替INT，节省磁盘和内存的消耗 拆解复杂SQL，减少join的出现 若查询字段全部为某联合索引字段，则避免使用“SELECT * ”，*会造成回表 WHERE、ORDER BY、JOIN的列尽量使用索引字段 使用EXPLAIN查看执行计划 可以使用ENUM的时候不要用VARCHAR 字段尽量设置为NOT NULL，尤其是索引字段 长度比较大的字段尽量拆分为副表，如果这个字段不会被经常使用 经常发生变动的数据库尽量把查询缓存关闭，否则在每次变动的时候都要删除缓存，查询的时候也要查询和更新缓存，浪费时间 索引字段的长度尽量不要太长，毕竟一个索引数据页只有16k，如果一个索引内容过长，那么可能就会造成一个数据页只能存储一个索引字段，浪费空间","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/categories/MySQL/"}],"tags":[]},{"title":"JVM垃圾收集器浅谈","slug":"JVM垃圾收集器浅谈","date":"2020-03-22T03:15:00.000Z","updated":"2020-03-24T06:20:21.492Z","comments":true,"path":"2020/03/22/JVM垃圾收集器浅谈/","link":"","permalink":"http://luxiaowan.github.io/2020/03/22/JVM垃圾收集器浅谈/","excerpt":"","text":"前言 JVM是做Java的同学都必须要了解的东西，为什么这么说，因为我们只有知道了Java程序运行环境的配置和工作逻辑，才能对运行环境进行相关的优化和配置修改，让JVM在不同的服务器环境使用不同的配置，从而达到JVM环境最优化。 说到JVM就不得不说一下GC（garbage collection），垃圾收集的意思是找到垃圾并清理掉，但是常规的垃圾收集器却是找到正在被使用的对象，然后把其他的对象全部当作是垃圾对象清理掉。 写过C语言的同学都知道，在C语言中，我们需要手动的去管理内存，在使用内存之前我们需要先申请（malloc）一定大小的内存，使用完成之后需要手动的把使用的内存释放掉（free），如果忘记释放内存则很快会导致内存溢出， GC算法 引用计数法 为每个对象添加一个引用计数器，在对象被引用时，计数器+1，引用结束后，计数器-1，最终清除掉引用计数器为0的对象，并级联删除该对象引用的所有的对象，只保留引用计数不为0的对象。 这种算法看起来是不是很屌，是的，非常简单，只需要在对象被引用的时候串行修改引用计数器的值即可，但也容易出现一种问题：循环引用！循环引用就是几个废对象之间循环引用，尽管他们的引用计数器都不为0，但是在整个程序中却没有被使用，但是他们永远不会被回收，这样的对象多了之后很容易造成内存泄漏。 标记-清除 标记-清理-整理 可达性分析法","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[{"name":"GC","slug":"GC","permalink":"http://luxiaowan.github.io/tags/GC/"}]},{"title":"RMI远程服务调用简述","slug":"RMI远程服务调用简述","date":"2020-03-19T16:15:00.000Z","updated":"2020-03-19T17:50:31.146Z","comments":true,"path":"2020/03/20/RMI远程服务调用简述/","link":"","permalink":"http://luxiaowan.github.io/2020/03/20/RMI远程服务调用简述/","excerpt":"","text":"RMI是什么 RMI(Remote Method Invocation)意为远程方式调用，顾名思义，是Java版的RPC实现技术，是建立分布式Java应用程序的方便途径。RMI是基于接口的，一般是接口定义和实现分开在不同的工程中。 相似：Hessian，Burlap，Httpinvoker，webservice 为什么使用RMI 它允许运行在一台虚拟机上的方法调用运行在另一台虚拟机上的对象方法，这样可以让每个任务运行在更适合的虚拟机上。 RMI怎么用 定义一个java.rmi.Remote的子接口，也就是定义一个接口AnimalService，继承自Remote，接口中的所有方法必须显式的抛出java.rmi.RemoteException异常，否则服务会注册失败，谨记! 创建AnimalService的实现类DogServiceImpl，这个类需要继承java.rmi.server.UnicastRemoteObject 将服务AnimalService注册到rmi中心 客户端引用接口定义jar包 客户端获取AnimalService的远程服务 进行相关方法调用 缺点 因为RMI是Java版的RPC通讯技术，所以他只适用于Java程序上，如果想跨语言通讯，那就只能另谋它法了 代码 接口（工程：rmi-api） 123456789import java.rmi.Remote;import java.rmi.RemoteException;public interface AnimalService extends Remote &#123; // 接口中所有的方法必须声明throws RemoteException void laugh() throws RemoteException;&#125; 实现（工程：rmi-service） 12345678910111213141516import java.rmi.RemoteException;import java.rmi.server.UnicastRemoteObject;import cc.kevinlu.spidemo.spi.AnimalService;public class DogServiceImpl extends UnicastRemoteObject implements AnimalService &#123; protected DogServiceImpl() throws RemoteException &#123; super(); &#125; @Override public void laugh() throws RemoteException &#123; System.out.println(\"汪汪!\"); &#125;&#125; Server 123456789101112131415161718192021import java.rmi.Naming;import java.rmi.registry.LocateRegistry;import cc.kevinlu.spidemo.spi.AnimalService;public class Server &#123; public static void main(String[] args) throws Exception &#123; AnimalService dogService = new DogServiceImpl(); AnimalService lionService = new LionServiceImpl(); // 设置服务提供的端口 LocateRegistry.createRegistry(8891); // 设置rmi的host为127.0.0.1，否则可能会出现connect refused错误 System.setProperty(\"java.rmi.server.host\", \"127.0.0.1\"); // 发布服务 Naming.bind(\"rmi://127.0.0.1:8891/dogs\", dogService); System.out.println(\"dog service publish success!\"); &#125;&#125; 客户端（rmi-client：引用rmi-api） 正常情况 12AnimalService dogService = (AnimalService) Naming.lookup(\"rmi://127.0.0.1:8891/dogs\");dogService.laugh(); 反射的方式去回调方法 123Object obj = Naming.lookup(\"rmi://127.0.0.1:8891/dogs\");Method method = obj.getClass().getMethod(\"laugh\");method.invoke(obj, null);","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[]},{"title":"设置生成CGLib和JDK代理生成的class文件","slug":"设置生成CGLib和JDK代理生成的class文件","date":"2020-03-17T13:20:00.000Z","updated":"2020-03-17T13:54:16.360Z","comments":true,"path":"2020/03/17/设置生成CGLib和JDK代理生成的class文件/","link":"","permalink":"http://luxiaowan.github.io/2020/03/17/设置生成CGLib和JDK代理生成的class文件/","excerpt":"","text":"jdk和cglib代理方式属于是老生常谈了，这里就不说了，我们说一下特别的。 我们都知道cglib是针对于类，jdk是针对于接口， cglib在目标类被代理后会自动生成目标类的子类，也就是xxxclass$$EnhancerByCGLIB$$c03f68c4.class jdk代理后会自动生成目标接口的实现，也就是$Proxy0.class 我们平时代码在编译过程中是不会生成代理类的class文件，只有在运行中才会生成 我们可以通过在启动类中设置代理类生成路径 1234// 设置CGLib代理类的生成位置System.setProperty(DebuggingClassWriter.DEBUG_LOCATION_PROPERTY, \"./cg\");// 设置JDK代理类的输出System.getProperties().put(\"sun.misc.ProxyGenerator.saveGeneratedFiles\", \"true\");","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[]},{"title":"Linux更换yum源","slug":"Centos更换yum源","date":"2020-03-12T05:05:00.000Z","updated":"2020-03-12T13:35:37.430Z","comments":true,"path":"2020/03/12/Centos更换yum源/","link":"","permalink":"http://luxiaowan.github.io/2020/03/12/Centos更换yum源/","excerpt":"","text":"我们安装的Linux虚拟机或购买的云服务器上默认使用的yum源在国内有时候会传输很慢，现在大多我们在使用Linux之前都会修改一下yum源，以下以centos为例，将yum源修改为阿里云的。可以到https://developer.aliyun.com/mirror/中查看帮助 备份机器中的源文件 备份的目的是为了操作失败后可以随时回滚 1mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak 下载新的 CentOS-Base.repo 到 /etc/yum.repos.d/ 我们的yum源配置文件在/etc/yum.repos.d/目录中，所以我们将阿里云的repo文件下载到该目录下（可以根据系统版本到http://mirrors.aliyun.com/repo/查看对应的文件） 123456# 要下载与系统版本一致的repo文件wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo或curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 运行yum makecache生成缓存 问题 若出现Couldn’t resolve host 'mirrors.cloud.aliyuncs.com’的信息，则表示网络不通，可以使用下面命令修改repo文件： 1sed -i -e '/mirrors.cloud.aliyuncs.com/d' -e '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo","categories":[{"name":"Linux","slug":"Linux","permalink":"http://luxiaowan.github.io/categories/Linux/"}],"tags":[]},{"title":"kubernates调整nodePort端口范围","slug":"kubernates调整nodePort端口范围","date":"2020-03-11T08:02:00.000Z","updated":"2020-03-11T08:12:05.657Z","comments":true,"path":"2020/03/11/kubernates调整nodePort端口范围/","link":"","permalink":"http://luxiaowan.github.io/2020/03/11/kubernates调整nodePort端口范围/","excerpt":"","text":"默认情况下，k8s 集群 nodePort 分配的端口范围为：30000-32767，如果我们指定的端口不在这个范围就会报错： Error: release kong failed: Service “xxx” is invalid: spec.ports[0].nodePort: Invalid value: 12306: provided port is not in the valid range. The range of valid ports is 30000-32767 所以我们修改 /etc/kubernetes/manifests/kube-apiserver.yaml 文件，在 command 下添加 --service-node-port-range=1-65535 参数，添加 nodePort 范围参数后会自动生效，无需进行其他操作： vim /etc/kubernetes/manifests/kube-apiserver.yaml","categories":[{"name":"kubernates","slug":"kubernates","permalink":"http://luxiaowan.github.io/categories/kubernates/"}],"tags":[]},{"title":"Centos7.7安装kubernates集群","slug":"Centos7.7安装kubernates集群","date":"2020-03-11T05:11:00.000Z","updated":"2020-03-11T06:52:52.764Z","comments":true,"path":"2020/03/11/Centos7.7安装kubernates集群/","link":"","permalink":"http://luxiaowan.github.io/2020/03/11/Centos7.7安装kubernates集群/","excerpt":"","text":"基础 本文主要讲解使用kubeadm搭建高可用的集群，这种方式是最简单最快的。 安装步骤 我们安装k8s的机器资源条件如下： centos7.7 内存不低于2G，CPU不少于2核，否则在安装的时候会报错 集群中的所有机器都要保证网络连通性 相关端口开放 swap关闭 更新系统 在开始安装服务之前，我们先更新一下yum源，然后安装相关的软件 12345# 更新yum源yum update# 安装git(可选)yum install git 禁用swap分区 12345# 关闭swap分区，该命令只是临时关闭，机器重启后还会自动打开swapoff -a# 永久性关闭swap分区，禁止机器重启后自动打开sed -i '/ swap / s/^/#/' /etc/fstab 更换yum源为国内镜像 centos的yum源默认为国外的，如果你的服务器是在国内，那么可能访问不了，所以我们需要把yum的源更换为国内的 123456# 这里有一个注意点，就是下面的Centos-7.repo，这里因为我们使用的centos7，如果你的系统是centos8，那么就改成Centos-8.repo，也就是改成相对应的版本，否则yum安装不了软件cd /etc/yum.repos.d &amp;&amp; \\sudo mv CentOS-Base.repo CentOS-Base.repo.bak &amp;&amp; \\sudo wget -O CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo &amp;&amp; \\yum clean all &amp;&amp; \\yum makecache 安装docker环境 k8s内部可以支持多种容器，我们最常使用的就是docker，所以我们这里也以docker为基础 1234567891011121314151617181920212223242526272829303132333435363738394041# 安装docker依赖包yum install yum-utils device-mapper-persistent-data lvm2# 添加docker库yum-config-manager --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo# 更新源yum update# 安装dockeryum install containerd.io-1.2.10 \\ docker-ce-19.03.4 \\ docker-ce-cli-19.03.4# 配置docker daemonmkdir /etc/dockercat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123; \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": &#123; \"max-size\": \"100m\" &#125;, \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ]&#125;EOF# 创建docker.service.dmkdir -p /etc/systemd/system/docker.service.d# 启用docker.servicesystemctl enable docker.service# 重载&amp;重启dockersystemctl daemon-reloadsystemctl restart docker 安装完之后使用docker -v查看版本 更换docker为国内源 123456789# 配置tee /etc/docker/daemon.json &lt;&lt;-'EOF'&#123;\"registry-mirrors\": [\"https://registry.docker-cn.com\"]&#125;EOF# 重启dockerservice docker restart 安装kubeadm、kubelet、和kubectl kubeadm 负责引导集群，kubelet 在集群的所有节点运行，负责启动 pods 和 containers，kubectl 则负责与集群交互，我们需要在所有节点安装这些组件 配置k8s国内源 我们把k8s的源修改为阿里云的 1234567891011# 配置国内源cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF 关闭SELinux 12setenforce 0sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config 配置网络参数 123456789cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system# 确保 br_netfilter 模块已经加载modprobe br_netfilter 安装并启动kubeadm、kubelet、和kubectl 12345# 安装yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes# 启动systemctl enable --now kubelet 使用kubeadm初始化集群 因为我的服务器在国内，且由于 kubeadm 初始化集群的依赖镜像在国内访问不了，所以初始化集群之前先使用国内源拉取依赖镜像 拉取依赖镜像 1234567891011# 获取依赖镜像列表kubeadm config images list# 使用阿里源下载 K8s 依赖镜像kubeadm config images list |sed -e 's/^/docker pull /g' -e 's#k8s.gcr.io#registry.cn-hangzhou.aliyuncs.com/google_containers#g' |sh -x# 通过docker tag修改镜像与国外镜像名称相同，这样的目的是在初始化的时候，docker会先在本地查找，若本地已有镜像则不会再去远程拉取，等于来了一招瞒天过海docker images |grep registry.cn-hangzhou.aliyuncs.com/google_containers |awk '&#123;print \"docker tag \",$1\":\"$2,$1\":\"$2&#125;' |sed -e 's#registry.cn-hangzhou.aliyuncs.com/google_containers#k8s.gcr.io#2' |sh -x# 删除原镜像，这个可选docker images |grep registry.cn-hangzhou.aliyuncs.com/google_containers |awk '&#123;print \"docker rmi \", $1\":\"$2&#125;' |sh -x master节点初始化 我们使用kubeadm init指令初始化master节点，具体的参数可参考官方文档：https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/ 12# local_ip是本机局域网ip，kubectl_version是我们镜像的版本kubeadm init --apiserver-advertise-address=&lt;local_ip&gt; --kubernetes-version=&lt;kubectl_version&gt; --pod-network-cidr=10.244.0.0/16 --v=5 执行成功之后，日志会打印出下面语句，并且会告知我们节点加入的方式 1234# root或非root用户均可执行mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 或 1export KUBECONFIG=/etc/kubernetes/admin.conf 开启使用master节点资源 默认master节点是不参与pod调度的，这样对于master节点的资源来说有点太过浪费，所以我们通过下面的命令使master节点也参与pod调度 1kubectl taint nodes --all node-role.kubernetes.io/master- 添加网络组件 我们通过kubectl get nodes查看集群内的节点，当前应该只有master一个节点，但是节点的状态为NotReady，查看coredns的pod（kubectl get pod --all-namespaces），会发现coredns处于pending状态，原因就是我们还未安装网络组件。 12# 网络组件我们选择WeaveNet，安装完之后稍等一会就可以了kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\" 向集群中添加node节点 每一台node都需要操作从1~6的步骤，然后我们使用kubeadm join来添加到集群中，这里的token是在master节点创建的，我们可以使用kubeadm token list命令查看可用的token，如果没有可用的token，那么我们使用kubeadm token create创建新的token，一个token的有效期为24小时 12kubeadm join 192.168.0.54:6443 --token 9dmyq2.c50cvh32r62o6jlx \\ --discovery-token-ca-cert-hash sha256:4640dd5d3788968d86ce3cb792c1e368586ee6731de5a07ad8ad331926a2f233 验证 加入之后我们在master节点通过kubectl get nodes来查看所有的节点，验证是否加入成功。","categories":[{"name":"kubernates","slug":"kubernates","permalink":"http://luxiaowan.github.io/categories/kubernates/"}],"tags":[]},{"title":"Centos中安装rz和sz替代ftp","slug":"Centos中安装rz和sz替代ftp","date":"2020-03-10T04:44:00.000Z","updated":"2020-03-10T04:47:26.799Z","comments":true,"path":"2020/03/10/Centos中安装rz和sz替代ftp/","link":"","permalink":"http://luxiaowan.github.io/2020/03/10/Centos中安装rz和sz替代ftp/","excerpt":"","text":"lrzsz 官网入口：http://freecode.com/projects/lrzsz/ 12# 安装lrzszyum install -y lrzsz 安装完成之后就可以直接使用了 12345# 上传文件rz# 下载文件sz 操作很简单","categories":[{"name":"Linux","slug":"Linux","permalink":"http://luxiaowan.github.io/categories/Linux/"}],"tags":[]},{"title":"k8s中YAML文件解释","slug":"k8s中YAML文件解释","date":"2020-03-09T16:30:00.000Z","updated":"2020-03-09T16:34:54.823Z","comments":true,"path":"2020/03/10/k8s中YAML文件解释/","link":"","permalink":"http://luxiaowan.github.io/2020/03/10/k8s中YAML文件解释/","excerpt":"","text":"YAML语法规则 大小写敏感 使用缩进表示层级关系 缩进时不允许使用Tal键，只允许使用空格 缩进的空格数目不重要，只要相同层级的元素左侧对齐即可 ”#” 表示注释，从这个字符一直到行尾，都会被解析器忽略 在Kubernetes中，只需要知道两种结构类型即可： Lists Maps ####YAML属性解释 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# yaml格式的pod定义文件完整内容：apiVersion: v1 #必选，版本号，例如v1kind: Pod #必选，Podmetadata: #必选，元数据 name: string #必选，Pod名称 namespace: string #必选，Pod所属的命名空间 labels: #自定义标签 - name: string #自定义标签名字 annotations: #自定义注释列表 - name: stringspec: #必选，Pod中容器的详细定义 containers: #必选，Pod中容器列表 - name: string #必选，容器名称 image: string #必选，容器的镜像名称 imagePullPolicy: [Always | Never | IfNotPresent] #获取镜像的策略 Alawys表示下载镜像 IfnotPresent表示优先使用本地镜像，否则下载镜像，Nerver表示仅使用本地镜像 command: [string] #容器的启动命令列表，如不指定，使用打包时使用的启动命令 args: [string] #容器的启动命令参数列表 workingDir: string #容器的工作目录 volumeMounts: #挂载到容器内部的存储卷配置 - name: string #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名 mountPath: string #存储卷在容器内mount的绝对路径，应少于512字符 readOnly: boolean #是否为只读模式 ports: #需要暴露的端口库号列表 - name: string #端口号名称 containerPort: int #容器需要监听的端口号 hostPort: int #容器所在主机需要监听的端口号，默认与Container相同 protocol: string #端口协议，支持TCP和UDP，默认TCP env: #容器运行前需设置的环境变量列表 - name: string #环境变量名称 value: string #环境变量的值 resources: #资源限制和请求的设置 limits: #资源限制的设置 cpu: string #Cpu的限制，单位为core数，将用于docker run --cpu-shares参数 memory: string #内存限制，单位可以为Mib/Gib，将用于docker run --memory参数 requests: #资源请求的设置 cpu: string #Cpu请求，容器启动的初始可用数量 memory: string #内存清楚，容器启动的初始可用数量 livenessProbe: #对Pod内个容器健康检查的设置，当探测无响应几次后将自动重启该容器，检查方法有exec、httpGet和tcpSocket，对一个容器只需设置其中一种方法即可 exec: #对Pod容器内检查方式设置为exec方式 command: [string] #exec方式需要制定的命令或脚本 httpGet: #对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port path: string port: number host: string scheme: string HttpHeaders: - name: string value: string tcpSocket: #对Pod内个容器健康检查方式设置为tcpSocket方式 port: number initialDelaySeconds: 0 #容器启动完成后首次探测的时间，单位为秒 timeoutSeconds: 0 #对容器健康检查探测等待响应的超时时间，单位秒，默认1秒 periodSeconds: 0 #对容器监控检查的定期探测时间设置，单位秒，默认10秒一次 successThreshold: 0 failureThreshold: 0 securityContext: privileged:false restartPolicy: [Always | Never | OnFailure]#Pod的重启策略，Always表示一旦不管以何种方式终止运行，kubelet都将重启，OnFailure表示只有Pod以非0退出码退出才重启，Nerver表示不再重启该Pod nodeSelector: obeject #设置NodeSelector表示将该Pod调度到包含这个label的node上，以key：value的格式指定 imagePullSecrets: #Pull镜像时使用的secret名称，以key：secretkey格式指定 - name: string hostNetwork:false #是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络 volumes: #在该pod上定义共享存储卷列表 - name: string #共享存储卷名称 （volumes类型有很多种） emptyDir: &#123;&#125; #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值 hostPath: string #类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录 path: string #Pod所在宿主机的目录，将被用于同期中mount的目录 secret: #类型为secret的存储卷，挂载集群与定义的secre对象到容器内部 scretname: string items: - key: string path: string configMap: #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部 name: string items: - key: string","categories":[{"name":"kubernates","slug":"kubernates","permalink":"http://luxiaowan.github.io/categories/kubernates/"}],"tags":[]},{"title":"Git修改已提交用户信息","slug":"Git修改已提交用户信息","date":"2020-02-11T08:46:00.000Z","updated":"2020-02-11T10:03:16.813Z","comments":true,"path":"2020/02/11/Git修改已提交用户信息/","link":"","permalink":"http://luxiaowan.github.io/2020/02/11/Git修改已提交用户信息/","excerpt":"","text":"修改Git用户名和邮箱 修改某一项目配置 终端进入到项目所在目录下，执行以下命令 123git config user.name \"cc\"git config user.email \"cc@cc.cc\" 修改全局配置 打开终端，执行以下命令 123git config --global user.name \"cc\"git config --global user.email \"cc@cc.cc\" 修改项目最近一次提交信息 修改提交用户 1git commit --amend --author=\"username &lt;email&gt;\" 修改提交备注信息 1git commit --amend 然后执行之后跳转到新的页面 修改顶部备注信息然后保存即可","categories":[{"name":"Git","slug":"Git","permalink":"http://luxiaowan.github.io/categories/Git/"}],"tags":[]},{"title":"Linux禁止root用户远程登录","slug":"Linux禁止root用户远程登录","date":"2020-02-11T03:35:00.000Z","updated":"2020-02-11T05:05:54.080Z","comments":true,"path":"2020/02/11/Linux禁止root用户远程登录/","link":"","permalink":"http://luxiaowan.github.io/2020/02/11/Linux禁止root用户远程登录/","excerpt":"","text":"添加一个新用户 添加新用户 useradd cc 设置新用户密码 passwd cc 修改/etc/sudoers文件 找到## Allow root to run any commands anywhere 在root ALL=(ALL) ALL下方添加语句cc ALL=(ALL) ALL 此文件为readonly文件，保存使用wq!命令 修改/etc/ssh/sshd_config文件 找到PermitRootLogin yes修改为PermitRootLogin no 保存之后执行service sshd restart命令即可","categories":[{"name":"Linux","slug":"Linux","permalink":"http://luxiaowan.github.io/categories/Linux/"}],"tags":[]},{"title":"MacBook连接iPhone不稳定","slug":"MacBook连接iPhone不稳定","date":"2020-02-11T01:32:00.000Z","updated":"2020-02-11T01:34:27.294Z","comments":true,"path":"2020/02/11/MacBook连接iPhone不稳定/","link":"","permalink":"http://luxiaowan.github.io/2020/02/11/MacBook连接iPhone不稳定/","excerpt":"","text":"问题 MacBook经常在使用USB连接iPhone的时候不稳定，连接一跳一跳的 解决 在电脑终端下运行sudo killall -STOP -c usbd，然后输入电脑密码，然后重新插上连接线，就OK了","categories":[{"name":"杂谈","slug":"杂谈","permalink":"http://luxiaowan.github.io/categories/杂谈/"}],"tags":[]},{"title":"头条、美团、滴滴、京东等大厂面试题","slug":"刷题","date":"2019-12-24T16:35:00.000Z","updated":"2019-12-24T16:36:27.409Z","comments":true,"path":"2019/12/25/刷题/","link":"","permalink":"http://luxiaowan.github.io/2019/12/25/刷题/","excerpt":"","text":"头条 聊项目，画项目架构图，画一个用户从发起请求到接收到响应，中间经过哪些服务，每个服务做什么事情的流程图。 讲项目中的难点、挑战，你是如何解决的？ Redis 中有几种类型 &amp; 各自底层怎么实现的 &amp; 项目中哪个地方用了什么类型，怎么使用的？ Redis如何实现分布式锁，zk如何实现分布式锁，两者的区别。如果service还没执行完，分布式锁在Redis中已经过期了，怎么解决这种问题？ synchronized底层实现，加在方法上和加在同步代码块中编译后的区别、类锁、对象锁。 锁升级的过程。 Java运行时区域及各个区域的作用、对GC的了解、Java内存模型及为什么要这么设计？ 对索引的理解，组合索引，索引的最佳实践 countDownLatch用过没有，在项目中如何使用的，对AQS的了解。 写生产者消费者问题，考虑高并发的情况，可以使用Java 类库，白纸写代码。 设计一个发号器，考虑集群和高并发的情况，要求发号器生成的id是递增趋势，通过id可以区分出来是今天生成的id还是昨天生成的id，但是生成的id中不能直接带有日期，要具有一定的混淆功能，白纸写代码。 一个二位数组，每个元素都可以往上下左右四个方向走，寻找最长递增路径。如下图所示，最长递增路径即红色字体路径。白纸写代码。 ![image-20190924230411189](/Users/chuan/Library/Application Support/typora-user-images/image-20190924230411189.png) 美团 数据库和缓存的一致性问题。先更新数据库，再更新缓存，若更新完数据库了，还没有更新缓存，此时有请求过来了，访问到了缓存中的数据，怎么办？ 聚簇索引/非聚簇索引，MySQL索引底层实现，为什么不用B-Tree，为什么不用hash，叶子结点存放的是数据还是指向数据的内存地址，使用索引需要注意的几个地方？ MySQL默认的事务隔离级别，MVCC、RR怎么实现的？RC如何实现的？ MySQL间隙锁有没有了解，死锁有没有了解，写一段会造成死锁的SQL语句，死锁发生了如何解决，MySQL有没有提供什么机制去解决死锁 谈下对GC的了解，何为垃圾，有哪些GC算法，有哪些垃圾回收器，cms和g1的区别，还有一个直击灵魂的问题，看过cms的源码吗？ 有没有排查过线上OOM的问题，如何排查的？ 有没有使用过JVM自带的工具，如何使用的？ 假设有下图所示的一个Full GC 的图，纵向是内存使用情况，横向是时间，你如何排查这个Full GC 的问题，怎么去解决你说出来的这些问题？ ![image-20190924230348754](/Users/chuan/Library/Application Support/typora-user-images/image-20190924230348754.png) 说说对Java中集合类的理解，项目中用过哪些，哪个地方用的，如何使用的？ 对CAS的理解，CAS带来的问题，如何解决这些问题？ volatile底层、synchronized底层、锁升级的过程、MESI Ehcache支持哪些缓存？ JUC有研究没有，讲一讲？ 聊项目，画项目架构图，画一个用户从发起请求到接收到响应，中间经过哪些服务，每个服务做什么事情的流程图。 讲项目中的难点、挑战，如何解决的，项目这一块会问的特别细。 如何保证RocketMQ 消息的顺序性，如何解决重复消费问题。 项目中如何保证接口的幂等操作。 讲一讲对Redis 的了解，项目中如何使用的，哪个地方使用的，为什么要使用？ 哨兵机制、Redis 两种备份方式的区别，项目中用的哪种，为什么？ 讲一讲对分布式锁的了解 项目中系统监控怎么做的？ 如何理解Spring中的AOP 和 IOC，以及DI，读过Spring源码没有？ 读过MyBatis源码没有？ 说一个你了解最多的框架，说出你的理解。 如何理解分布式事务，为什么会出现这个问题，如何去解决，了解哪些分布式事务中间件？ 聊一聊对分库分表的理解。 Hystrix功能和在项目中怎么使用的？Hystrix怎么检测断路器是否要开启/关闭？Hystrix实现原理？除Hystrix之外的其他熔断限流中间件有了解没有，了解多少说多少？ Dubbo有了解没有？ 怎么理解Java 中和 MySQL中的乐观锁、悲观锁？ 一致性hash 滴滴 聊项目，画项目架构图，画一个用户从发起请求到接收到响应，中间经过哪些服务，每个服务做什么事情的流程图，讲数据库设计。 处理过线上OOM问题没有，如何处理的？ 遇到过线上服务器CPU飙高的情况没有，如何处理的？ 线上有没有遇到其他问题，如何处理的？ 对线程池的理解，项目中哪个地方使用了，如何使用的，用的Excutor框架中的哪个实现类，为什么用这个 对CAS的理解，CAS带来的问题，如何解决这些问题? volatile底层、synchronized底层、锁升级的过程、MESI 对MySQL索引的理解、对组合索引的理解、索引的最佳实践 分布式锁的实现、对比Redis分布式锁 &amp; ZK分布式锁 唯一ID如何实现的，Snowflake实现原理，Snowflake有哪些问题，如何避免根据订单号可以推算出今天的订单量 如果线上一个功能是用栈结构实现的，使用过程中要注意哪些问题，为什么？ 怎么理解线程安全？ 怎么理解接口幂等？项目中如何保证的接口幂等？ 怎么理解微服务，服务如何划分，可以从哪几个方面去划分，为什么这样划分，微服务带来了哪些好处，哪些坏处，如何看待这个问题？ 如何理解网关，网关带来的好处和坏处，如何解决 Hystrix功能 &amp; 在项目中怎么使用的 &amp; Hystrix怎么检测断路器是否要开启/关闭 &amp;Hystrix实现原理 怎么理解命令模式和观察者模式，手写一个观察者模式或者命令模式的代码，策略模式也行 掌握哪些设计模式，常用哪些，项目中如何使用的，为什么用这个，不用那个？手写一个线程安全的单例模式 如何设计一个秒杀系统？ 如果我现在就是要实现每秒10w请求，不能熔断限流，如何去设计？ 假设现在双十一零点，大量下单请求，如何对这些订单进行分库分表，为什么？ 服务A调用服务B中一个接口，服务B调用服务C中一个接口，如何实现若服务B响应服务A成功，则服务C一定响应服务B成功，需要考虑系统性能问题？ 递归使用中有什么需要注意的地方，递归写法一般可以用什么去替换？ 有两个表，table a，table b，写SQL查询出仅在table a中的数据、仅在table b中的数据、既在table a 又在table b 中的数据？ Spring 源码有了解没有？ MyBatis源码有了解没有？ MySQL事务隔离级别、MVCC？ 京东 一个final修饰的属性，定义的时候没有初始化，在无参构造函数中初始化，可以吗，为什么 说说对Java中集合类的理解，项目中用过哪些，哪个地方用的，如何使用的，为什么不用其他的集合类 HashMap，concurrentHashMap底层实现 List删除是怎么实现的，遍历的时候可以删除吗？为什么? Redis中有哪些数据结构，了解过其底层怎么实现的吗，和Java中相似的数据结构的对比？ Redis是单线程的还是多线程的，为什么这么快？ Redis Hash中某个key过大，变为String类型的大key，怎么处理，使用中如何避免出现这种问题? 设计模式在项目中哪个地方用到了，怎么使用的，能不能画一个你熟悉的设计模式的UML图，手写单例模式，手写静态内部类实现的单例模式。 讲一讲MySQL索引，实际工作中，哪些场景用了B+Tree索引，哪些场景用了hash索引？ explain 可以看到哪些信息，什么信息说明什么，explain的结果列讲一下 Spring源码看过没有，会多少讲多少？ MyBatis源码看过没有，会多少讲多少？ CAS的缺点，如何解决？ AQS、countDownLatch如何实现？ 线程池如何实现，核心线程数和最大线程数设置成多少，为什么这么设置，项目中哪个地方使用了线程池，使用时需要注意什么 MySQL事务隔离级别，幻读，脏读，项目中用什么事务隔离级别，为什么？ volatile底层原理、synchronized实现机制 对XA、TCC的理解，了解哪些分布式事务框架，有什么缺点？ Feign 和 Dubbo，了解多少说多少？ Eureka 和 Zookeeper，了解多少说多少？ Hystrix 和 sentinel，了解多少说多少？ Spring Cloud Alibaba，了解多少说多少？ 对分库分表、读写分离的了解，了解多少说多少？ 画一下Java 线程几个状态及状态之间互相转换的图？ 聊项目，画项目架构图，画一个用户从发起请求到接收到响应，中间经过哪些服务，每个服务做什么事情的流程图，讲数据库设计具体到部分表中有哪些字段？ 部门体量比较大，可能需要加班，到凌晨两三点的那种，也可能通宵，通宵是大促期间，你能接受吗？ 也会加班到十点，这个不是大促期间，但也不是每天，非常态情况，你能接受吗，你在哪里住，过来要多久，有男朋友吗？ 火币 Kafka 如何保证消息顺序消费、在consumer group 中新增一个consumer 会提高消费消息的速度吗、那如果我想提高消息消费的速度，我要怎么办？ Redis几种数据结构及底层，项目中如何使用的Redis？ 哨兵机制、选举算法 一致性hash Redis是单线程的还是多线程的，为什么速度这么快？ 多路复用的几种方式以及区别？ 对线程池的理解，在项目中如何使用的，多个线程之间如何共享数据，多个进程之间如何共享数据？ HashMap、concurrentHashMap的区别及底层实现、HashMap和HashTable 的区别？ 什么是红黑树，什么是B-Tree，为什么HashMap中用红黑树不用其他树？ 对MySQL索引的理解，为什么MySQL索引中用B+Tree，不用B-Tree 或者其他树，为什么不用hash 索引？ 数据库和缓存的双写一致性问题？ 每日一淘 用过哪些Object类的方法，如何使用的 Java如何实现序列化的，Serialization底层如何实现的 countDownLatch如何实现的 项目中监控报警机制如何做的，说说你的了解 线上服务器CPU飙高，如何处理这个问题 服务A调用服务B，用户请求服务A，发现返回较慢，如何定位这个问题 TIME_WAIT是什么状态还记得吗，什么情况下网络会出现这个状态 linkedme 内核态和用户态、cas 和 sout 哪个用到了内核态和用户态的切换 哪些典型的应用用的是UDP？ 线程池有了解吗，项目中如何使用的？ 计算密集型/IO密集型任务分别如何设置线程池的核心线程数和最大线程数，为什么这么设置？ 假如我下午5点要和5个人一起开会，但是这5个人现在都出去了，不在公司，但是今天会回来，问，我如何开这场会，用Java 并发方面的知识回答。 算法题 [1,1,2,2,3,4,4,5,5,5] 找出不重复的元素（黄包车） 反转链表，要求时间复杂度O(N)，空间复杂度O(1) （火币） 非递归实现斐波那契数列 （爱奇艺） 这一周股市价格为[2,6,1,4,8]，求哪一天买入哪一天卖出，可获得最大收益，最大收益为多少 （爱奇艺） 按照箭头方向查找二叉树 （金山云） ![image-20190924230728819](/Users/chuan/Library/Application Support/typora-user-images/image-20190924230728819.png) 表a b c之间用ID关联，求阴影部分的数据 （金山云） ![image-20190924230750484](/Users/chuan/Library/Application Support/typora-user-images/image-20190924230750484.png) 一个整形无序数组，里面三个数只和等于一个目标值，求这三个数 （小米） 链表问题 （小米） ![image-20190924230830166](/Users/chuan/Library/Application Support/typora-user-images/image-20190924230830166.png) 扑克牌问题 （小米） 有十张扑克牌，从上面开始抽，抽出一张放桌子上，然后再抽出一张放扑克牌的最下面，这样循环往复的操作，直到手里的牌都没有了。这时，桌子上牌的顺序正好是1 2 3 4 5 6 7 8 9 10。要求写代码求出原顺序 手写大顶堆 （linkedMe） 手写LRU 算法 （火币） 字符串相加 （滴滴） 两个数字类型的字符串，直接转int或者double肯定都放不下，然后求这两个数的和，返回值还是字符串，15分钟时间，要求无Bug 寻找目标值位置 （滴滴） 有一个二维数组，数组横向有序，纵向有序，求目标值的位置，10分钟时间 求字符串“efabcbaefehiabcba”中最长的回文数，不去重（美团） 反转int类型的值x，不要借用String，只用int 即可。&amp;&amp; 针对该程序，写出其应有的测试用例 （美团） top K 问题（每日一淘）","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://luxiaowan.github.io/tags/面试/"}]},{"title":"工程师和程序员的区别","slug":"工程师和程序员的区别","date":"2019-12-20T17:03:00.000Z","updated":"2019-12-20T17:03:48.136Z","comments":true,"path":"2019/12/21/工程师和程序员的区别/","link":"","permalink":"http://luxiaowan.github.io/2019/12/21/工程师和程序员的区别/","excerpt":"","text":"开一个杂谈的类目，探讨平时工作中遇到的趣事，可能偶尔也会转载一些其他地方看到的有意思的行业趣事，纯属一乐。 开端 今天下班后，公司里一个工作多年的小朋友找我闲聊，然后聊着聊着不知为啥聊到了行业上： 12345678小朋友：串串，你为什么不愿意带团队？串 串：不感兴趣！小朋友：你打算敲代码到35岁吗😅串 串：40小朋友：就算技术上再牛逼，程序员在中国，到了四十也到头了吧串 串：工程师和程序员是两码事小朋友：本质区别在哪呢？你指有架构的能力吗？串 串：。。。。。。 我想说 其实聊到这里，我突然发现很多做开发的同学对程序员和工程师这两个职业的认知好像并不是那么的分明，其实上面的对话还没有结束，我把工程师和程序员对等为进程和线程，做开发的同学应该都知道进程和线程的区别(说不了解的回去把操作系统再细学一遍)，为什么这么比喻呢(其实这个比喻也不恰当，就是想把两者的区别扩大化一下，各位不要挑这个比喻啦，挑了我也不改)，因为进程是由很多线程组成的，每一个线程都只处理进程中很小很小的一个模块，可能仅仅就是去把磁盘上的数据读到内存中而已，进程就像是一片森林，线程只是一棵树，我想表达的意思就是工程师所要掌握的知识技能，要比程序员多的多的多，也就是需要有大局观、知识广度、知识深度、行军线路等，而程序员是什么？你可以认为会写代码的都叫程序员，但只会写代码的程序员不能叫做工程师!!! 我还想说 我们来看一下BOSS直聘上招聘程序员和工程师岗位JD的区别，这里可能要有图： 看到没，同样是高级岗位，但是岗位JD差别却很大，我们来分析一下区别： 1234Java高级程序员： Java高级研发工程师： 无学历要求，会写代码就行 最低科班出身 能用技术框架写代码，其他低要求 业务、产品、研发均要擅长，自我驱动，技术攻坚，架构设计 强调技术要求，罗列一堆技术框架 无技术框架要求，对软技能提到多次(代码洁癖、计算机原理、算法、行业经验、职责、质量、业务/产品/设计/研发) 我们从分析结果可以看出，一样的职级，一样的岗位，一样的技术，对程序员的要求是你会写代码就行，对工程师的要求是你不仅要会写代码，还要懂业务、产品、设计、产品和研发质量、数据结构和算法、计算机原理、软件工程学，现在是不是很清晰的知道了程序员和工程师的区别？ 我再说一下 很多人会问：为什么国外的程序员到60岁还可以写代码，而中国的程序员35岁就要被淘汰？ 其实行业淘汰的是程序员而不是工程师，一个有思想有眼界有能力的工程师是每一个公司都想聘用的，这一类人只会越老越吃香。 那么程序员为什么到35岁就会被淘汰呢？因为35岁还只是会写代码的人，身体已经被加班摧残的&quot;风烛残年&quot;了，每一年都会有一大批毕业生/培训生走入社会，他们身体健康、强壮、任劳任怨、爱加班，身为35岁的&quot;老年人&quot;，除了年龄大、发际线高、体重超标、脑子迟钝，还有哪一点比得过那么一群小鲜肉？哦~还有上有老下有小的山一般大的压力!!! 那么工程师为什么越老越吃香呢？因为工程师靠的不是写代码，靠的是自我沉淀和行业经验。 总结下 大家一开始都是从程序员做起的，为什么有的人可以成为工程师，有的人还是程序员呢？这就在于个人平时的积累了。 玩 最近一年面试了二十多个7、8年工作经验的人，很多人连最基本的技术知识和数据结构都不知道，记忆最深的就是有一个跟着7年的人跟我说：我会写代码，我能实现业务需求，不就行了么，我要知道那些原理干啥？ 是不是又很多人也这么想的？这么想就对啦，等着30岁就被淘汰吧，35都不用等啦！ 知道的越多，不知道的就越多","categories":[{"name":"杂谈","slug":"杂谈","permalink":"http://luxiaowan.github.io/categories/杂谈/"}],"tags":[]},{"title":"Docker安装phpmyadmin","slug":"Docker安装phpmyadmin","date":"2019-12-20T09:00:00.000Z","updated":"2019-12-20T15:07:26.882Z","comments":true,"path":"2019/12/20/Docker安装phpmyadmin/","link":"","permalink":"http://luxiaowan.github.io/2019/12/20/Docker安装phpmyadmin/","excerpt":"","text":"我们在云服务器上的MySQL数据库在暴露端口和开启远程连接的情况下，是非常不安全的，昨天我的一台服务器就遭到了勒索病毒的攻击，让我支付0.06比特币赎回我的数据。还好那个库中没有重要数据，只是一个弃用了半年的数据库，密码我自己都忘记了，还好通过手段找回了密码，登上去之后就傻眼了。那么如果我们不想开启远程登录还想操作数据库，怎么办？怎么办？用phpmyadmin就可以解决了，不过phpmyadmin也有一定的风险，下面看下怎么玩。 使用Docker发布phpmyadmin并且连接已经存在的MySQL容器 首先下载phpmyadmin的docker镜像 1234567# 先查询镜像仓库里有哪些镜像docker search phpmyadmin# 拉取star最多的镜像or拉取你想用的镜像docker pull docker.io/phpmyadmin/phpmyadmin拉取镜像需要一段时间，这个要看服务器的带宽网速了 启动镜像，连接到已存在的MySQL容器 1234567891011121314# 启动镜像docker run --name myadmin -p 80:80 -d --link mysql-db:db docker.io/phpmyadmin/phpmyadmin# 修改容器配置文件## 将配置文件复制到宿主机中docker cp myadmin:/etc/phpmyadmin/config.inc.php .## 修改配置文件信息(这里修改的db就是在启动的时候--link后面指定的别名)$cfg['Servers'][$i]['host'] = 'localhost' ——&gt; $cfg['Servers'][$i]['host'] = 'db'## 将修改后的配置文件复制回容器中docker cp ./config.inc.php myadmin:/etc/phpmyadmin/# 重启phpmyadmin容器 此处就可以连接了，当然你也可以修改配置文件限制连接的用户，然后在MySQL中给连接用户授权 使用docker-compose创建 安装docker-compose，这里就不赘述了，回头专门用篇文章来解释 编写docker-compose.yml文件 1234567891011121314151617181920212223242526272829version: \"2\"services: mysql: image: hub.c.163.com/library/mysql container_name: test-mysql restart: always ports: - \"3306:3306\" environment: MYSQL_USER: \"root\" MYSQL_PASSWORD: \"root\" MYSQL_ROOT_PASSWORD: \"root\" networks: - net-mysql phpmyadmin: image: docker.io/phpmyadmin/phpmyadmin container_name: test-myadmin ports: - \"80:80\" environment: MYSQL_USER: \"root\" MYSQL_PASSWORD: \"root\" MYSQL_ROOT_PASSWORD: \"root\" networks: - net-mysqlnetworks: net-mysql: 发布容器 12# 使用命令发布容器docker-compose up -d 然后就可以使用了，不需要修改任何配置文件","categories":[{"name":"Docker","slug":"Docker","permalink":"http://luxiaowan.github.io/categories/Docker/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/tags/MySQL/"},{"name":"Docker","slug":"Docker","permalink":"http://luxiaowan.github.io/tags/Docker/"}]},{"title":"Nacos基本使用","slug":"Nacos基本使用","date":"2019-12-20T01:30:00.000Z","updated":"2019-12-20T15:07:26.882Z","comments":true,"path":"2019/12/20/Nacos基本使用/","link":"","permalink":"http://luxiaowan.github.io/2019/12/20/Nacos基本使用/","excerpt":"","text":"啥 Nacos是什么 Nacos解决什么问题 Nacos怎么使用 Nacos架构图 Nacos作为配置中心 Nacos作为服务注册中心 以上问题，在Nacos官网都有详细的说明，要学会查看官方文档，其他地方写的基本也都是copy的官方文档，所以我这里就不再赘述了，直接上官网地址给你们：https://nacos.io/zh-cn/docs/what-is-nacos.html 在使用过程中如果遇到什么问题，可以去提issue：https://github.com/alibaba/nacos/issues","categories":[{"name":"Nacos","slug":"Nacos","permalink":"http://luxiaowan.github.io/categories/Nacos/"}],"tags":[]},{"title":"忘记MySQL的root用户密码怎么办","slug":"忘记MySQL的root用户密码怎么办","date":"2019-12-19T16:53:00.000Z","updated":"2019-12-20T15:07:26.883Z","comments":true,"path":"2019/12/20/忘记MySQL的root用户密码怎么办/","link":"","permalink":"http://luxiaowan.github.io/2019/12/20/忘记MySQL的root用户密码怎么办/","excerpt":"","text":"在工作中，如果我们忘记了数据库的密码，那么我们该怎么办？其实方法很多，下面我们主要说一下如何修改宿主机上的MySQL以及Docker容器中的MySQL。 1. Docker容器中的MySQL 启动MySQL的容器 通过docker命令进入到容器中docker exec -it container_id /bin/bash 找到docker.cnf配置文件，大概在/etc/mysql/conf.d/目录下 打开docker.cnf文件，在最后一行后添加skip-grant-tables跳过用户权限验证 退出docker容器，然后重启容器docker restart container_id 再次进入到容器中，执行如下命令： 12345678910111. 进入mysql控制台 mysql2. 进入mysql数据库 use mysql;3. 修改root用户密码 MySQL5.7+: update user set authentication_string=password(&apos;newpwd&apos;) where user=&apos;root&apos;; MySQL5.6-: update user set password=password(&apos;newpwd&apos;) where user=&apos;root&apos;;4. 刷新权限 flush privileges; 将docker.cnf中添加的那一行skip-grant-tables删除，退出docker容器，然后重启容器，搞定 2. 宿主机中的MySQL 找到my.ini配置文件，并在[mysqld]组下加入skip-grant-tables跳过用户权限验证 修改密码，同上步骤6 将my.ini文件中添加的那一行删除，重启MySQL服务service mysqld restart，搞定","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/categories/MySQL/"}],"tags":[]},{"title":"Nacos编译报错Use STAR or POSIX Extensions to Overcome This Limit","slug":"Nacos编译报错Use-STAR-or-POSIX-extensions-to-overcome-this-limit","date":"2019-12-19T04:23:00.000Z","updated":"2019-12-19T16:51:55.995Z","comments":true,"path":"2019/12/19/Nacos编译报错Use-STAR-or-POSIX-extensions-to-overcome-this-limit/","link":"","permalink":"http://luxiaowan.github.io/2019/12/19/Nacos编译报错Use-STAR-or-POSIX-extensions-to-overcome-this-limit/","excerpt":"","text":"Nacas安装 可以通过下载源码进行编译和下载发行包两种方式来启动Nacos 源码 1234567git clone https://github.com/alibaba/nacos.gitcd nacos/mvn -Prelease-nacos clean install -U ls -al distribution/target/// change the $version to your actual pathcd distribution/target/nacos-server-$version/nacos/bin 发行包 您可以从 最新稳定版本 下载 nacos-server-$version.zip 包。 12unzip nacos-server-$version.zip 或者 tar -xvf nacos-server-$version.tar.gzcd nacos/bin 如何启动在这里就不做说明了，我们重点说一下通过源码安装时出现的一个编译错误 错误信息 Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:3.0.0:single (make-assembly) on project nacos-distribution: Execution make-assembly of goal org.apache.maven.plugins:maven-assembly-plugin:3.0.0:single failed: group id ‘1326029969’ is too big ( &gt; 2097151 ). Use STAR or POSIX extensions to overcome this limit -&gt; [Help 1] 这个错误主要出现在编译distribution这个目录时出现的，执行命令是mvn -Prelease-nacos clean install -U，这个错误导致nacos-server-1.2.0-SNAPSHOT生成失败 解决办法 在distribution目录下的pom.xml文件中找到id为release-nacos的profile，在plugin标签的configuration内加上&lt;tarLongFileMode&gt;posix&lt;/tarLongFileMode&gt;即可解决","categories":[{"name":"Nacos","slug":"Nacos","permalink":"http://luxiaowan.github.io/categories/Nacos/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/tags/Java/"},{"name":"Maven","slug":"Maven","permalink":"http://luxiaowan.github.io/tags/Maven/"},{"name":"基础应用","slug":"基础应用","permalink":"http://luxiaowan.github.io/tags/基础应用/"}]},{"title":"阿里面试题","slug":"阿里面试题","date":"2019-12-08T15:55:00.000Z","updated":"2019-12-19T17:07:06.991Z","comments":true,"path":"2019/12/08/阿里面试题/","link":"","permalink":"http://luxiaowan.github.io/2019/12/08/阿里面试题/","excerpt":"","text":"阿里巴巴一面 先介绍一下自己吧 说一下自己的优缺点 具体讲一下之前做过的项目 你觉得项目里给里最大的挑战是什么？ HashMap 了解么吗？说一下 put 方法过程 HashMap是键值对存储数据结构，内部以key-value+链表的形式存储数据，jdk1.8之前，是以Entry创建数据节点，jdk1.8之后以Node创建数据节点。 put方法中要先检查存储数据的数组是否已初始化，计算key的哈希值，若该哈希值所在槽无对象，则直接设置进去；若该槽有值，则校验key值是否相等，若相等则直接替换，并将旧值返回；若key不相等，则校验节点的类型是否为TreeNode，若为TreeNode则走TreeNode的putTreeVal；若为普通Node节点，则迭代该槽上的Node节点，通过当前节点的next属性获取下一节点，若next的key与put的key相等(== &amp;&amp; equals)，则替换next的value；若next为null，则在此处存储，并且当前节点的next指向新创建的Node对象；最终返回原值或null HashMap 是不是线程安全？ 不是线程安全的，在高并发情况下会出现脏数据的情况 如果想使用线程安全的Map，则可以使用Collections类中的synchronizedMap方法去创建一个Collections内部类SynchronizedMap的实例，该内部类中的方法是使用synchronized关键字实现的线程并发锁；Hashtable类同；或者如果项目使用的是jdk1.5及以上版本的话，可以使用ConcurrentHashMap类，该类是使用了分段锁和CAS的方式实现的并发控制，比synchronized更加灵活高效，分段锁可以实现在写的同时放任其他线程读取非本段的数据，CAS方式则仅仅锁住被操作的节点。 HashMap 为什么不用平衡树？ 平衡树在极端情况下会出现非常高的树，在查找的过程中会变慢，而红黑树在插入数据的时候会通过自旋转缩短树的高度 AQS 知道吗？知道哪一些呢？讲一讲。 AQS是AbstractQueuedSynchronizer类的缩写，JUC包中的锁基本实现于AQS，可以实现共享锁和排它锁，AQS是读写锁ReentrantLock的基类，内部通过acquire和acquireShared方法分别实现排它锁和共享锁。 CLH 同步队列是怎么实现非公平和公平的？ CLH队列是通过链表的形式将每一个节点的连接到一起，公平是指在头结点被处理的时候，其他节点都处于wait状态，当处理完之后，将后续所有的节点全部唤醒，所有队列是公平的；即使队列中所有的节点都被唤醒，也不会出现谁先竞争到资源谁执行的情况，永远都是先执行离对列头最近的无中断标志的节点，所以CLH队列又是非公平的。 ReentrantLock 和 synchronized 的区别 两者都是本地锁，都是为了防止并发情况下出现数据错乱的情况，都是可重入锁。ReentrantLock有共享锁和排它锁两种锁机制，而synchronized只能是排它锁，synchronized在jdk1.5之后自带锁升级机制，包括了偏向锁、轻量级锁和重量级锁，但ReentrantLock只是重量级锁。 讲一下 JVM 的内存结构 JVM内存结构分为线程共享内存区和线程私有内存区，线程共享内存区包括方法区/元空间、堆，线程私有内存区包括程序计数器、Java栈、本地方法栈，元空间是在jdk1.8之后用来代替方法区的，可以通过-XX:+MetaspaceSize=10g -XX:+MaxMetaspaceSize=20g来指定元空间的大小，方法区/元空间用于存放类的定义信息、常量等信息，堆用于存放new出来的对象，Java栈和本地方法栈为线程私有主要存储线程中的对象引用和方法调用信息等，程序计数器则为了记录所在线程当前执行的指令位置。 JVM 里 new 对象时，堆会发生抢占吗？你是怎么去设计 JVM 的堆的线程安全的？ 不会发生抢占，JVM可以将new出来的对象存在堆上也可以存入线程栈中，也就是通过逃逸分析决定对象是分配到线程共享的堆上还是栈上分配，可以通过-XX:+DoEscapeAnalysis -XX:+EliminateLocks来开启逃逸分析，但是仅在-server模式下有效，可以通过java -version来查看若jre是server版本，则默认就是server模式。JVM堆的线程安全通过使用volatile关键字、ThreadLocal类或者加锁来实现。 讲一下 Redis 的数据结构 Redis内部是以key-value的形式存储数据的，每一个key-value都会以redisObject结构体，结构体中包括数据类型、编码方式、过期策略、引用数量、值结构体实例，值的结构体中包括的key的值， Redis 缓存同步问题 讲一讲 MySQL 的索引结构 MySQL的索引有B+树和Hash两种结构，在InnoDB存储引擎中，默认支持B+树结构，不支持Hash结构，但是我们可以使用Hash结构，InnoDB通过B+树实现自适应Hash来满足我们使用Hash结构的索引。MySQL之所以选择B+树作为存储结构，是因为其比AVL树高度更可控，比B-树在查询效率上更快。 讲一下 Redis 分布式锁的实现 Redis通过使用setnx+expire或者set key value nx ex time来设置分布式锁，也可以使用lua脚本创建分布式锁。三种方式中，setnx+expire可能会发生异常情况导致锁的key设置过期时间失败，最终锁无法自动释放而影响具体业务处理；分布式锁之所以设置过期时间，是为了防止在创建了锁之后未释放锁而产生的永久锁，这种情况将会导致其他线程永久性的加锁失败。 实际生产环境中，我们一般都是使用Redis集群为应用提供缓存服务，如果在多写的情况下，Master尚未同步到全部Slave之前，会出现同时有多个线程向不同的写服务器发起加锁请求，为了预防这种情况，在加锁的时候，可以向所有的节点同时发起写请求，这样保证了数据的强一致性。 ConcurrentHashMap 如何保证线程安全？ ConcurrentHashMap在jdk1.8之前使用Segment Lock(分段锁)的方式对数据段进行加锁，在该数据段加锁的时候，不会影响其他数据段的数据读写，从而达到提高并发的效率；在jdk1.8之后，使用CAS对具体的某个Node进行加锁，此方法仅仅对线程正在操作的那条数据加锁，不会影响到Map中其他数据的读写。 数据库索引了解吗？讲一下 MySQL索引有主键索引、普通索引两种，又可细分为单索引和复合索引，InnoDB中的索引存储结构默认为B+树，MyIsam中主键索引存储的是数据在磁盘上的位置，InnoDB中主键索引存储的是实际数据，普通索引中存储的是主键数据，所以在使用普通索引查找数据时，是先通过二分法查询到主键值，然后再到主键索引树中根据主键值查询出具体的数据，若是复合索引且所查询的字段均在复合索引中，此类索引查询称为覆盖索引，也就是不需要通过回表查询主键索引树即可返回。 常见排序算法 TCP 三次握手，四次挥手。 TCP在通信时需要先建立连接，为了保证数据发送的稳定性及可靠性，需要进行预通信，也就是我们平时与人见面时的握手礼仪，第一次握手是client端向server端发送一个通知(seq=x,SYN=1)，告诉server端我要向你发送数据了，是否做好了接收准备，然后client端状态变为SYN_SENT，server端若已经做好准备，则向client端回应一条消息告诉它我已经做好了准备，可以将数据发送过来了，此为第二次握手(seq=y,SYN=1,ACK=1,ack=x+1)，server端状态变为SYN_RECV，client端接收到server端回应的消息后，向server端发送确认包(seq=x+1,ACK=1,ack=y+1)，然后client和server端同时变更状态为ESTABLISHED，到此完成三次握手。 在通信结束后需要关闭连接，client端向server端发送断开请求(第一次挥手)，server端接收到请求后回应client端，然后server端进入到等待关闭状态(第二次挥手)，client接收到回应后向server端发送确认断开请求(第三次挥手)，server端收到确认消息后回应client进行断开(第四次挥手)，若最后client迟迟未接收到server的确认断开回应，则会重试一次，重试仍然未收到回应则自动断开。 深入问了乐观锁，悲观锁及其实现 悲观锁和乐观锁在读写效率上有很大的区别，悲观锁是在操作数据时，被操作的数据不可被其他线程/连接访问，除非等当前操作的事务提交或释放锁，在MySQL中，可以通过select for update来实现悲观锁；乐观锁在效率上比悲观锁高很多，可以通过版本进行数据控制，比如MySQL中的MVCC。 阿里巴巴二面 自我介绍 + 项目介绍。 你在项目中担任什么样的角色？ 那你觉得你比别人的优势在哪里？你用了哪些别人没有的东西吗？ 说一下 HashMap 的数据结构 红黑树和 AVL 树有什么区别？ 树高度 如何才能得到一个线程安全的 HashMap？ 讲一下 JVM 常用垃圾回收器 JVM常用垃圾收集器有CMS、G1， Redis 分布式锁 再描述一下你之前的项目吧 你觉得这个项目的亮点在哪里呢？ 你设计的数据库遵循的范式？ Java 怎么加载类？ linux 常用命令有哪些？ Spring 的 IOC, AOP。 讲一下 ORM 框架 Hibernate 设计模式了解吗？讲一下 自己实现一个二阶段提交，如何设计？ 阿里巴巴三面 在项目中，并发量大的情况下，如何才能够保证数据的一致性？ ElasticSearch 为什么检索快，它的底层数据结构是怎么样的？ JVM 内存模型 Netty 应用在哪些中间件和框架中呢？ 线程池的参数 讲一下 B 树和 B+ 树的区别 为什么要用 Redis 做缓存？ 了解 SpringBoot 吗？那讲一下 SpringBoot 的启动流程吧 如何解决 bean 的循环依赖问题？ Java 有哪些队列？ 讲一讲 Spring 和 Springboot 的区别 最近看了什么书？为什么？ 你平时是怎么学习 Java 的呢？ wait() 和 sleep() 的区别 原子变量的实现原理 CAS 的问题，讲一下解决方案。 有没有更好的计数器解决策略 讲一讲 NIO 和 BIO 的区别 Nginx 负载均衡时是如何判断某个节点挂掉了？ 讲一下 Redis 的数据类型和使用场景 k8s 的储存方式是怎样的？ Spring AOP 原理是什么？怎么使用？什么是切点，什么是切面？最好是举个例子 算法题：给一堆硬币的 array，返回所有的组合 阿里巴巴总监面 算法：给一个 set 打印出所有子集；多线程从多个文件中读入数据，写到同一个文件中； 判断 ip 是否在给定范围内；打乱一副扑克牌，不能用额外空间，证明为什么是随机的。 TCP 和 UDP 区别 线程池的原理以及各种线程池的应用场景 线程池中使用有限的阻塞队列和无限的阻塞队列的区别 如果你发现你的 SQL 语句始终走另一个索引，但是你希望它走你想要的索引，怎么办？ MySQL 执行计划 数据库索引为什么用 B+ 树？ 你在做 SQL 优化主要从哪几个方面做，用到哪些方法工具？ 有没有想问的？ 阿里巴巴 HR 面 自我介绍 平时怎么学习的？ 有什么兴趣爱好吗？ 怎么看待 996？ 怎么平衡工作和学习？ …… 有没有什么想问的","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://luxiaowan.github.io/tags/面试/"},{"name":"阿里","slug":"阿里","permalink":"http://luxiaowan.github.io/tags/阿里/"}]},{"title":"使用Docker发布项目","slug":"使用Docker发布项目","date":"2019-12-04T03:50:00.000Z","updated":"2020-03-24T05:09:46.712Z","comments":true,"path":"2019/12/04/使用Docker发布项目/","link":"","permalink":"http://luxiaowan.github.io/2019/12/04/使用Docker发布项目/","excerpt":"","text":"1、下载项目 此处使用公开的github上的项目：git clone https://github.com/luxiaowan/simple-eureka-server.git 2、编写Dockerfile #使用自己构建的jdk镜像 &lt; 查看 &gt; 1234567891011FROM docker.kevinlu.cc/env/jdk-8u191:190114MAINTAINER ccADD ./target/simple-eureka-server-1.0.jar /root/startup/ WORKDIR /root/startupEXPOSE 8080CMD [\"java\", \"-jar\", \"simple-eureka-server-1.0.jar\"] 3、构建项目 docker build -t simple-eureka-server:7 . 发现最后报错了，找不到我们的jar包，那是因为我们还没有编译打包我们的项目 所以在执行Dockerfile之前要先对项目进行编译打包 因为项目是Maven管理的，所以我们使用 mvn clean package进行打包(第一次使用Maven会有点慢，因为要下载Maven的基础库) &lt; 安装Maven &gt; 打包完成之后再进行构建，发现成功了，然后使用docker images查看刚构建的镜像 4、运行镜像 docker run -d -P 8080:8080 --name eureka-server simple-eureka-server:7 5、查看 http://IP:8080","categories":[{"name":"Docker","slug":"Docker","permalink":"http://luxiaowan.github.io/categories/Docker/"}],"tags":[]},{"title":"Docker网络模式简讲","slug":"Docker网络模式简讲","date":"2019-12-03T04:59:00.000Z","updated":"2020-03-24T05:10:17.425Z","comments":true,"path":"2019/12/03/Docker网络模式简讲/","link":"","permalink":"http://luxiaowan.github.io/2019/12/03/Docker网络模式简讲/","excerpt":"","text":"docker有四种网络模式：Bridge、Container、Host、None docker默认的网络模式为Bridge，通过在镜像启动时使用–net进行指定，不特殊声明则为默认模式。 docker进程启动时会在宿主机上创建一个名为docker0的虚拟网桥，此主机上启动的所有的docker镜像容器都会链接到这个虚拟网桥上。（虚拟网桥就是为了将镜像容器的虚拟机网络与主机网络建立桥接的一个交换机，与交换机的工作方式类似：：此处不再累赘，这都不懂就自刎了吧） 四大家族：Bridge、Container、Host、None 1、Bridge模式：默认、使用–net=bridge bridge模式是将每个docker镜像容器隔离开运行，分别映射到宿主机的端口上，每个容器都会被分配一个独立的网络命名空间，并且会把网络命名空间的IP映射到宿主机的docker0上。 a.启动命令： docker run --net=bridge --name xiaolu_one -dt xiaolu b.原理图： c.宿主服务器： d. docker容器： 2、host模式：–net=host host模式是与宿主机共用同一个网络空间，容器不会虚拟出自己的网卡、配置自己的IP等，也不会映射到docker0虚拟网桥，容器的IP、端口都与宿主机共用，极其容易发生冲突，所以不推荐使用此模式。虽然网络空间是与宿主机共用，但是文件系统、进程列表与宿主机是隔离的。 a.启动命令： docker run --net=host --name xiaolu_two -dt xiaolu b.原理图： c. 宿主服务器： d. docker容器： Tip：可以看出宿主机和docker容器查看到的内容一样 3、Container模式：–net=container:docker_container_name Container模式是将新创建的容器指定与一个已经存在的容器共用一个网络空间，新创建的容器不会创建自己的网卡、配置自己的IP，容器内的端口和IP都与指定容器使用同一个，但是新建容器的文件系统、进程列表是与宿主容器相隔离。 a.启动命令 123docker run --name xiaolu_three -dt xiaoludocker run --net=container:xiaolu_three --name xiaolu_four -dt xiaolu b.原理图 c.宿主服务器： d.Container容器： e. docker容器： Tip：可以看到docker容器看到的网络信息和Container容器的一样 4、None模式：–net=none None模式表示docker容器拥有自己的网络空间，但是并不为docker容器进行任何网络配置，也就是说这个docker容器没有网卡、IP、路由等信息，需要手动为docker容器配置。 a.启动命令： docker run --net=none --name xiaolu_five -dt xiaolu b.原理图","categories":[{"name":"Docker","slug":"Docker","permalink":"http://luxiaowan.github.io/categories/Docker/"}],"tags":[]},{"title":"使用Docker构建自己的JDK容器","slug":"使用Docker构建自己的JDK容器","date":"2019-12-02T16:10:00.000Z","updated":"2020-03-24T05:10:49.862Z","comments":true,"path":"2019/12/03/使用Docker构建自己的JDK容器/","link":"","permalink":"http://luxiaowan.github.io/2019/12/03/使用Docker构建自己的JDK容器/","excerpt":"","text":"★为了在Dockerfile中使用FROM自己的jdk，在此构建一个独立专属的 1、下载需要构建的jdk压缩包，使用压缩包即可，此处使用了jdk_8u191&lt; 下载 &gt;，根据自己需要，别乱下 2、因为jdk属于是系统环境配置，所以此处需要借助系统镜像去构建，此处借助centos:7来构建，查找可用的centos镜像： 2.1、docker search centos 第一个是官方的镜像，直接使用这个即可：docker pull docker.io/centos:7 (这个命令就不解释了，看不懂的此文也可用就此打住了) 2.2、下载完成之后使用命令查看当前存在的镜像：docker images -a 3、编写构建jdk镜像的Dockerfile，内容如下： 1234567891011FROM docker.io/centos:7MAINTAINER ccADD jdk-8u191-linux-x64.tar.gz /opt/localhost/ ------这个路径即为jdk的安装路径ENV JAVA_HOME /opt/localhost/jdk1.8.0_191 ------设置环境变量ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarENV PATH $PATH:$JAVA_HOME/bin 4、构建jdk的docker镜像 docker build -t docker.kevinlu.cc/env/jdk-8u191:190114 . -f Dockerfile 这个命令就没啥好讲的了，不认识的可以看&lt; 这里 &gt; 构建完成之后通过docker images命令查看镜像信息，构建的时候就会将构建信息打印到控制台： 可以看到最后的Successfully built 47a3b1aa0e55，这个47a3b1aa0e55就是镜像的IMAGE ID，同一个镜像多次构建，生成的IMAGE ID相同 5、运行jdk镜像 运行jdk镜像与其他普通镜像有些许不同，因为jdk属于是系统环境配置，所以运行命令为： docker run -d --name jdk8u191 -it 47a3b1aa0e55 /bin/bash ★创建容器的时候一定要使用 -it /bin/bash，不然jdk的容器起不来。 6、验证 docker ps查看当前运行的容器 docker exec -it jdk8u191 /bin/bash进入jdk容器内 java -version查看当前环境中jdk版本 大功告成！ 7、使用 &lt; 查看 &gt;","categories":[{"name":"Docker","slug":"Docker","permalink":"http://luxiaowan.github.io/categories/Docker/"}],"tags":[]},{"title":"Git常用命令","slug":"Git常用命令","date":"2019-12-02T09:23:00.000Z","updated":"2019-12-03T16:12:26.788Z","comments":true,"path":"2019/12/02/Git常用命令/","link":"","permalink":"http://luxiaowan.github.io/2019/12/02/Git常用命令/","excerpt":"","text":"","categories":[{"name":"Git","slug":"Git","permalink":"http://luxiaowan.github.io/categories/Git/"}],"tags":[]},{"title":"Docker基本命令使用","slug":"Docker基本命令使用","date":"2019-12-01T04:20:00.000Z","updated":"2020-03-24T05:11:15.111Z","comments":true,"path":"2019/12/01/Docker基本命令使用/","link":"","permalink":"http://luxiaowan.github.io/2019/12/01/Docker基本命令使用/","excerpt":"","text":"1.查看镜像 ​ docker images 2.查看启动中的容器 ​ docker ps 3.删除容器 ​ docker rm containerid 4.删除镜像 ​ docker rmi imageid 5.启动容器 ​ docker run -p 8080:8080 --name tomcat_one -dt xxx.xxx.xxx/tomcat 6.停止容器 ​ docker stop containerid ​ docker kill containerid 7.进入容器 ​ docker exec -it containerid bash 8.制作镜像(Dcokerfile所在目录) ​ docker build . -t image_name:latest 9.查看容器启动日志 ​ docker logs containerid","categories":[{"name":"Docker","slug":"Docker","permalink":"http://luxiaowan.github.io/categories/Docker/"}],"tags":[]},{"title":"Dockerfile详解","slug":"Dockerfile详解","date":"2019-11-30T16:20:00.000Z","updated":"2020-03-24T05:11:17.734Z","comments":true,"path":"2019/12/01/Dockerfile详解/","link":"","permalink":"http://luxiaowan.github.io/2019/12/01/Dockerfile详解/","excerpt":"","text":"Dockerfile分为四部分：基础镜像信息、维护者信息、镜像操作指令、容器启动时执行指令。 基础镜像信息：FROM ubutu 格式为FROM或FROM: ，Dockerfile第一条必须为FROM指令，如果一个Dockerfile要指定多个镜像，则每个镜像使用一个FROM指令 2.维护者信息：MAINTAINER xiaolu xiaolu@qq.com 格式为MAINTAINER，指定维护者信息，可省略 3.镜像操作指令：RUN apt-get update &amp;&amp; apt-get install -y vim 格式为RUN或RUN [&quot;executable&quot;, &quot;Param1&quot;, &quot;Param2&quot;] 格式RUN：在shell终端运行，即/bin/sh -C 格式RUN [“executable”, “Param1”, “Param2”]：使用exec执行 每条run指令在当前基础镜像执行，并且重新提交新镜像，当命令比较长时可用&quot;/&quot;换行 4.容器启动时执行指令：CMD /usr/sbin/nginx 每个容器只能执行一条CMD命令，多个CMD命令时，只会执行最后一条。 支持三种格式： 12345CMD [\"executable\", \"Param1\", \"Param2\"]：使用exec执行，推荐CMD command param1 param2：使用/bin/sh上执行CMD [\"Param1\", \"Param2\"]：提供给ENTRYPOINT做默认参数 5.暴露端口指令：EXPOSE，例如：EXPOSE 80 22 8080 格式为：EXPOSE port1 port2 port3 告诉Docker服务端容器暴露的端口号，供互联系统使用。 在启动Docker时，主机会自动分配一个端口号转发到指定的端口，可用通过-P/-p，指定主机具体端口号进行映射。 6.设置环境变量ENV 格式为ENV，指定一个环境变量，会被后续的RUN指令使用，并且会在容器运行过程中保持。 1234567ENV YC_NAME yc_frameworkENV YC_VERSION 1.0RUN wget -C http://www.kevinlu.cc/$YC_NAME/$YC_VERSION | mvn install /usr/src/$YC_NAME-$YC_VERSIONENV PATH /usr/src/$YC_NAME-$YC_VERSION/bin:$PATH 7.复制指定的文件到容器中ADD ADD hom* /usr/file 若/usr/file目录不存在，则自动创建 源目录可以是Dockerfile所在目录的一个相对路径；也可以是一个URL；也可以是一个tar文件（自动解压为目录） 8.复制本地主机的文件到容器COPY 格式：COPY file directory 拷贝的文件为Dockerfile所在目录的相对路径 9.容器启动后执行指令：ENTRYPOINT 格式：ENTRYPOINT [&quot;executable&quot;, &quot;Param1&quot;, &quot;Param2&quot;] ​ ENTRYPOINT command param1 param2 配置容器启动后需要执行的指令，并且不会被docker run提供的参数覆盖 每个Dockerfile中只能有一个ENTRYPOINT，如果有多个，则只会执行最后一个 10.指定工作目录WORKDIR 格式：WORKDIR /a/b/c 为后续的RUN、CMD、ENTRYPOINT指令配置工作目录 可以使用多个WORKDIR指令，后续的命令的参数为相对路径时，会基于之前命令指定的目录 例如： 123456789WORKDIR /aWORKDIR bWORKDIR cWORKDIR dRUN pwd&lt;打印出/a/b/c/d&gt;","categories":[{"name":"Docker","slug":"Docker","permalink":"http://luxiaowan.github.io/categories/Docker/"}],"tags":[]},{"title":"JVM之OOM","slug":"Java之OOM","date":"2019-11-28T14:23:00.000Z","updated":"2019-12-03T16:04:43.133Z","comments":true,"path":"2019/11/28/Java之OOM/","link":"","permalink":"http://luxiaowan.github.io/2019/11/28/Java之OOM/","excerpt":"","text":"java.lang.StackOverflowError 栈溢出错误，这个错误很容易模拟，且看下面的代码： 1234567891011121314151617public static void main(String[] args) &#123; new StackOverflowTest().test();&#125;private static int high = 0;private void test() &#123; try &#123; ++high; test(); &#125; finally &#123; System.out.println(\"栈的深度为: \" + high); &#125;&#125;---JVM ARGS: -server -Xmn2m -Xss1m -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+DoEscapeAnalysis -XX:+EliminateAllocations -XX:-UseTLAB 释 我们都知道方法的调用是通过入栈和计算出栈来实现的，所以我们在方法递归调用一定次数时，必然会发生栈溢出，栈溢出后，程序自动停止，是一类不可捕获和恢复的Error类型的错误，所以我们在使用递归算法时，应当注意递归的深度，防止出现栈溢出错误导致服务错误 java.lang.OutOfMemoryError:java heap space JVM堆空间不足引起的内存溢出错误，这类错误比较常见，此处就不做太多的解释，出现这类错误，你就去看GC日志，看看新生代、老年代、永久代/Metaspace的使用情况，如果是想查看GC的情况，使用如下JVM指令： -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:./gclog.log，gc的所有信息都会输出到gclog.log文件中 gclog.log 123456789101112131415161718192021222324252627*JVM信息*Java HotSpot(TM) 64-Bit Server VM (25.161-b12) for bsd-amd64 JRE (1.8.0_161-b12), built on Dec 19 2017 16:22:20 by \"java_re\" with gcc 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2336.11.00)Memory: 4k page, physical 16777216k(2991720k free)/proc/meminfo:*JVM ARGS*CommandLine flags: -XX:+DoEscapeAnalysis -XX:+EliminateAllocations -XX:InitialHeapSize=268435456 -XX:MaxHeapSize=4294967296 -XX:MaxNewSize=2097152 -XX:NewSize=2097152 -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:ThreadStackSize=1024 -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGC -XX:-UseTLAB *GC日志信息*0.125: [GC (Allocation Failure) [PSYoungGen: 1023K-&gt;512K(1536K)] 1023K-&gt;536K(261632K), 0.0010704 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] 0.157: [GC (Allocation Failure) [PSYoungGen: 1535K-&gt;493K(1536K)] 1559K-&gt;847K(261632K), 0.0010655 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] ...0.360: [GC (Allocation Failure) [PSYoungGen: 1247K-&gt;256K(1536K)] 2614K-&gt;1727K(261632K), 0.0008285 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] Heap *年轻代* PSYoungGen total 1536K, used 396K [0x00000007bfe00000, 0x00000007c0000000, 0x00000007c0000000) eden space 1024K, 13% used [0x00000007bfe00000,0x00000007bfe23268,0x00000007bff00000) from space 512K, 50% used [0x00000007bff00000,0x00000007bff40000,0x00000007bff80000) to space 512K, 0% used [0x00000007bff80000,0x00000007bff80000,0x00000007c0000000) *老年代* ParOldGen total 260096K, used 1471K [0x00000006c0000000, 0x00000006cfe00000, 0x00000007bfe00000) object space 260096K, 0% used [0x00000006c0000000,0x00000006c016fc00,0x00000006cfe00000) *Metaspace空间，jdk8+* Metaspace used 3402K, capacity 4500K, committed 4864K, reserved 1056768K class space used 368K, capacity 388K, committed 512K, reserved 1048576K 代码 12345678910111213public static void main(String[] args) &#123; new StackOverflowTest().heapSpace();&#125;private void heapSpace() &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); while (true) &#123; list.add(new String(\"abc\")); &#125;&#125;--- JVM ARGS: -server -Xmn2m -Xss1m -Xms1m -Xmx1m -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:-DoEscapeAnalysis -XX:-EliminateAllocations -XX:-UseTLAB java.lang.OutOfMemoryError:GC overhead limit exceeded 超出了GC开销限制引起的内存溢出，这个错误不是特别常见，Sun 官方对此的定义：超过98%的时间用来做GC并且回收了不到2%的堆内存时会抛出此异常，可以使用参数-XX:-UseGCOverheadLimit 禁用这个检查，但是这个参数解决不了内存问题，只是把错误的信息延后，替换成 java.lang.OutOfMemoryError: Java heap space java.lang.OutOfMemoryError:Metaspace Metaspace内存溢出，Metaspace是jdk8+特有的东西，用来代替之前的PermGen，主要存储class名称、字段、方法、字节码、常量池、JIT优化代码等等，我们可以使用-XX:MetaspaceSize和-XX:MaxMetaspaceSize来指定其大小，一般情况下Metaspace不会发生OOM，Metaspace的使用量与JVM加载的class数量有很大关系： 代码 12345678910111213141516static ClassPool cp = ClassPool.getDefault();public static void main(String[] args) throws CannotCompileException &#123; int i = 0; try &#123; for (;; i++) &#123; Class cz = cp.makeClass(\"com.example.demo.bean.DemoBean\" + i).toClass(); &#125; &#125; catch (Exception e) &#123; &#125; finally &#123; System.out.println(i); &#125;&#125;--- JVM ARGS: -XX:MetaspaceSize=10m -XX:MaxMetaspaceSize=10m -XX:+PrintGCDetails -XX:+PrintGCTimeStamps 输出 1234567891011121314151617181920210.598: [GC (Metadata GC Threshold) [PSYoungGen: 39345K-&gt;10741K(76288K)] 39345K-&gt;15811K(251392K), 0.0111319 secs] [Times: user=0.05 sys=0.01, real=0.01 secs] 0.609: [Full GC (Metadata GC Threshold) [PSYoungGen: 10741K-&gt;0K(76288K)] [ParOldGen: 5069K-&gt;15550K(139776K)] 15811K-&gt;15550K(216064K), [Metaspace: 9735K-&gt;9735K(1056768K)], 0.0504762 secs] [Times: user=0.29 sys=0.01, real=0.05 secs] ...0.754: [GC (Last ditch collection) [PSYoungGen: 0K-&gt;0K(82944K)] 15477K-&gt;15477K(472064K), 0.0008113 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] 0.755: [Full GC (Last ditch collection) [PSYoungGen: 0K-&gt;0K(82944K)] [ParOldGen: 15477K-&gt;15477K(607232K)] 15477K-&gt;15477K(690176K), [Metaspace: 9733K-&gt;9733K(1056768K)], 0.0204189 secs] [Times: user=0.08 sys=0.00, real=0.02 secs] 5341Exception in thread \"main\" java.lang.OutOfMemoryError: Metaspace at javassist.ClassPool.toClass(ClassPool.java:1170) at javassist.ClassPool.toClass(ClassPool.java:1113) at javassist.ClassPool.toClass(ClassPool.java:1071) at javassist.CtClass.toClass(CtClass.java:1275) at com.example.demo.jvm.MetaspceOOMTest.main(MetaspceOOMTest.java:13)Heap PSYoungGen total 82944K, used 2390K [0x000000076ab00000, 0x0000000772c00000, 0x00000007c0000000) eden space 82432K, 2% used [0x000000076ab00000,0x000000076ad55ab0,0x000000076fb80000) from space 512K, 0% used [0x0000000772b80000,0x0000000772b80000,0x0000000772c00000) to space 10752K, 0% used [0x0000000771700000,0x0000000771700000,0x0000000772180000) ParOldGen total 607232K, used 15477K [0x00000006c0000000, 0x00000006e5100000, 0x000000076ab00000) object space 607232K, 2% used [0x00000006c0000000,0x00000006c0f1d4c8,0x00000006e5100000) Metaspace used 9770K, capacity 10084K, committed 10240K, reserved 1056768K class space used 3165K, capacity 3214K, committed 3328K, reserved 1048576K 我们将Metaspace的初始大小和最大值都设置为10m，最终i的值大概会在5340左右的时候报OOM，从FGC的日志可以看出，Metaspace在整个GC阶段都未进行任务的内存回收，直至被全部用完，具体的关于Metaspace的介绍可以看下PerfMa社区的这篇文章：https://club.perfma.com/article/210111 java.lang.OutOfMemoryError:Direct buffer memory ByteBuffer. allocateDirect (int capability)是分配操作系统的本地内存，不在GC管辖范围之内，由于不需要内存拷贝所以速度相对较快，但如果不断分配本地内存，堆内存就会很少使用，那么JVM就不需要进行GC，那创建的DirectByteBuffer对象就不会被回收，就会出现堆内存充足但本地内存不足的情况，继续尝试分配本地内存就会出现OOM。 代码 1234567public static void main(String[] args) &#123; System.out.println(\"当前direct大小: \" + (VM.maxDirectMemory() / 1024 / 1024) + \" MB\"); ByteBuffer bb = ByteBuffer.allocateDirect(Math.toIntExact(VM.maxDirectMemory() + 10));&#125;--- JVM ARGS: -XX:MaxDirectMemorySize=10m 这里我们需要通过JVM参数-XX:MaxDirectMemorySize=10将JVM本地最大使用内存设置为10MB，不然如果你本地剩余内存很大，那么就很难模拟出此错误 输出 123456当前direct大小: 10 MBException in thread \"main\" java.lang.OutOfMemoryError: Direct buffer memory at java.nio.Bits.reserveMemory(Bits.java:694) at java.nio.DirectByteBuffer.&lt;init&gt;(DirectByteBuffer.java:123) at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311) at com.example.demo.jvm.DirectBufferOOMTest.main(DirectBufferOOMTest.java:11) java.lang.OutOfMemoryError:unable create new native thread 线程创建的太多，导致无法继续创建线程，出现这个问题就要去使用jstack导出线程栈查看具体情况 代码 12345678910111213public static void main(String[] args) &#123; while (true) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(100000); &#125; catch (InterruptedException e) &#123; &#125; &#125; &#125;).start(); &#125;&#125; 这一段代码必然会出现该ERROR，不论你的机器有多牛掰，你会发现出现了OOM之后，进程并未终止，这个时候你可以用jps命令查看进程号，然后使用jstack pid查看线程栈，会发现有非常多的线程处于TIMED_WAITING (sleeping)状态： 12345\"Thread-256\" #267 prio=5 os_prio=31 tid=0x00007fccdd8cc000 nid=0x27d03 waiting on condition [0x0000700019b85000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at com.example.demo.jvm.NativeThreadOOMTest$1.run(NativeThreadOOMTest.java:11) at java.lang.Thread.run(Thread.java:748)","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[]},{"title":"SpringBoot官方配置文档(英文版)","slug":"SpringBoot官方配置文档(英文版)","date":"2019-11-22T17:13:00.000Z","updated":"2019-11-25T17:18:31.849Z","comments":true,"path":"2019/11/23/SpringBoot官方配置文档(英文版)/","link":"","permalink":"http://luxiaowan.github.io/2019/11/23/SpringBoot官方配置文档(英文版)/","excerpt":"","text":"BANNER banner.charset=UTF-8 # Banner file encoding. banner.location=classpath:banner.txt # Banner file location. banner.image.location=classpath:banner.gif # Banner image file location (jpg/png can also be used). banner.image.width= # Width of the banner image in chars (default 76) banner.image.height= # Height of the banner image in chars (default based on image height) banner.image.margin= # Left hand image margin in chars (default 2) banner.image.invert= # If images should be inverted for dark terminal themes (default false) LOGGING logging.config= # Location of the logging configuration file. For instance classpath:logback.xml for Logback logging.exception-conversion-word=%wEx # Conversion word used when logging exceptions. logging.file= # Log file name. For instance myapp.log logging.level.*= # Log levels severity mapping. For instance logging.level.org.springframework=DEBUG logging.path= # Location of the log file. For instance /var/log logging.pattern.console= # Appender pattern for output to the console. Only supported with the default logback setup. logging.pattern.file= # Appender pattern for output to the file. Only supported with the default logback setup. logging.pattern.level= # Appender pattern for log level (default %5p). Only supported with the default logback setup. logging.register-shutdown-hook=false # Register a shutdown hook for the logging system when it is initialized. AOP spring.aop.auto=true # Add @EnableAspectJAutoProxy. spring.aop.proxy-target-class=true # Whether subclass-based (CGLIB) proxies are to be created (true) as opposed to standard Java interface-based proxies (false). IDENTITY (ContextIdApplicationContextInitializer) spring.application.index= # Application index. spring.application.name= # Application name. ADMIN (SpringApplicationAdminJmxAutoConfiguration) spring.application.admin.enabled=false # Enable admin features for the application. spring.application.admin.jmx-name=org.springframework.boot:type=Admin,name=SpringApplication # JMX name of the application admin MBean. AUTO-CONFIGURATION spring.autoconfigure.exclude= # Auto-configuration classes to exclude. SPRING CORE spring.beaninfo.ignore=true # Skip search of BeanInfo classes. SPRING CACHE (CacheProperties) spring.cache.cache-names= # Comma-separated list of cache names to create if supported by the underlying cache manager. spring.cache.caffeine.spec= # The spec to use to create caches. Check CaffeineSpec for more details on the spec format. spring.cache.couchbase.expiration=0 # Entry expiration in milliseconds. By default the entries never expire. spring.cache.ehcache.config= # The location of the configuration file to use to initialize EhCache. spring.cache.infinispan.config= # The location of the configuration file to use to initialize Infinispan. spring.cache.jcache.config= # The location of the configuration file to use to initialize the cache manager. spring.cache.jcache.provider= # Fully qualified name of the CachingProvider implementation to use to retrieve the JSR-107 compliant cache manager. Only needed if more than one JSR-107 implementation is available on the classpath. spring.cache.type= # Cache type, auto-detected according to the environment by default. SPRING CONFIG - using environment property only (ConfigFileApplicationListener) spring.config.location= # Config file locations. spring.config.name=application # Config file name. HAZELCAST (HazelcastProperties) spring.hazelcast.config= # The location of the configuration file to use to initialize Hazelcast. PROJECT INFORMATION (ProjectInfoProperties) spring.info.build.location=classpath:META-INF/build-info.properties # Location of the generated build-info.properties file. spring.info.git.location=classpath:git.properties # Location of the generated git.properties file. JMX spring.jmx.default-domain= # JMX domain name. spring.jmx.enabled=true # Expose management beans to the JMX domain. spring.jmx.server=mbeanServer # MBeanServer bean name. Email (MailProperties) spring.mail.default-encoding=UTF-8 # Default MimeMessage encoding. spring.mail.host= # SMTP server host. For instance smtp.example.com spring.mail.jndi-name= # Session JNDI name. When set, takes precedence to others mail settings. spring.mail.password= # Login password of the SMTP server. spring.mail.port= # SMTP server port. spring.mail.properties.*= # Additional JavaMail session properties. spring.mail.protocol=smtp # Protocol used by the SMTP server. spring.mail.test-connection=false # Test that the mail server is available on startup. spring.mail.username= # Login user of the SMTP server. APPLICATION SETTINGS (SpringApplication) spring.main.banner-mode=console # Mode used to display the banner when the application runs. spring.main.sources= # Sources (class name, package name or XML resource location) to include in the ApplicationContext. spring.main.web-application-type= # Flag to explicitly request a specific type of web application. Auto-detected based on the classpath if not set. FILE ENCODING (FileEncodingApplicationListener) spring.mandatory-file-encoding= # Expected character encoding the application must use. INTERNATIONALIZATION (MessageSourceAutoConfiguration) spring.messages.always-use-message-format=false # Set whether to always apply the MessageFormat rules, parsing even messages without arguments. spring.messages.basename=messages # Comma-separated list of basenames, each following the ResourceBundle convention. spring.messages.cache-seconds=-1 # Loaded resource bundle files cache expiration, in seconds. When set to -1, bundles are cached forever. spring.messages.encoding=UTF-8 # Message bundles encoding. spring.messages.fallback-to-system-locale=true # Set whether to fall back to the system Locale if no files for a specific Locale have been found. OUTPUT spring.output.ansi.enabled=detect # Configure the ANSI output. PID FILE (ApplicationPidFileWriter) spring.pid.fail-on-write-error= # Fail if ApplicationPidFileWriter is used but it cannot write the PID file. spring.pid.file= # Location of the PID file to write (if ApplicationPidFileWriter is used). PROFILES spring.profiles.active= # Comma-separated list (or list if using YAML) of active profiles. spring.profiles.include= # Unconditionally activate the specified comma separated profiles (or list of profiles if using YAML). Reactor spring.reactor.stacktrace-mode.enabled=false # Set whether Reactor should collect stacktrace information at runtime. SENDGRID (SendGridAutoConfiguration) spring.sendgrid.api-key= # SendGrid api key (alternative to username/password) spring.sendgrid.proxy.host= # SendGrid proxy host spring.sendgrid.proxy.port= # SendGrid proxy port EMBEDDED SERVER CONFIGURATION (ServerProperties) server.address= # Network address to which the server should bind to. server.compression.enabled=false # If response compression is enabled. server.compression.excluded-user-agents= # List of user-agents to exclude from compression. server.compression.mime-types= # Comma-separated list of MIME types that should be compressed. For instance text/html,text/css,application/json server.compression.min-response-size= # Minimum response size that is required for compression to be performed. For instance 2048 server.connection-timeout= # Time in milliseconds that connectors will wait for another HTTP request before closing the connection. When not set, the connector’s container-specific default will be used. Use a value of -1 to indicate no (i.e. infinite) timeout. server.display-name=application # Display name of the application. server.max-http-header-size=0 # Maximum size in bytes of the HTTP message header. server.error.include-exception=false # Include the “exception” attribute. server.error.include-stacktrace=never # When to include a “stacktrace” attribute. server.error.path=/error # Path of the error controller. server.error.whitelabel.enabled=true # Enable the default error page displayed in browsers in case of a server error. server.jetty.acceptors= # Number of acceptor threads to use. server.jetty.accesslog.append=false # Append to log. server.jetty.accesslog.date-format=dd/MMM/yyyy:HH:mm:ss Z # Timestamp format of the request log. server.jetty.accesslog.enabled=false # Enable access log. server.jetty.accesslog.extended-format=false # Enable extended NCSA format. server.jetty.accesslog.file-date-format= # Date format to place in log file name. server.jetty.accesslog.filename= # Log filename. If not specified, logs will be redirected to “System.err”. server.jetty.accesslog.locale= # Locale of the request log. server.jetty.accesslog.log-cookies=false # Enable logging of the request cookies. server.jetty.accesslog.log-latency=false # Enable logging of request processing time. server.jetty.accesslog.log-server=false # Enable logging of the request hostname. server.jetty.accesslog.retention-period=31 # Number of days before rotated log files are deleted. server.jetty.accesslog.time-zone=GMT # Timezone of the request log. server.jetty.max-http-post-size=0 # Maximum size in bytes of the HTTP post or put content. server.jetty.selectors= # Number of selector threads to use. server.port=8080 # Server HTTP port. server.server-header= # Value to use for the Server response header (no header is sent if empty) server.use-forward-headers= # If X-Forwarded-* headers should be applied to the HttpRequest. server.servlet.context-parameters.= # Servlet context init parameters server.servlet.context-path= # Context path of the application. server.servlet.jsp.class-name=org.apache.jasper.servlet.JspServlet # The class name of the JSP servlet. server.servlet.jsp.init-parameters.= # Init parameters used to configure the JSP servlet server.servlet.jsp.registered=true # Whether or not the JSP servlet is registered server.servlet.path=/ # Path of the main dispatcher servlet. server.session.cookie.comment= # Comment for the session cookie. server.session.cookie.domain= # Domain for the session cookie. server.session.cookie.http-only= # “HttpOnly” flag for the session cookie. server.session.cookie.max-age= # Maximum age of the session cookie in seconds. server.session.cookie.name= # Session cookie name. server.session.cookie.path= # Path of the session cookie. server.session.cookie.secure= # “Secure” flag for the session cookie. server.session.persistent=false # Persist session data between restarts. server.session.store-dir= # Directory used to store session data. server.session.timeout= # Session timeout in seconds. server.session.tracking-modes= # Session tracking modes (one or more of the following: “cookie”, “url”, “ssl”). server.ssl.ciphers= # Supported SSL ciphers. server.ssl.client-auth= # Whether client authentication is wanted (“want”) or needed (“need”). Requires a trust store. server.ssl.enabled= # Enable SSL support. server.ssl.enabled-protocols= # Enabled SSL protocols. server.ssl.key-alias= # Alias that identifies the key in the key store. server.ssl.key-password= # Password used to access the key in the key store. server.ssl.key-store= # Path to the key store that holds the SSL certificate (typically a jks file). server.ssl.key-store-password= # Password used to access the key store. server.ssl.key-store-provider= # Provider for the key store. server.ssl.key-store-type= # Type of the key store. server.ssl.protocol=TLS # SSL protocol to use. server.ssl.trust-store= # Trust store that holds SSL certificates. server.ssl.trust-store-password= # Password used to access the trust store. server.ssl.trust-store-provider= # Provider for the trust store. server.ssl.trust-store-type= # Type of the trust store. server.tomcat.accept-count= # Maximum queue length for incoming connection requests when all possible request processing threads are in use. server.tomcat.accesslog.buffered=true # Buffer output such that it is only flushed periodically. server.tomcat.accesslog.directory=logs # Directory in which log files are created. Can be relative to the tomcat base dir or absolute. server.tomcat.accesslog.enabled=false # Enable access log. server.tomcat.accesslog.file-date-format=.yyyy-MM-dd # Date format to place in log file name. server.tomcat.accesslog.pattern=common # Format pattern for access logs. server.tomcat.accesslog.prefix=access_log # Log file name prefix. server.tomcat.accesslog.rename-on-rotate=false # Defer inclusion of the date stamp in the file name until rotate time. server.tomcat.accesslog.request-attributes-enabled=false # Set request attributes for IP address, Hostname, protocol and port used for the request. server.tomcat.accesslog.rotate=true # Enable access log rotation. server.tomcat.accesslog.suffix=.log # Log file name suffix. server.tomcat.additional-tld-skip-patterns= # Comma-separated list of additional patterns that match jars to ignore for TLD scanning. server.tomcat.background-processor-delay=30 # Delay in seconds between the invocation of backgroundProcess methods. server.tomcat.basedir= # Tomcat base directory. If not specified a temporary directory will be used. server.tomcat.internal-proxies=10\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}|\\ 192\\.168\\.\\d{1,3}\\.\\d{1,3}|\\ 169\\.254\\.\\d{1,3}\\.\\d{1,3}|\\ 127\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}|\\ 172\\.1[6-9]{1}\\.\\d{1,3}\\.\\d{1,3}|\\ 172\\.2[0-9]{1}\\.\\d{1,3}\\.\\d{1,3}|\\ 172\\.3[0-1]{1}\\.\\d{1,3}\\.\\d{1,3} # regular expression matching trusted IP addresses. server.tomcat.max-connections= # Maximum number of connections that the server will accept and process at any given time. server.tomcat.max-http-header-size=0 # Maximum size in bytes of the HTTP message header. server.tomcat.max-http-post-size=0 # Maximum size in bytes of the HTTP post content. server.tomcat.max-threads=0 # Maximum amount of worker threads. server.tomcat.min-spare-threads=0 # Minimum amount of worker threads. server.tomcat.port-header=X-Forwarded-Port # Name of the HTTP header used to override the original port value. server.tomcat.protocol-header= # Header that holds the incoming protocol, usually named “X-Forwarded-Proto”. server.tomcat.protocol-header-https-value=https # Value of the protocol header that indicates that the incoming request uses SSL. server.tomcat.redirect-context-root= # Whether requests to the context root should be redirected by appending a / to the path. server.tomcat.remote-ip-header= # Name of the http header from which the remote ip is extracted. For instance X-FORWARDED-FOR server.tomcat.uri-encoding=UTF-8 # Character encoding to use to decode the URI. server.undertow.accesslog.dir= # Undertow access log directory. server.undertow.accesslog.enabled=false # Enable access log. server.undertow.accesslog.pattern=common # Format pattern for access logs. server.undertow.accesslog.prefix=access_log. # Log file name prefix. server.undertow.accesslog.rotate=true # Enable access log rotation. server.undertow.accesslog.suffix=log # Log file name suffix. server.undertow.buffer-size= # Size of each buffer in bytes. server.undertow.direct-buffers= # Allocate buffers outside the Java heap. server.undertow.io-threads= # Number of I/O threads to create for the worker. server.undertow.eager-filter-init=true # Whether servlet filters should be initialized on startup. server.undertow.max-http-post-size=0 # Maximum size in bytes of the HTTP post content. server.undertow.worker-threads= # Number of worker threads. FREEMARKER (FreeMarkerAutoConfiguration) spring.freemarker.allow-request-override=false # Set whether HttpServletRequest attributes are allowed to override (hide) controller generated model attributes of the same name. spring.freemarker.allow-session-override=false # Set whether HttpSession attributes are allowed to override (hide) controller generated model attributes of the same name. spring.freemarker.cache=false # Enable template caching. spring.freemarker.charset=UTF-8 # Template encoding. spring.freemarker.check-template-location=true # Check that the templates location exists. spring.freemarker.content-type=text/html # Content-Type value. spring.freemarker.enabled=true # Enable MVC view resolution for this technology. spring.freemarker.expose-request-attributes=false # Set whether all request attributes should be added to the model prior to merging with the template. spring.freemarker.expose-session-attributes=false # Set whether all HttpSession attributes should be added to the model prior to merging with the template. spring.freemarker.expose-spring-macro-helpers=true # Set whether to expose a RequestContext for use by Spring’s macro library, under the name “springMacroRequestContext”. spring.freemarker.prefer-file-system-access=true # Prefer file system access for template loading. File system access enables hot detection of template changes. spring.freemarker.prefix= # Prefix that gets prepended to view names when building a URL. spring.freemarker.request-context-attribute= # Name of the RequestContext attribute for all views. spring.freemarker.settings.*= # Well-known FreeMarker keys which will be passed to FreeMarker’s Configuration. spring.freemarker.suffix= # Suffix that gets appended to view names when building a URL. spring.freemarker.template-loader-path=classpath:/templates/ # Comma-separated list of template paths. spring.freemarker.view-names= # White list of view names that can be resolved. GROOVY TEMPLATES (GroovyTemplateAutoConfiguration) spring.groovy.template.allow-request-override=false # Set whether HttpServletRequest attributes are allowed to override (hide) controller generated model attributes of the same name. spring.groovy.template.allow-session-override=false # Set whether HttpSession attributes are allowed to override (hide) controller generated model attributes of the same name. spring.groovy.template.cache= # Enable template caching. spring.groovy.template.charset=UTF-8 # Template encoding. spring.groovy.template.check-template-location=true # Check that the templates location exists. spring.groovy.template.configuration.*= # See GroovyMarkupConfigurer spring.groovy.template.content-type=test/html # Content-Type value. spring.groovy.template.enabled=true # Enable MVC view resolution for this technology. spring.groovy.template.expose-request-attributes=false # Set whether all request attributes should be added to the model prior to merging with the template. spring.groovy.template.expose-session-attributes=false # Set whether all HttpSession attributes should be added to the model prior to merging with the template. spring.groovy.template.expose-spring-macro-helpers=true # Set whether to expose a RequestContext for use by Spring’s macro library, under the name “springMacroRequestContext”. spring.groovy.template.prefix= # Prefix that gets prepended to view names when building a URL. spring.groovy.template.request-context-attribute= # Name of the RequestContext attribute for all views. spring.groovy.template.resource-loader-path=classpath:/templates/ # Template path. spring.groovy.template.suffix=.tpl # Suffix that gets appended to view names when building a URL. spring.groovy.template.view-names= # White list of view names that can be resolved. SPRING HATEOAS (HateoasProperties) spring.hateoas.use-hal-as-default-json-media-type=true # Specify if application/hal+json responses should be sent to requests that accept application/json. HTTP message conversion spring.http.converters.preferred-json-mapper=jackson # Preferred JSON mapper to use for HTTP message conversion. Set to “gson” to force the use of Gson when both it and Jackson are on the classpath. HTTP encoding (HttpEncodingProperties) spring.http.encoding.charset=UTF-8 # Charset of HTTP requests and responses. Added to the “Content-Type” header if not set explicitly. spring.http.encoding.enabled=true # Enable http encoding support. spring.http.encoding.force= # Force the encoding to the configured charset on HTTP requests and responses. spring.http.encoding.force-request= # Force the encoding to the configured charset on HTTP requests. Defaults to true when “force” has not been specified. spring.http.encoding.force-response= # Force the encoding to the configured charset on HTTP responses. spring.http.encoding.mapping= # Locale to Encoding mapping. MULTIPART (MultipartProperties) spring.servlet.multipart.enabled=true # Enable support of multipart uploads. spring.servlet.multipart.file-size-threshold=0 # Threshold after which files will be written to disk. Values can use the suffixes “MB” or “KB” to indicate megabytes or kilobytes respectively. spring.servlet.multipart.location= # Intermediate location of uploaded files. spring.servlet.multipart.max-file-size=1MB # Max file size. Values can use the suffixes “MB” or “KB” to indicate megabytes or kilobytes respectively. spring.servlet.multipart.max-request-size=10MB # Max request size. Values can use the suffixes “MB” or “KB” to indicate megabytes or kilobytes respectively. spring.servlet.multipart.resolve-lazily=false # Whether to resolve the multipart request lazily at the time of file or parameter access. JACKSON (JacksonProperties) spring.jackson.date-format= # Date format string or a fully-qualified date format class name. For instance yyyy-MM-dd HH:mm:ss. spring.jackson.default-property-inclusion= # Controls the inclusion of properties during serialization. spring.jackson.deserialization.= # Jackson on/off features that affect the way Java objects are deserialized. spring.jackson.generator.= # Jackson on/off features for generators. spring.jackson.joda-date-time-format= # Joda date time format string. If not configured, “date-format” will be used as a fallback if it is configured with a format string. spring.jackson.locale= # Locale used for formatting. spring.jackson.mapper.= # Jackson general purpose on/off features. spring.jackson.parser.= # Jackson on/off features for parsers. spring.jackson.property-naming-strategy= # One of the constants on Jackson’s PropertyNamingStrategy. Can also be a fully-qualified class name of a PropertyNamingStrategy subclass. spring.jackson.serialization.*= # Jackson on/off features that affect the way Java objects are serialized. spring.jackson.time-zone= # Time zone used when formatting dates. For instance America/Los_Angeles JERSEY (JerseyProperties) spring.jersey.application-path= # Path that serves as the base URI for the application. Overrides the value of “@ApplicationPath” if specified. spring.jersey.filter.order=0 # Jersey filter chain order. spring.jersey.init.*= # Init parameters to pass to Jersey via the servlet or filter. spring.jersey.servlet.load-on-startup=-1 # Load on startup priority of the Jersey servlet. spring.jersey.type=servlet # Jersey integration type. SPRING LDAP (LdapProperties) spring.ldap.urls= # LDAP URLs of the server. spring.ldap.base= # Base suffix from which all operations should originate. spring.ldap.username= # Login user of the server. spring.ldap.password= # Login password of the server. spring.ldap.base-environment.*= # LDAP specification settings. EMBEDDED LDAP (EmbeddedLdapProperties) spring.ldap.embedded.base-dn= # The base DN spring.ldap.embedded.credential.username= # Embedded LDAP username. spring.ldap.embedded.credential.password= # Embedded LDAP password. spring.ldap.embedded.ldif=classpath:schema.ldif # Schema (LDIF) script resource reference. spring.ldap.embedded.port= # Embedded LDAP port. spring.ldap.embedded.validation.enabled=true # Enable LDAP schema validation. spring.ldap.embedded.validation.schema= # Path to the custom schema. SPRING MOBILE DEVICE VIEWS (DeviceDelegatingViewResolverAutoConfiguration) spring.mobile.devicedelegatingviewresolver.enable-fallback=false # Enable support for fallback resolution. spring.mobile.devicedelegatingviewresolver.enabled=false # Enable device view resolver. spring.mobile.devicedelegatingviewresolver.mobile-prefix=mobile/ # Prefix that gets prepended to view names for mobile devices. spring.mobile.devicedelegatingviewresolver.mobile-suffix= # Suffix that gets appended to view names for mobile devices. spring.mobile.devicedelegatingviewresolver.normal-prefix= # Prefix that gets prepended to view names for normal devices. spring.mobile.devicedelegatingviewresolver.normal-suffix= # Suffix that gets appended to view names for normal devices. spring.mobile.devicedelegatingviewresolver.tablet-prefix=tablet/ # Prefix that gets prepended to view names for tablet devices. spring.mobile.devicedelegatingviewresolver.tablet-suffix= # Suffix that gets appended to view names for tablet devices. SPRING MOBILE SITE PREFERENCE (SitePreferenceAutoConfiguration) spring.mobile.sitepreference.enabled=true # Enable SitePreferenceHandler. MUSTACHE TEMPLATES (MustacheAutoConfiguration) spring.mustache.allow-request-override= # Set whether HttpServletRequest attributes are allowed to override (hide) controller generated model attributes of the same name. spring.mustache.allow-session-override= # Set whether HttpSession attributes are allowed to override (hide) controller generated model attributes of the same name. spring.mustache.cache= # Enable template caching. spring.mustache.charset= # Template encoding. spring.mustache.check-template-location= # Check that the templates location exists. spring.mustache.content-type= # Content-Type value. spring.mustache.enabled= # Enable MVC view resolution for this technology. spring.mustache.expose-request-attributes= # Set whether all request attributes should be added to the model prior to merging with the template. spring.mustache.expose-session-attributes= # Set whether all HttpSession attributes should be added to the model prior to merging with the template. spring.mustache.expose-spring-macro-helpers= # Set whether to expose a RequestContext for use by Spring’s macro library, under the name “springMacroRequestContext”. spring.mustache.prefix=classpath:/templates/ # Prefix to apply to template names. spring.mustache.request-context-attribute= # Name of the RequestContext attribute for all views. spring.mustache.suffix=.mustache # Suffix to apply to template names. spring.mustache.view-names= # White list of view names that can be resolved. SPRING MVC (WebMvcProperties) spring.mvc.async.request-timeout= # Amount of time (in milliseconds) before asynchronous request handling times out. spring.mvc.date-format= # Date format to use. For instance dd/MM/yyyy. spring.mvc.dispatch-trace-request=false # Dispatch TRACE requests to the FrameworkServlet doService method. spring.mvc.dispatch-options-request=true # Dispatch OPTIONS requests to the FrameworkServlet doService method. spring.mvc.favicon.enabled=true # Enable resolution of favicon.ico. spring.mvc.formcontent.putfilter.enabled=true # Enable Spring’s HttpPutFormContentFilter. spring.mvc.ignore-default-model-on-redirect=true # If the content of the “default” model should be ignored during redirect scenarios. spring.mvc.locale= # Locale to use. By default, this locale is overridden by the “Accept-Language” header. spring.mvc.locale-resolver=accept-header # Define how the locale should be resolved. spring.mvc.log-resolved-exception=false # Enable warn logging of exceptions resolved by a “HandlerExceptionResolver”. spring.mvc.media-types.*= # Maps file extensions to media types for content negotiation. spring.mvc.message-codes-resolver-format= # Formatting strategy for message codes. For instance PREFIX_ERROR_CODE. spring.mvc.servlet.load-on-startup=-1 # Load on startup priority of the Spring Web Services servlet. spring.mvc.static-path-pattern=/** # Path pattern used for static resources. spring.mvc.throw-exception-if-no-handler-found=false # If a “NoHandlerFoundException” should be thrown if no Handler was found to process a request. spring.mvc.view.prefix= # Spring MVC view prefix. spring.mvc.view.suffix= # Spring MVC view suffix. SPRING RESOURCES HANDLING (ResourceProperties) spring.resources.add-mappings=true # Enable default resource handling. spring.resources.cache-period= # Cache period for the resources served by the resource handler, in seconds. spring.resources.chain.cache=true # Enable caching in the Resource chain. spring.resources.chain.enabled= # Enable the Spring Resource Handling chain. Disabled by default unless at least one strategy has been enabled. spring.resources.chain.gzipped=false # Enable resolution of already gzipped resources. spring.resources.chain.html-application-cache=false # Enable HTML5 application cache manifest rewriting. spring.resources.chain.strategy.content.enabled=false # Enable the content Version Strategy. spring.resources.chain.strategy.content.paths=/** # Comma-separated list of patterns to apply to the Version Strategy. spring.resources.chain.strategy.fixed.enabled=false # Enable the fixed Version Strategy. spring.resources.chain.strategy.fixed.paths=/** # Comma-separated list of patterns to apply to the Version Strategy. spring.resources.chain.strategy.fixed.version= # Version string to use for the Version Strategy. spring.resources.static-locations=classpath:/META-INF/resources/,classpath:/resources/,classpath:/static/,classpath:/public/ # Locations of static resources. SPRING SESSION (SessionProperties) spring.session.hazelcast.flush-mode=on-save # Sessions flush mode. spring.session.hazelcast.map-name=spring:session:sessions # Name of the map used to store sessions. spring.session.jdbc.initializer.enabled= # Create the required session tables on startup if necessary. Enabled automatically if the default table name is set or a custom schema is configured. spring.session.jdbc.schema=classpath:org/springframework/session/jdbc/schema-@@platform@@.sql # Path to the SQL file to use to initialize the database schema. spring.session.jdbc.table-name=SPRING_SESSION # Name of database table used to store sessions. spring.session.redis.flush-mode=on-save # Sessions flush mode. spring.session.redis.namespace= # Namespace for keys used to store sessions. spring.session.store-type= # Session store type. SPRING SOCIAL (SocialWebAutoConfiguration) spring.social.auto-connection-views=false # Enable the connection status view for supported providers. SPRING SOCIAL FACEBOOK (FacebookAutoConfiguration) spring.social.facebook.app-id= # your application’s Facebook App ID spring.social.facebook.app-secret= # your application’s Facebook App Secret SPRING SOCIAL LINKEDIN (LinkedInAutoConfiguration) spring.social.linkedin.app-id= # your application’s LinkedIn App ID spring.social.linkedin.app-secret= # your application’s LinkedIn App Secret SPRING SOCIAL TWITTER (TwitterAutoConfiguration) spring.social.twitter.app-id= # your application’s Twitter App ID spring.social.twitter.app-secret= # your application’s Twitter App Secret THYMELEAF (ThymeleafAutoConfiguration) spring.thymeleaf.cache=true # Enable template caching. spring.thymeleaf.check-template=true # Check that the template exists before rendering it. spring.thymeleaf.check-template-location=true # Check that the templates location exists. spring.thymeleaf.enabled=true # Enable Thymeleaf view resolution for Web frameworks. spring.thymeleaf.encoding=UTF-8 # Template files encoding. spring.thymeleaf.excluded-view-names= # Comma-separated list of view names that should be excluded from resolution. spring.thymeleaf.mode=HTML5 # Template mode to be applied to templates. See also StandardTemplateModeHandlers. spring.thymeleaf.prefix=classpath:/templates/ # Prefix that gets prepended to view names when building a URL. spring.thymeleaf.reactive.max-chunk-size= # Maximum size of data buffers used for writing to the response, in bytes. spring.thymeleaf.reactive.media-types= # Media types supported by the view technology. spring.thymeleaf.servlet.content-type=text/html # Content-Type value written to HTTP responses. spring.thymeleaf.suffix=.html # Suffix that gets appended to view names when building a URL. spring.thymeleaf.template-resolver-order= # Order of the template resolver in the chain. spring.thymeleaf.view-names= # Comma-separated list of view names that can be resolved. SPRING WEB FLUX (WebFluxProperties) spring.webflux.static-path-pattern=/** # Path pattern used for static resources. SPRING WEB SERVICES (WebServicesProperties) spring.webservices.path=/services # Path that serves as the base URI for the services. spring.webservices.servlet.init= # Servlet init parameters to pass to Spring Web Services. spring.webservices.servlet.load-on-startup=-1 # Load on startup priority of the Spring Web Services servlet. SECURITY (SecurityProperties) security.basic.authorize-mode=role # Security authorize mode to apply. security.basic.enabled=true # Enable basic authentication. security.basic.path=/** # Comma-separated list of paths to secure. security.basic.realm=Spring # HTTP basic realm name. security.enable-csrf=false # Enable Cross Site Request Forgery support. security.filter-order=0 # Security filter chain order. security.filter-dispatcher-types=ASYNC, FORWARD, INCLUDE, REQUEST # Security filter chain dispatcher types. security.headers.cache=true # Enable cache control HTTP headers. security.headers.content-security-policy= # Value for content security policy header. security.headers.content-security-policy-mode=default # Content security policy mode. security.headers.content-type=true # Enable “X-Content-Type-Options” header. security.headers.frame=true # Enable “X-Frame-Options” header. security.headers.hsts=all # HTTP Strict Transport Security (HSTS) mode (none, domain, all). security.headers.xss=true # Enable cross site scripting (XSS) protection. security.ignored= # Comma-separated list of paths to exclude from the default secured paths. security.require-ssl=false # Enable secure channel for all requests. security.sessions=stateless # Session creation policy (always, never, if_required, stateless). security.user.name=user # Default user name. security.user.password= # Password for the default user name. A random password is logged on startup by default. security.user.role=USER # Granted roles for the default user name. SECURITY OAUTH2 CLIENT (OAuth2ClientProperties) security.oauth2.client.client-id= # OAuth2 client id. security.oauth2.client.client-secret= # OAuth2 client secret. A random secret is generated by default SECURITY OAUTH2 RESOURCES (ResourceServerProperties) security.oauth2.resource.filter-order= # The order of the filter chain used to authenticate tokens. security.oauth2.resource.id= # Identifier of the resource. security.oauth2.resource.jwt.key-uri= # The URI of the JWT token. Can be set if the value is not available and the key is public. security.oauth2.resource.jwt.key-value= # The verification key of the JWT token. Can either be a symmetric secret or PEM-encoded RSA public key. security.oauth2.resource.prefer-token-info=true # Use the token info, can be set to false to use the user info. security.oauth2.resource.service-id=resource # security.oauth2.resource.token-info-uri= # URI of the token decoding endpoint. security.oauth2.resource.token-type= # The token type to send when using the userInfoUri. security.oauth2.resource.user-info-uri= # URI of the user endpoint. SECURITY OAUTH2 SSO (OAuth2SsoProperties) security.oauth2.sso.filter-order= # Filter order to apply if not providing an explicit WebSecurityConfigurerAdapter security.oauth2.sso.login-path=/login # Path to the login page, i.e. the one that triggers the redirect to the OAuth2 Authorization Server FLYWAY (FlywayProperties) flyway.allow-mixed-migrations= # flyway.baseline-description= # flyway.baseline-on-migrate= # flyway.baseline-version=1 # version to start migration flyway.check-location=false # Check that migration scripts location exists. flyway.clean-disabled= # flyway.clean-on-validation-error= # flyway.enabled=true # Enable flyway. flyway.encoding= # flyway.ignore-failed-future-migration= # flyway.ignore-future-migrations= # flyway.ignore-missing-migrations= # flyway.init-sqls= # SQL statements to execute to initialize a connection immediately after obtaining it. flyway.installed-by= # flyway.locations=classpath:db/migration # locations of migrations scripts flyway.out-of-order= # flyway.password= # JDBC password if you want Flyway to create its own DataSource flyway.placeholder-prefix= # flyway.placeholder-replacement= # flyway.placeholder-suffix= # flyway.placeholders.*= # flyway.repeatable-sql-migration-prefix= # flyway.schemas= # schemas to update flyway.skip-default-callbacks= # flyway.skip-default-resolvers= # flyway.sql-migration-prefix=V # flyway.sql-migration-separator= # flyway.sql-migration-suffix=.sql # flyway.table= # flyway.target= # flyway.url= # JDBC url of the database to migrate. If not set, the primary configured data source is used. flyway.user= # Login user of the database to migrate. flyway.validate-on-migrate= # LIQUIBASE (LiquibaseProperties) liquibase.change-log=classpath:/db/changelog/db.changelog-master.yaml # Change log configuration path. liquibase.check-change-log-location=true # Check the change log location exists. liquibase.contexts= # Comma-separated list of runtime contexts to use. liquibase.default-schema= # Default database schema. liquibase.drop-first=false # Drop the database schema first. liquibase.enabled=true # Enable liquibase support. liquibase.labels= # Comma-separated list of runtime labels to use. liquibase.parameters.*= # Change log parameters. liquibase.password= # Login password of the database to migrate. liquibase.rollback-file= # File to which rollback SQL will be written when an update is performed. liquibase.url= # JDBC url of the database to migrate. If not set, the primary configured data source is used. liquibase.user= # Login user of the database to migrate. COUCHBASE (CouchbaseProperties) spring.couchbase.bootstrap-hosts= # Couchbase nodes (host or IP address) to bootstrap from. spring.couchbase.bucket.name=default # Name of the bucket to connect to. spring.couchbase.bucket.password= # Password of the bucket. spring.couchbase.env.endpoints.key-value=1 # Number of sockets per node against the Key/value service. spring.couchbase.env.endpoints.query=1 # Number of sockets per node against the Query (N1QL) service. spring.couchbase.env.endpoints.view=1 # Number of sockets per node against the view service. spring.couchbase.env.ssl.enabled= # Enable SSL support. Enabled automatically if a “keyStore” is provided unless specified otherwise. spring.couchbase.env.ssl.key-store= # Path to the JVM key store that holds the certificates. spring.couchbase.env.ssl.key-store-password= # Password used to access the key store. spring.couchbase.env.timeouts.connect=5000 # Bucket connections timeout in milliseconds. spring.couchbase.env.timeouts.key-value=2500 # Blocking operations performed on a specific key timeout in milliseconds. spring.couchbase.env.timeouts.query=7500 # N1QL query operations timeout in milliseconds. spring.couchbase.env.timeouts.socket-connect=1000 # Socket connect connections timeout in milliseconds. spring.couchbase.env.timeouts.view=7500 # Regular and geospatial view operations timeout in milliseconds. DAO (PersistenceExceptionTranslationAutoConfiguration) spring.dao.exceptiontranslation.enabled=true # Enable the PersistenceExceptionTranslationPostProcessor. CASSANDRA (CassandraProperties) spring.data.cassandra.cluster-name= # Name of the Cassandra cluster. spring.data.cassandra.compression=none # Compression supported by the Cassandra binary protocol. spring.data.cassandra.connect-timeout-millis= # Socket option: connection time out. spring.data.cassandra.consistency-level= # Queries consistency level. spring.data.cassandra.contact-points=localhost # Comma-separated list of cluster node addresses. spring.data.cassandra.fetch-size= # Queries default fetch size. spring.data.cassandra.keyspace-name= # Keyspace name to use. spring.data.cassandra.load-balancing-policy= # Class name of the load balancing policy. spring.data.cassandra.port= # Port of the Cassandra server. spring.data.cassandra.password= # Login password of the server. spring.data.cassandra.reactive-repositories.enabled=true # Enable Cassandra reactive repositories. spring.data.cassandra.read-timeout-millis= # Socket option: read time out. spring.data.cassandra.reconnection-policy= # Reconnection policy class. spring.data.cassandra.repositories.enabled= # Enable Cassandra repositories. spring.data.cassandra.retry-policy= # Class name of the retry policy. spring.data.cassandra.serial-consistency-level= # Queries serial consistency level. spring.data.cassandra.schema-action=none # Schema action to take at startup. spring.data.cassandra.ssl=false # Enable SSL support. spring.data.cassandra.username= # Login user of the server. DATA COUCHBASE (CouchbaseDataProperties) spring.data.couchbase.auto-index=false # Automatically create views and indexes. spring.data.couchbase.consistency=read-your-own-writes # Consistency to apply by default on generated queries. spring.data.couchbase.repositories.enabled=true # Enable Couchbase repositories. ELASTICSEARCH (ElasticsearchProperties) spring.data.elasticsearch.cluster-name=elasticsearch # Elasticsearch cluster name. spring.data.elasticsearch.cluster-nodes= # Comma-separated list of cluster node addresses. If not specified, starts a client node. spring.data.elasticsearch.properties.*= # Additional properties used to configure the client. spring.data.elasticsearch.repositories.enabled=true # Enable Elasticsearch repositories. DATA LDAP spring.data.ldap.repositories.enabled=true # Enable LDAP repositories. MONGODB (MongoProperties) spring.data.mongodb.authentication-database= # Authentication database name. spring.data.mongodb.database=test # Database name. spring.data.mongodb.field-naming-strategy= # Fully qualified name of the FieldNamingStrategy to use. spring.data.mongodb.grid-fs-database= # GridFS database name. spring.data.mongodb.host=localhost # Mongo server host. Cannot be set with uri. spring.data.mongodb.password= # Login password of the mongo server. Cannot be set with uri. spring.data.mongodb.port=27017 # Mongo server port. Cannot be set with uri. spring.data.mongodb.reactive-repositories.enabled=true # Enable Mongo reactive repositories. spring.data.mongodb.repositories.enabled=true # Enable Mongo repositories. spring.data.mongodb.uri=mongodb://localhost/test # Mongo database URI. Cannot be set with host, port and credentials. spring.data.mongodb.username= # Login user of the mongo server. Cannot be set with uri. DATA REDIS spring.data.redis.repositories.enabled=true # Enable Redis repositories. NEO4J (Neo4jProperties) spring.data.neo4j.auto-index=none # Auto index mode. spring.data.neo4j.embedded.enabled=true # Enable embedded mode if the embedded driver is available. spring.data.neo4j.open-in-view=false # Register OpenSessionInViewInterceptor. Binds a Neo4j Session to the thread for the entire processing of the request. spring.data.neo4j.password= # Login password of the server. spring.data.neo4j.repositories.enabled=true # Enable Neo4j repositories. spring.data.neo4j.uri= # URI used by the driver. Auto-detected by default. spring.data.neo4j.username= # Login user of the server. DATA REST (RepositoryRestProperties) spring.data.rest.base-path= # Base path to be used by Spring Data REST to expose repository resources. spring.data.rest.default-page-size= # Default size of pages. spring.data.rest.detection-strategy=default # Strategy to use to determine which repositories get exposed. spring.data.rest.enable-enum-translation= # Enable enum value translation via the Spring Data REST default resource bundle. spring.data.rest.limit-param-name= # Name of the URL query string parameter that indicates how many results to return at once. spring.data.rest.max-page-size= # Maximum size of pages. spring.data.rest.page-param-name= # Name of the URL query string parameter that indicates what page to return. spring.data.rest.return-body-on-create= # Return a response body after creating an entity. spring.data.rest.return-body-on-update= # Return a response body after updating an entity. spring.data.rest.sort-param-name= # Name of the URL query string parameter that indicates what direction to sort results. SOLR (SolrProperties) spring.data.solr.host=http://127.0.0.1:8983/solr # Solr host. Ignored if “zk-host” is set. spring.data.solr.repositories.enabled=true # Enable Solr repositories. spring.data.solr.zk-host= # ZooKeeper host address in the form HOST:PORT. DATASOURCE (DataSourceAutoConfiguration &amp; DataSourceProperties) spring.datasource.continue-on-error=false # Do not stop if an error occurs while initializing the database. spring.datasource.data= # Data (DML) script resource references. spring.datasource.data-username= # User of the database to execute DML scripts (if different). spring.datasource.data-password= # Password of the database to execute DML scripts (if different). spring.datasource.dbcp2.= # Commons DBCP2 specific settings spring.datasource.driver-class-name= # Fully qualified name of the JDBC driver. Auto-detected based on the URL by default. spring.datasource.generate-unique-name=false # Generate a random datasource name. spring.datasource.hikari.= # Hikari specific settings spring.datasource.initialize=true # Populate the database using ‘data.sql’. spring.datasource.jmx-enabled=false # Enable JMX support (if provided by the underlying pool). spring.datasource.jndi-name= # JNDI location of the datasource. Class, url, username &amp; password are ignored when set. spring.datasource.name=testdb # Name of the datasource. spring.datasource.password= # Login password of the database. spring.datasource.platform=all # Platform to use in the schema resource (schema-${platform}.sql). spring.datasource.schema= # Schema (DDL) script resource references. spring.datasource.schema-username= # User of the database to execute DDL scripts (if different). spring.datasource.schema-password= # Password of the database to execute DDL scripts (if different). spring.datasource.separator=; # Statement separator in SQL initialization scripts. spring.datasource.sql-script-encoding= # SQL scripts encoding. spring.datasource.tomcat.*= # Tomcat datasource specific settings spring.datasource.type= # Fully qualified name of the connection pool implementation to use. By default, it is auto-detected from the classpath. spring.datasource.url= # JDBC url of the database. spring.datasource.username= # Login user of the database. spring.datasource.xa.data-source-class-name= # XA datasource fully qualified name. spring.datasource.xa.properties= # Properties to pass to the XA data source. JEST (Elasticsearch HTTP client) (JestProperties) spring.elasticsearch.jest.connection-timeout=3000 # Connection timeout in milliseconds. spring.elasticsearch.jest.multi-threaded=true # Enable connection requests from multiple execution threads. spring.elasticsearch.jest.password= # Login password. spring.elasticsearch.jest.proxy.host= # Proxy host the HTTP client should use. spring.elasticsearch.jest.proxy.port= # Proxy port the HTTP client should use. spring.elasticsearch.jest.read-timeout=3000 # Read timeout in milliseconds. spring.elasticsearch.jest.uris=http://localhost:9200 # Comma-separated list of the Elasticsearch instances to use. spring.elasticsearch.jest.username= # Login user. H2 Web Console (H2ConsoleProperties) spring.h2.console.enabled=false # Enable the console. spring.h2.console.path=/h2-console # Path at which the console will be available. spring.h2.console.settings.trace=false # Enable trace output. spring.h2.console.settings.web-allow-others=false # Enable remote access. JOOQ (JooqAutoConfiguration) spring.jooq.sql-dialect= # SQLDialect JOOQ used when communicating with the configured datasource. For instance POSTGRES JPA (JpaBaseConfiguration, HibernateJpaAutoConfiguration) spring.data.jpa.repositories.enabled=true # Enable JPA repositories. spring.jpa.database= # Target database to operate on, auto-detected by default. Can be alternatively set using the “databasePlatform” property. spring.jpa.database-platform= # Name of the target database to operate on, auto-detected by default. Can be alternatively set using the “Database” enum. spring.jpa.generate-ddl=false # Initialize the schema on startup. spring.jpa.hibernate.ddl-auto= # DDL mode. This is actually a shortcut for the “hibernate.hbm2ddl.auto” property. Default to “create-drop” when using an embedded database, “none” otherwise. spring.jpa.hibernate.naming.implicit-strategy= # Hibernate 5 implicit naming strategy fully qualified name. spring.jpa.hibernate.naming.physical-strategy= # Hibernate 5 physical naming strategy fully qualified name. spring.jpa.hibernate.use-new-id-generator-mappings= # Use Hibernate’s newer IdentifierGenerator for AUTO, TABLE and SEQUENCE. spring.jpa.open-in-view=true # Register OpenEntityManagerInViewInterceptor. Binds a JPA EntityManager to the thread for the entire processing of the request. spring.jpa.properties.*= # Additional native properties to set on the JPA provider. spring.jpa.show-sql=false # Enable logging of SQL statements. JTA (JtaAutoConfiguration) spring.jta.enabled=true # Enable JTA support. spring.jta.log-dir= # Transaction logs directory. spring.jta.transaction-manager-id= # Transaction manager unique identifier. ATOMIKOS (AtomikosProperties) spring.jta.atomikos.connectionfactory.borrow-connection-timeout=30 # Timeout, in seconds, for borrowing connections from the pool. spring.jta.atomikos.connectionfactory.ignore-session-transacted-flag=true # Whether or not to ignore the transacted flag when creating session. spring.jta.atomikos.connectionfactory.local-transaction-mode=false # Whether or not local transactions are desired. spring.jta.atomikos.connectionfactory.maintenance-interval=60 # The time, in seconds, between runs of the pool’s maintenance thread. spring.jta.atomikos.connectionfactory.max-idle-time=60 # The time, in seconds, after which connections are cleaned up from the pool. spring.jta.atomikos.connectionfactory.max-lifetime=0 # The time, in seconds, that a connection can be pooled for before being destroyed. 0 denotes no limit. spring.jta.atomikos.connectionfactory.max-pool-size=1 # The maximum size of the pool. spring.jta.atomikos.connectionfactory.min-pool-size=1 # The minimum size of the pool. spring.jta.atomikos.connectionfactory.reap-timeout=0 # The reap timeout, in seconds, for borrowed connections. 0 denotes no limit. spring.jta.atomikos.connectionfactory.unique-resource-name=jmsConnectionFactory # The unique name used to identify the resource during recovery. spring.jta.atomikos.datasource.borrow-connection-timeout=30 # Timeout, in seconds, for borrowing connections from the pool. spring.jta.atomikos.datasource.default-isolation-level= # Default isolation level of connections provided by the pool. spring.jta.atomikos.datasource.login-timeout= # Timeout, in seconds, for establishing a database connection. spring.jta.atomikos.datasource.maintenance-interval=60 # The time, in seconds, between runs of the pool’s maintenance thread. spring.jta.atomikos.datasource.max-idle-time=60 # The time, in seconds, after which connections are cleaned up from the pool. spring.jta.atomikos.datasource.max-lifetime=0 # The time, in seconds, that a connection can be pooled for before being destroyed. 0 denotes no limit. spring.jta.atomikos.datasource.max-pool-size=1 # The maximum size of the pool. spring.jta.atomikos.datasource.min-pool-size=1 # The minimum size of the pool. spring.jta.atomikos.datasource.reap-timeout=0 # The reap timeout, in seconds, for borrowed connections. 0 denotes no limit. spring.jta.atomikos.datasource.test-query= # SQL query or statement used to validate a connection before returning it. spring.jta.atomikos.datasource.unique-resource-name=dataSource # The unique name used to identify the resource during recovery. spring.jta.atomikos.properties.checkpoint-interval=500 # Interval between checkpoints. spring.jta.atomikos.properties.console-file-count=1 # Number of debug logs files that can be created. spring.jta.atomikos.properties.console-file-limit=-1 # How many bytes can be stored at most in debug logs files. spring.jta.atomikos.properties.console-file-name=tm.out # Debug logs file name. spring.jta.atomikos.properties.console-log-level=warn # Console log level. spring.jta.atomikos.properties.default-jta-timeout=10000 # Default timeout for JTA transactions. spring.jta.atomikos.properties.enable-logging=true # Enable disk logging. spring.jta.atomikos.properties.force-shutdown-on-vm-exit=false # Specify if a VM shutdown should trigger forced shutdown of the transaction core. spring.jta.atomikos.properties.log-base-dir= # Directory in which the log files should be stored. spring.jta.atomikos.properties.log-base-name=tmlog # Transactions log file base name. spring.jta.atomikos.properties.max-actives=50 # Maximum number of active transactions. spring.jta.atomikos.properties.max-timeout=300000 # Maximum timeout (in milliseconds) that can be allowed for transactions. spring.jta.atomikos.properties.output-dir= # Directory in which to store the debug log files. spring.jta.atomikos.properties.serial-jta-transactions=true # Specify if sub-transactions should be joined when possible. spring.jta.atomikos.properties.service= # Transaction manager implementation that should be started. spring.jta.atomikos.properties.threaded-two-phase-commit=false # Use different (and concurrent) threads for two-phase commit on the participating resources. spring.jta.atomikos.properties.transaction-manager-unique-name= # Transaction manager’s unique name. BITRONIX spring.jta.bitronix.connectionfactory.acquire-increment=1 # Number of connections to create when growing the pool. spring.jta.bitronix.connectionfactory.acquisition-interval=1 # Time, in seconds, to wait before trying to acquire a connection again after an invalid connection was acquired. spring.jta.bitronix.connectionfactory.acquisition-timeout=30 # Timeout, in seconds, for acquiring connections from the pool. spring.jta.bitronix.connectionfactory.allow-local-transactions=true # Whether or not the transaction manager should allow mixing XA and non-XA transactions. spring.jta.bitronix.connectionfactory.apply-transaction-timeout=false # Whether or not the transaction timeout should be set on the XAResource when it is enlisted. spring.jta.bitronix.connectionfactory.automatic-enlisting-enabled=true # Whether or not resources should be enlisted and delisted automatically. spring.jta.bitronix.connectionfactory.cache-producers-consumers=true # Whether or not produces and consumers should be cached. spring.jta.bitronix.connectionfactory.defer-connection-release=true # Whether or not the provider can run many transactions on the same connection and supports transaction interleaving. spring.jta.bitronix.connectionfactory.ignore-recovery-failures=false # Whether or not recovery failures should be ignored. spring.jta.bitronix.connectionfactory.max-idle-time=60 # The time, in seconds, after which connections are cleaned up from the pool. spring.jta.bitronix.connectionfactory.max-pool-size=10 # The maximum size of the pool. 0 denotes no limit. spring.jta.bitronix.connectionfactory.min-pool-size=0 # The minimum size of the pool. spring.jta.bitronix.connectionfactory.password= # The password to use to connect to the JMS provider. spring.jta.bitronix.connectionfactory.share-transaction-connections=false # Whether or not connections in the ACCESSIBLE state can be shared within the context of a transaction. spring.jta.bitronix.connectionfactory.test-connections=true # Whether or not connections should be tested when acquired from the pool. spring.jta.bitronix.connectionfactory.two-pc-ordering-position=1 # The position that this resource should take during two-phase commit (always first is Integer.MIN_VALUE, always last is Integer.MAX_VALUE). spring.jta.bitronix.connectionfactory.unique-name=jmsConnectionFactory # The unique name used to identify the resource during recovery. spring.jta.bitronix.connectionfactory.use-tm-join=true Whether or not TMJOIN should be used when starting XAResources. spring.jta.bitronix.connectionfactory.user= # The user to use to connect to the JMS provider. spring.jta.bitronix.datasource.acquire-increment=1 # Number of connections to create when growing the pool. spring.jta.bitronix.datasource.acquisition-interval=1 # Time, in seconds, to wait before trying to acquire a connection again after an invalid connection was acquired. spring.jta.bitronix.datasource.acquisition-timeout=30 # Timeout, in seconds, for acquiring connections from the pool. spring.jta.bitronix.datasource.allow-local-transactions=true # Whether or not the transaction manager should allow mixing XA and non-XA transactions. spring.jta.bitronix.datasource.apply-transaction-timeout=false # Whether or not the transaction timeout should be set on the XAResource when it is enlisted. spring.jta.bitronix.datasource.automatic-enlisting-enabled=true # Whether or not resources should be enlisted and delisted automatically. spring.jta.bitronix.datasource.cursor-holdability= # The default cursor holdability for connections. spring.jta.bitronix.datasource.defer-connection-release=true # Whether or not the database can run many transactions on the same connection and supports transaction interleaving. spring.jta.bitronix.datasource.enable-jdbc4-connection-test= # Whether or not Connection.isValid() is called when acquiring a connection from the pool. spring.jta.bitronix.datasource.ignore-recovery-failures=false # Whether or not recovery failures should be ignored. spring.jta.bitronix.datasource.isolation-level= # The default isolation level for connections. spring.jta.bitronix.datasource.local-auto-commit= # The default auto-commit mode for local transactions. spring.jta.bitronix.datasource.login-timeout= # Timeout, in seconds, for establishing a database connection. spring.jta.bitronix.datasource.max-idle-time=60 # The time, in seconds, after which connections are cleaned up from the pool. spring.jta.bitronix.datasource.max-pool-size=10 # The maximum size of the pool. 0 denotes no limit. spring.jta.bitronix.datasource.min-pool-size=0 # The minimum size of the pool. spring.jta.bitronix.datasource.prepared-statement-cache-size=0 # The target size of the prepared statement cache. 0 disables the cache. spring.jta.bitronix.datasource.share-transaction-connections=false # Whether or not connections in the ACCESSIBLE state can be shared within the context of a transaction. spring.jta.bitronix.datasource.test-query= # SQL query or statement used to validate a connection before returning it. spring.jta.bitronix.datasource.two-pc-ordering-position=1 # The position that this resource should take during two-phase commit (always first is Integer.MIN_VALUE, always last is Integer.MAX_VALUE). spring.jta.bitronix.datasource.unique-name=dataSource # The unique name used to identify the resource during recovery. spring.jta.bitronix.datasource.use-tm-join=true Whether or not TMJOIN should be used when starting XAResources. spring.jta.bitronix.properties.allow-multiple-lrc=false # Allow multiple LRC resources to be enlisted into the same transaction. spring.jta.bitronix.properties.asynchronous2-pc=false # Enable asynchronously execution of two phase commit. spring.jta.bitronix.properties.background-recovery-interval-seconds=60 # Interval in seconds at which to run the recovery process in the background. spring.jta.bitronix.properties.current-node-only-recovery=true # Recover only the current node. spring.jta.bitronix.properties.debug-zero-resource-transaction=false # Log the creation and commit call stacks of transactions executed without a single enlisted resource. spring.jta.bitronix.properties.default-transaction-timeout=60 # Default transaction timeout in seconds. spring.jta.bitronix.properties.disable-jmx=false # Enable JMX support. spring.jta.bitronix.properties.exception-analyzer= # Set the fully qualified name of the exception analyzer implementation to use. spring.jta.bitronix.properties.filter-log-status=false # Enable filtering of logs so that only mandatory logs are written. spring.jta.bitronix.properties.force-batching-enabled=true # Set if disk forces are batched. spring.jta.bitronix.properties.forced-write-enabled=true # Set if logs are forced to disk. spring.jta.bitronix.properties.graceful-shutdown-interval=60 # Maximum amount of seconds the TM will wait for transactions to get done before aborting them at shutdown time. spring.jta.bitronix.properties.jndi-transaction-synchronization-registry-name= # JNDI name of the TransactionSynchronizationRegistry. spring.jta.bitronix.properties.jndi-user-transaction-name= # JNDI name of the UserTransaction. spring.jta.bitronix.properties.journal=disk # Name of the journal. Can be ‘disk’, ‘null’ or a class name. spring.jta.bitronix.properties.log-part1-filename=btm1.tlog # Name of the first fragment of the journal. spring.jta.bitronix.properties.log-part2-filename=btm2.tlog # Name of the second fragment of the journal. spring.jta.bitronix.properties.max-log-size-in-mb=2 # Maximum size in megabytes of the journal fragments. spring.jta.bitronix.properties.resource-configuration-filename= # ResourceLoader configuration file name. spring.jta.bitronix.properties.server-id= # ASCII ID that must uniquely identify this TM instance. Default to the machine’s IP address. spring.jta.bitronix.properties.skip-corrupted-logs=false # Skip corrupted transactions log entries. spring.jta.bitronix.properties.warn-about-zero-resource-transaction=true # Log a warning for transactions executed without a single enlisted resource. NARAYANA (NarayanaProperties) spring.jta.narayana.default-timeout=60 # Transaction timeout in seconds. spring.jta.narayana.expiry-scanners=com.arjuna.ats.internal.arjuna.recovery.ExpiredTransactionStatusManagerScanner # Comma-separated list of expiry scanners. spring.jta.narayana.log-dir= # Transaction object store directory. spring.jta.narayana.one-phase-commit=true # Enable one phase commit optimisation. spring.jta.narayana.periodic-recovery-period=120 # Interval in which periodic recovery scans are performed in seconds. spring.jta.narayana.recovery-backoff-period=10 # Back off period between first and second phases of the recovery scan in seconds. spring.jta.narayana.recovery-db-pass= # Database password to be used by recovery manager. spring.jta.narayana.recovery-db-user= # Database username to be used by recovery manager. spring.jta.narayana.recovery-jms-pass= # JMS password to be used by recovery manager. spring.jta.narayana.recovery-jms-user= # JMS username to be used by recovery manager. spring.jta.narayana.recovery-modules= # Comma-separated list of recovery modules. spring.jta.narayana.transaction-manager-id=1 # Unique transaction manager id. spring.jta.narayana.xa-resource-orphan-filters= # Comma-separated list of orphan filters. EMBEDDED MONGODB (EmbeddedMongoProperties) spring.mongodb.embedded.features=SYNC_DELAY # Comma-separated list of features to enable. spring.mongodb.embedded.storage.database-dir= # Directory used for data storage. spring.mongodb.embedded.storage.oplog-size= # Maximum size of the oplog in megabytes. spring.mongodb.embedded.storage.repl-set-name= # Name of the replica set. spring.mongodb.embedded.version=2.6.10 # Version of Mongo to use. REDIS (RedisProperties) spring.redis.cluster.max-redirects= # Maximum number of redirects to follow when executing commands across the cluster. spring.redis.cluster.nodes= # Comma-separated list of “host:port” pairs to bootstrap from. spring.redis.database=0 # Database index used by the connection factory. spring.redis.url= # Connection URL, will override host, port and password (user will be ignored), e.g. redis://user:password@example.com:6379 spring.redis.host=localhost # Redis server host. spring.redis.jedis.pool.max-active=8 # Max number of connections that can be allocated by the pool at a given time. Use a negative value for no limit. spring.redis.jedis.pool.max-idle=8 # Max number of “idle” connections in the pool. Use a negative value to indicate an unlimited number of idle connections. spring.redis.jedis.pool.max-wait=-1 # Maximum amount of time (in milliseconds) a connection allocation should block before throwing an exception when the pool is exhausted. Use a negative value to block indefinitely. spring.redis.jedis.pool.min-idle=0 # Target for the minimum number of idle connections to maintain in the pool. This setting only has an effect if it is positive. spring.redis.lettuce.pool.max-active=8 # Max number of connections that can be allocated by the pool at a given time. Use a negative value for no limit. spring.redis.lettuce.pool.max-idle=8 # Max number of “idle” connections in the pool. Use a negative value to indicate an unlimited number of idle connections. spring.redis.lettuce.pool.max-wait=-1 # Maximum amount of time (in milliseconds) a connection allocation should block before throwing an exception when the pool is exhausted. Use a negative value to block indefinitely. spring.redis.lettuce.pool.min-idle=0 # Target for the minimum number of idle connections to maintain in the pool. This setting only has an effect if it is positive. spring.redis.lettuce.shutdown-timeout=2000 # Shutdown timeout in milliseconds. spring.redis.password= # Login password of the redis server. spring.redis.port=6379 # Redis server port. spring.redis.sentinel.master= # Name of Redis server. spring.redis.sentinel.nodes= # Comma-separated list of host:port pairs. spring.redis.ssl=false # Enable SSL support. spring.redis.timeout=0 # Connection timeout in milliseconds. TRANSACTION (TransactionProperties) spring.transaction.default-timeout= # Default transaction timeout in seconds. spring.transaction.rollback-on-commit-failure= # Perform the rollback on commit failures. ACTIVEMQ (ActiveMQProperties) spring.activemq.broker-url= # URL of the ActiveMQ broker. Auto-generated by default. For instance tcp://localhost:61616 spring.activemq.in-memory=true # Specify if the default broker URL should be in memory. Ignored if an explicit broker has been specified. spring.activemq.password= # Login password of the broker. spring.activemq.user= # Login user of the broker. spring.activemq.packages.trust-all=false # Trust all packages. spring.activemq.packages.trusted= # Comma-separated list of specific packages to trust (when not trusting all packages). spring.activemq.pool.configuration.*= # See PooledConnectionFactory. spring.activemq.pool.enabled=false # Whether a PooledConnectionFactory should be created instead of a regular ConnectionFactory. spring.activemq.pool.expiry-timeout=0 # Connection expiration timeout in milliseconds. spring.activemq.pool.idle-timeout=30000 # Connection idle timeout in milliseconds. spring.activemq.pool.max-connections=1 # Maximum number of pooled connections. ARTEMIS (ArtemisProperties) spring.artemis.embedded.cluster-password= # Cluster password. Randomly generated on startup by default. spring.artemis.embedded.data-directory= # Journal file directory. Not necessary if persistence is turned off. spring.artemis.embedded.enabled=true # Enable embedded mode if the Artemis server APIs are available. spring.artemis.embedded.persistent=false # Enable persistent store. spring.artemis.embedded.queues= # Comma-separated list of queues to create on startup. spring.artemis.embedded.server-id= # Server id. By default, an auto-incremented counter is used. spring.artemis.embedded.topics= # Comma-separated list of topics to create on startup. spring.artemis.host=localhost # Artemis broker host. spring.artemis.mode= # Artemis deployment mode, auto-detected by default. spring.artemis.password= # Login password of the broker. spring.artemis.port=61616 # Artemis broker port. spring.artemis.user= # Login user of the broker. SPRING BATCH (BatchProperties) spring.batch.initializer.enabled= # Create the required batch tables on startup if necessary. Enabled automatically if no custom table prefix is set or if a custom schema is configured. spring.batch.job.enabled=true # Execute all Spring Batch jobs in the context on startup. spring.batch.job.names= # Comma-separated list of job names to execute on startup (For instance job1,job2). By default, all Jobs found in the context are executed. spring.batch.schema=classpath:org/springframework/batch/core/schema-@@platform@@.sql # Path to the SQL file to use to initialize the database schema. spring.batch.table-prefix= # Table prefix for all the batch meta-data tables. SPRING INTEGRATION (IntegrationProperties) spring.integration.jdbc.initializer.enabled=false # Create the required integration tables on startup. spring.integration.jdbc.schema=classpath:org/springframework/integration/jdbc/schema-@@platform@@.sql # Path to the SQL file to use to initialize the database schema. JMS (JmsProperties) spring.jms.jndi-name= # Connection factory JNDI name. When set, takes precedence to others connection factory auto-configurations. spring.jms.listener.acknowledge-mode= # Acknowledge mode of the container. By default, the listener is transacted with automatic acknowledgment. spring.jms.listener.auto-startup=true # Start the container automatically on startup. spring.jms.listener.concurrency= # Minimum number of concurrent consumers. spring.jms.listener.max-concurrency= # Maximum number of concurrent consumers. spring.jms.pub-sub-domain=false # Specify if the default destination type is topic. spring.jms.template.default-destination= # Default destination to use on send/receive operations that do not have a destination parameter. spring.jms.template.delivery-delay= # Delivery delay to use for send calls in milliseconds. spring.jms.template.delivery-mode= # Delivery mode. Enable QoS when set. spring.jms.template.priority= # Priority of a message when sending. Enable QoS when set. spring.jms.template.qos-enabled= # Enable explicit QoS when sending a message. spring.jms.template.receive-timeout= # Timeout to use for receive calls in milliseconds. spring.jms.template.time-to-live= # Time-to-live of a message when sending in milliseconds. Enable QoS when set. APACHE KAFKA (KafkaProperties) spring.kafka.bootstrap-servers= # Comma-delimited list of host:port pairs to use for establishing the initial connection to the Kafka cluster. spring.kafka.client-id= # Id to pass to the server when making requests; used for server-side logging. spring.kafka.consumer.auto-commit-interval= # Frequency in milliseconds that the consumer offsets are auto-committed to Kafka if ‘enable.auto.commit’ true. spring.kafka.consumer.auto-offset-reset= # What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server. spring.kafka.consumer.bootstrap-servers= # Comma-delimited list of host:port pairs to use for establishing the initial connection to the Kafka cluster. spring.kafka.consumer.client-id= # Id to pass to the server when making requests; used for server-side logging. spring.kafka.consumer.enable-auto-commit= # If true the consumer’s offset will be periodically committed in the background. spring.kafka.consumer.fetch-max-wait= # Maximum amount of time in milliseconds the server will block before answering the fetch request if there isn’t sufficient data to immediately satisfy the requirement given by “fetch.min.bytes”. spring.kafka.consumer.fetch-min-size= # Minimum amount of data the server should return for a fetch request in bytes. spring.kafka.consumer.group-id= # Unique string that identifies the consumer group this consumer belongs to. spring.kafka.consumer.heartbeat-interval= # Expected time in milliseconds between heartbeats to the consumer coordinator. spring.kafka.consumer.key-deserializer= # Deserializer class for keys. spring.kafka.consumer.max-poll-records= # Maximum number of records returned in a single call to poll(). spring.kafka.consumer.ssl.key-password= # Password of the private key in the key store file. spring.kafka.consumer.ssl.keystore-location= # Location of the key store file. spring.kafka.consumer.ssl.keystore-password= # Store password for the key store file. spring.kafka.consumer.ssl.truststore-location= # Location of the trust store file. spring.kafka.consumer.ssl.truststore-password= # Store password for the trust store file. spring.kafka.consumer.value-deserializer= # Deserializer class for values. spring.kafka.jaas.control-flag=required # Control flag for login configuration. spring.kafka.jaas.enabled= # Enable JAAS configuration. spring.kafka.jaas.login-module=com.sun.security.auth.module.Krb5LoginModule # Login module. spring.kafka.jaas.options= # Additional JAAS options. spring.kafka.listener.ack-count= # Number of records between offset commits when ackMode is “COUNT” or “COUNT_TIME”. spring.kafka.listener.ack-mode= # Listener AckMode; see the spring-kafka documentation. spring.kafka.listener.ack-time= # Time in milliseconds between offset commits when ackMode is “TIME” or “COUNT_TIME”. spring.kafka.listener.concurrency= # Number of threads to run in the listener containers. spring.kafka.listener.poll-timeout= # Timeout in milliseconds to use when polling the consumer. spring.kafka.producer.acks= # Number of acknowledgments the producer requires the leader to have received before considering a request complete. spring.kafka.producer.batch-size= # Number of records to batch before sending. spring.kafka.producer.bootstrap-servers= # Comma-delimited list of host:port pairs to use for establishing the initial connection to the Kafka cluster. spring.kafka.producer.buffer-memory= # Total bytes of memory the producer can use to buffer records waiting to be sent to the server. spring.kafka.producer.client-id= # Id to pass to the server when making requests; used for server-side logging. spring.kafka.producer.compression-type= # Compression type for all data generated by the producer. spring.kafka.producer.key-serializer= # Serializer class for keys. spring.kafka.producer.retries= # When greater than zero, enables retrying of failed sends. spring.kafka.producer.ssl.key-password= # Password of the private key in the key store file. spring.kafka.producer.ssl.keystore-location= # Location of the key store file. spring.kafka.producer.ssl.keystore-password= # Store password for the key store file. spring.kafka.producer.ssl.truststore-location= # Location of the trust store file. spring.kafka.producer.ssl.truststore-password= # Store password for the trust store file. spring.kafka.producer.value-serializer= # Serializer class for values. spring.kafka.properties.*= # Additional properties used to configure the client. spring.kafka.ssl.key-password= # Password of the private key in the key store file. spring.kafka.ssl.keystore-location= # Location of the key store file. spring.kafka.ssl.keystore-password= # Store password for the key store file. spring.kafka.ssl.truststore-location= # Location of the trust store file. spring.kafka.ssl.truststore-password= # Store password for the trust store file. spring.kafka.template.default-topic= # Default topic to which messages will be sent. RABBIT (RabbitProperties) spring.rabbitmq.addresses= # Comma-separated list of addresses to which the client should connect. spring.rabbitmq.cache.channel.checkout-timeout= # Number of milliseconds to wait to obtain a channel if the cache size has been reached. spring.rabbitmq.cache.channel.size= # Number of channels to retain in the cache. spring.rabbitmq.cache.connection.mode=channel # Connection factory cache mode. spring.rabbitmq.cache.connection.size= # Number of connections to cache. spring.rabbitmq.connection-timeout= # Connection timeout, in milliseconds; zero for infinite. spring.rabbitmq.dynamic=true # Create an AmqpAdmin bean. spring.rabbitmq.host=localhost # RabbitMQ host. spring.rabbitmq.listener.direct.acknowledge-mode= # Acknowledge mode of container. spring.rabbitmq.listener.direct.auto-startup=true # Start the container automatically on startup. spring.rabbitmq.listener.direct.consumers-per-queue= # Number of consumers per queue. spring.rabbitmq.listener.direct.default-requeue-rejected= # Whether rejected deliveries are requeued by default; default true. spring.rabbitmq.listener.direct.idle-event-interval= # How often idle container events should be published in milliseconds. spring.rabbitmq.listener.direct.prefetch= # Number of messages to be handled in a single request. It should be greater than or equal to the transaction size (if used). spring.rabbitmq.listener.simple.acknowledge-mode= # Acknowledge mode of container. spring.rabbitmq.listener.simple.auto-startup=true # Start the container automatically on startup. spring.rabbitmq.listener.simple.concurrency= # Minimum number of listener invoker threads. spring.rabbitmq.listener.simple.default-requeue-rejected= # Whether or not to requeue delivery failures. spring.rabbitmq.listener.simple.idle-event-interval= # How often idle container events should be published in milliseconds. spring.rabbitmq.listener.simple.max-concurrency= # Maximum number of listener invoker. spring.rabbitmq.listener.simple.prefetch= # Number of messages to be handled in a single request. It should be greater than or equal to the transaction size (if used). spring.rabbitmq.listener.simple.retry.enabled=false # Whether or not publishing retries are enabled. spring.rabbitmq.listener.simple.retry.initial-interval=1000 # Interval between the first and second attempt to deliver a message. spring.rabbitmq.listener.simple.retry.max-attempts=3 # Maximum number of attempts to deliver a message. spring.rabbitmq.listener.simple.retry.max-interval=10000 # Maximum interval between attempts. spring.rabbitmq.listener.simple.retry.multiplier=1.0 # A multiplier to apply to the previous delivery retry interval. spring.rabbitmq.listener.simple.retry.stateless=true # Whether or not retry is stateless or stateful. spring.rabbitmq.listener.simple.transaction-size= # Number of messages to be processed in a transaction; number of messages between acks. For best results it should be less than or equal to the prefetch count. spring.rabbitmq.listener.type=simple # Listener container type. spring.rabbitmq.password= # Login to authenticate against the broker. spring.rabbitmq.port=5672 # RabbitMQ port. spring.rabbitmq.publisher-confirms=false # Enable publisher confirms. spring.rabbitmq.publisher-returns=false # Enable publisher returns. spring.rabbitmq.requested-heartbeat= # Requested heartbeat timeout, in seconds; zero for none. spring.rabbitmq.ssl.enabled=false # Enable SSL support. spring.rabbitmq.ssl.key-store= # Path to the key store that holds the SSL certificate. spring.rabbitmq.ssl.key-store-password= # Password used to access the key store. spring.rabbitmq.ssl.trust-store= # Trust store that holds SSL certificates. spring.rabbitmq.ssl.trust-store-password= # Password used to access the trust store. spring.rabbitmq.ssl.algorithm= # SSL algorithm to use. By default configure by the rabbit client library. spring.rabbitmq.template.mandatory=false # Enable mandatory messages. spring.rabbitmq.template.receive-timeout=0 # Timeout for receive() methods. spring.rabbitmq.template.reply-timeout=5000 # Timeout for sendAndReceive() methods. spring.rabbitmq.template.retry.enabled=false # Set to true to enable retries in the RabbitTemplate. spring.rabbitmq.template.retry.initial-interval=1000 # Interval between the first and second attempt to publish a message. spring.rabbitmq.template.retry.max-attempts=3 # Maximum number of attempts to publish a message. spring.rabbitmq.template.retry.max-interval=10000 # Maximum number of attempts to publish a message. spring.rabbitmq.template.retry.multiplier=1.0 # A multiplier to apply to the previous publishing retry interval. spring.rabbitmq.username= # Login user to authenticate to the broker. spring.rabbitmq.virtual-host= # Virtual host to use when connecting to the broker. ENDPOINTS (AbstractEndpoint subclasses) endpoints.enabled=true # Enable endpoints. endpoints.sensitive= # Default endpoint sensitive setting. endpoints.actuator.enabled=true # Enable the endpoint. endpoints.actuator.path= # Endpoint URL path. endpoints.actuator.sensitive=false # Enable security on the endpoint. endpoints.auditevents.enabled= # Enable the endpoint. endpoints.auditevents.path= # Endpoint path. endpoints.auditevents.sensitive=false # Enable security on the endpoint. endpoints.autoconfig.enabled= # Enable the endpoint. endpoints.autoconfig.id= # Endpoint identifier. endpoints.autoconfig.path= # Endpoint path. endpoints.autoconfig.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.beans.enabled= # Enable the endpoint. endpoints.beans.id= # Endpoint identifier. endpoints.beans.path= # Endpoint path. endpoints.beans.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.configprops.enabled= # Enable the endpoint. endpoints.configprops.id= # Endpoint identifier. endpoints.configprops.keys-to-sanitize=password,secret,key,token,.credentials.,vcap_services # Keys that should be sanitized. Keys can be simple strings that the property ends with or regex expressions. endpoints.configprops.path= # Endpoint path. endpoints.configprops.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.docs.curies.enabled=false # Enable the curie generation. endpoints.docs.enabled=true # Enable actuator docs endpoint. endpoints.docs.path=/docs # endpoints.docs.sensitive=false # endpoints.dump.enabled= # Enable the endpoint. endpoints.dump.id= # Endpoint identifier. endpoints.dump.path= # Endpoint path. endpoints.dump.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.env.enabled= # Enable the endpoint. endpoints.env.id= # Endpoint identifier. endpoints.env.keys-to-sanitize=password,secret,key,token,.credentials.,vcap_services # Keys that should be sanitized. Keys can be simple strings that the property ends with or regex expressions. endpoints.env.path= # Endpoint path. endpoints.env.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.flyway.enabled= # Enable the endpoint. endpoints.flyway.id= # Endpoint identifier. endpoints.flyway.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.health.enabled= # Enable the endpoint. endpoints.health.id= # Endpoint identifier. endpoints.health.mapping.*= # Mapping of health statuses to HttpStatus codes. By default, registered health statuses map to sensible defaults (i.e. UP maps to 200). endpoints.health.path= # Endpoint path. endpoints.health.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.health.time-to-live=1000 # Time to live for cached result, in milliseconds. endpoints.heapdump.enabled= # Enable the endpoint. endpoints.heapdump.path= # Endpoint path. endpoints.heapdump.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.hypermedia.enabled=false # Enable hypermedia support for endpoints. endpoints.info.enabled= # Enable the endpoint. endpoints.info.id= # Endpoint identifier. endpoints.info.path= # Endpoint path. endpoints.info.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.jolokia.enabled=true # Enable Jolokia endpoint. endpoints.jolokia.path=/jolokia # Endpoint URL path. endpoints.jolokia.sensitive=true # Enable security on the endpoint. endpoints.liquibase.enabled= # Enable the endpoint. endpoints.liquibase.id= # Endpoint identifier. endpoints.liquibase.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.logfile.enabled=true # Enable the endpoint. endpoints.logfile.external-file= # External Logfile to be accessed. endpoints.logfile.path=/logfile # Endpoint URL path. endpoints.logfile.sensitive=true # Enable security on the endpoint. endpoints.loggers.enabled=true # Enable the endpoint. endpoints.loggers.id= # Endpoint identifier. endpoints.loggers.path=/logfile # Endpoint path. endpoints.loggers.sensitive=true # Mark if the endpoint exposes sensitive information. endpoints.mappings.enabled= # Enable the endpoint. endpoints.mappings.id= # Endpoint identifier. endpoints.mappings.path= # Endpoint path. endpoints.mappings.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.metrics.enabled= # Enable the endpoint. endpoints.metrics.filter.enabled=true # Enable the metrics servlet filter. endpoints.metrics.filter.gauge-submissions=merged # Http filter gauge submissions (merged, per-http-method) endpoints.metrics.filter.counter-submissions=merged # Http filter counter submissions (merged, per-http-method) endpoints.metrics.id= # Endpoint identifier. endpoints.metrics.path= # Endpoint path. endpoints.metrics.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.shutdown.enabled= # Enable the endpoint. endpoints.shutdown.id= # Endpoint identifier. endpoints.shutdown.path= # Endpoint path. endpoints.shutdown.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.trace.enabled= # Enable the endpoint. endpoints.trace.filter.enabled=true # Enable the trace servlet filter. endpoints.trace.id= # Endpoint identifier. endpoints.trace.path= # Endpoint path. endpoints.trace.sensitive= # Mark if the endpoint exposes sensitive information. ENDPOINTS CORS CONFIGURATION (EndpointCorsProperties) endpoints.cors.allow-credentials= # Set whether credentials are supported. When not set, credentials are not supported. endpoints.cors.allowed-headers= # Comma-separated list of headers to allow in a request. ‘’ allows all headers. endpoints.cors.allowed-methods=GET # Comma-separated list of methods to allow. '’ allows all methods. endpoints.cors.allowed-origins= # Comma-separated list of origins to allow. ‘*’ allows all origins. When not set, CORS support is disabled. endpoints.cors.exposed-headers= # Comma-separated list of headers to include in a response. endpoints.cors.max-age=1800 # How long, in seconds, the response from a pre-flight request can be cached by clients. JMX ENDPOINT (EndpointMBeanExportProperties) endpoints.jmx.domain= # JMX domain name. Initialized with the value of ‘spring.jmx.default-domain’ if set. endpoints.jmx.enabled=true # Enable JMX export of all endpoints. endpoints.jmx.static-names= # Additional static properties to append to all ObjectNames of MBeans representing Endpoints. endpoints.jmx.unique-names=false # Ensure that ObjectNames are modified in case of conflict. JOLOKIA (JolokiaProperties) jolokia.config.*= # See Jolokia manual MANAGEMENT HTTP SERVER (ManagementServerProperties) management.add-application-context-header=false # Add the “X-Application-Context” HTTP header in each response. management.address= # Network address that the management endpoints should bind to. management.context-path= # Management endpoint context-path. For instance /actuator management.cloudfoundry.enabled= # Enable extended Cloud Foundry actuator endpoints management.cloudfoundry.skip-ssl-validation= # Skip SSL verification for Cloud Foundry actuator endpoint security calls management.port= # Management endpoint HTTP port. Uses the same port as the application by default. Configure a different port to use management-specific SSL. management.security.enabled=true # Enable security. management.security.roles=ACTUATOR # Comma-separated list of roles that can access the management endpoint. management.security.sessions=stateless # Session creating policy to use (always, never, if_required, stateless). management.ssl.ciphers= # Supported SSL ciphers. Requires a custom management.port. management.ssl.client-auth= # Whether client authentication is wanted (“want”) or needed (“need”). Requires a trust store. Requires a custom management.port. management.ssl.enabled= # Enable SSL support. Requires a custom management.port. management.ssl.enabled-protocols= # Enabled SSL protocols. Requires a custom management.port. management.ssl.key-alias= # Alias that identifies the key in the key store. Requires a custom management.port. management.ssl.key-password= # Password used to access the key in the key store. Requires a custom management.port. management.ssl.key-store= # Path to the key store that holds the SSL certificate (typically a jks file). Requires a custom management.port. management.ssl.key-store-password= # Password used to access the key store. Requires a custom management.port. management.ssl.key-store-provider= # Provider for the key store. Requires a custom management.port. management.ssl.key-store-type= # Type of the key store. Requires a custom management.port. management.ssl.protocol=TLS # SSL protocol to use. Requires a custom management.port. management.ssl.trust-store= # Trust store that holds SSL certificates. Requires a custom management.port. management.ssl.trust-store-password= # Password used to access the trust store. Requires a custom management.port. management.ssl.trust-store-provider= # Provider for the trust store. Requires a custom management.port. management.ssl.trust-store-type= # Type of the trust store. Requires a custom management.port. HEALTH INDICATORS management.health.db.enabled=true # Enable database health check. management.health.cassandra.enabled=true # Enable cassandra health check. management.health.couchbase.enabled=true # Enable couchbase health check. management.health.defaults.enabled=true # Enable default health indicators. management.health.diskspace.enabled=true # Enable disk space health check. management.health.diskspace.path= # Path used to compute the available disk space. management.health.diskspace.threshold=0 # Minimum disk space that should be available, in bytes. management.health.elasticsearch.enabled=true # Enable elasticsearch health check. management.health.elasticsearch.indices= # Comma-separated index names. management.health.elasticsearch.response-timeout=100 # The time, in milliseconds, to wait for a response from the cluster. management.health.jms.enabled=true # Enable JMS health check. management.health.ldap.enabled=true # Enable LDAP health check. management.health.mail.enabled=true # Enable Mail health check. management.health.mongo.enabled=true # Enable MongoDB health check. management.health.rabbit.enabled=true # Enable RabbitMQ health check. management.health.redis.enabled=true # Enable Redis health check. management.health.solr.enabled=true # Enable Solr health check. management.health.status.order=DOWN, OUT_OF_SERVICE, UP, UNKNOWN # Comma-separated list of health statuses in order of severity. INFO CONTRIBUTORS (InfoContributorProperties) management.info.build.enabled=true # Enable build info. management.info.defaults.enabled=true # Enable default info contributors. management.info.env.enabled=true # Enable environment info. management.info.git.enabled=true # Enable git info. management.info.git.mode=simple # Mode to use to expose git information. TRACING (TraceProperties) management.trace.include=request-headers,response-headers,cookies,errors # Items to be included in the trace. METRICS EXPORT (MetricExportProperties) spring.metrics.export.aggregate.key-pattern= # Pattern that tells the aggregator what to do with the keys from the source repository. spring.metrics.export.aggregate.prefix= # Prefix for global repository if active. spring.metrics.export.delay-millis=5000 # Delay in milliseconds between export ticks. Metrics are exported to external sources on a schedule with this delay. spring.metrics.export.enabled=true # Flag to enable metric export (assuming a MetricWriter is available). spring.metrics.export.excludes= # List of patterns for metric names to exclude. Applied after the includes. spring.metrics.export.includes= # List of patterns for metric names to include. spring.metrics.export.redis.key=keys.spring.metrics # Key for redis repository export (if active). spring.metrics.export.redis.prefix=spring.metrics # Prefix for redis repository if active. spring.metrics.export.send-latest= # Flag to switch off any available optimizations based on not exporting unchanged metric values. spring.metrics.export.statsd.host= # Host of a statsd server to receive exported metrics. spring.metrics.export.statsd.port=8125 # Port of a statsd server to receive exported metrics. spring.metrics.export.statsd.prefix= # Prefix for statsd exported metrics. spring.metrics.export.triggers.*= # Specific trigger properties per MetricWriter bean name. DEVTOOLS (DevToolsProperties) spring.devtools.livereload.enabled=true # Enable a livereload.com compatible server. spring.devtools.livereload.port=35729 # Server port. spring.devtools.restart.additional-exclude= # Additional patterns that should be excluded from triggering a full restart. spring.devtools.restart.additional-paths= # Additional paths to watch for changes. spring.devtools.restart.enabled=true # Enable automatic restart. spring.devtools.restart.exclude=META-INF/maven/,META-INF/resources/,resources/,static/,public/,templates/,/*Test.class,/*Tests.class,git.properties # Patterns that should be excluded from triggering a full restart. spring.devtools.restart.poll-interval=1000 # Amount of time (in milliseconds) to wait between polling for classpath changes. spring.devtools.restart.quiet-period=400 # Amount of quiet time (in milliseconds) required without any classpath changes before a restart is triggered. spring.devtools.restart.trigger-file= # Name of a specific file that when changed will trigger the restart check. If not specified any classpath file change will trigger the restart. REMOTE DEVTOOLS (RemoteDevToolsProperties) spring.devtools.remote.context-path=/.~~spring-boot!~ # Context path used to handle the remote connection. spring.devtools.remote.debug.enabled=true # Enable remote debug support. spring.devtools.remote.debug.local-port=8000 # Local remote debug server port. spring.devtools.remote.proxy.host= # The host of the proxy to use to connect to the remote application. spring.devtools.remote.proxy.port= # The port of the proxy to use to connect to the remote application. spring.devtools.remote.restart.enabled=true # Enable remote restart. spring.devtools.remote.secret= # A shared secret required to establish a connection (required to enable remote support). spring.devtools.remote.secret-header-name=X-AUTH-TOKEN # HTTP header used to transfer the shared secret. TEST spring.test.database.replace=any # Type of existing DataSource to replace. spring.test.mockmvc.print=default # MVC Print option.","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://luxiaowan.github.io/categories/SpringBoot/"}],"tags":[]},{"title":"ThreadLocal是怎么实现线程隔离的","slug":"ThreadLocal是怎么实现线程隔离的","date":"2019-11-20T15:21:56.000Z","updated":"2019-11-25T17:08:11.991Z","comments":true,"path":"2019/11/20/ThreadLocal是怎么实现线程隔离的/","link":"","permalink":"http://luxiaowan.github.io/2019/11/20/ThreadLocal是怎么实现线程隔离的/","excerpt":"","text":"ThreadLocal大家应该都不陌生，见过最多的使用场景应该是和SimpleDateFormat一起使用吧，因为这个SDF非线程安全的，所以需要使用ThreadLocal将它在线程之间隔离开，避免造成脏数据的🐞。那么ThreadLocal是怎么保证线程安全，又是如何操作的呢？ 案例 123456789101112131415161718192021public static void main(String[] args) &#123; ThreadLocal&lt;Integer&gt; threadLocal = new ThreadLocal&lt;&gt;(); new Thread(new Runnable() &#123; @Override public void run() &#123; threadLocal.set(1); threadLocal.set(2); System.out.println(\"cc1: \" + threadLocal.get()); &#125; &#125;, \"cc1\").start(); new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(\"cc2: \" + threadLocal.get()); &#125; &#125;, \"cc2\").start();&#125; 输出: 12cc1: 2cc2: null 哦哟~cc2打印出来null，也就是在cc1线程中设置的值在线程cc2中获取不到，这也就是所谓的线程隔离，我们来看下ThreadLocal具体的代码实现吧： ThreadLocal的set(T t)方法源码 123456789101112public void set(T value) &#123; // 获取当前线程 Thread t = Thread.currentThread(); // 获取当前线程的threadLocals属性，这个属性在Thread类中定义的，为Thread的实例变量 ThreadLocalMap map = getMap(t); // 若线程的ThreadLocalMap已经存在，则调用ThreadLocalMap的set(ThreadLocal&lt;T&gt; key, Object value)方法 // 否则创建新的ThreadLocalMap实例，并set对应的value if (map != null) map.set(this, value); else createMap(t, value);&#125; ThreadLocalMap的set(ThreadLocal key, Object value)方法源码 12345678910111213141516171819202122232425262728private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; Entry[] tab = table; int len = tab.length; // 简单计算key所在的位置 int i = key.threadLocalHashCode &amp; (len-1); // 从key所在位置开始遍历table数组，找到具体key所在的位置 for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; // 获取Entry实例的key值，这里调用的是超类java.lang.ref.Reference中的get(T t)方法 ThreadLocal&lt;?&gt; k = e.get(); // 若k与传入的参数key是同一个，则用参数value替换Entry实例的value，然后结束方法 if (k == key) &#123; e.value = value; return; &#125; // 若获取的k为null，则表示这个变量已经被删除了，则去清理一下table数组，并对数组中元素进行清理并设置新的Entry实例 if (k == null) &#123; replaceStaleEntry(key, value, i); return; &#125; &#125; // 代码走到这一步，说明该线程第一次设置数据，创建新的Entry实例放在table的第i个位置上 tab[i] = new Entry(key, value); int sz = ++size; // 清理table中的元素，若长度达到了扩容阈值，则对table进行扩容，扩容为原数组长度的2倍 if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash();&#125; ThreadLocal的createMap(Thread t, T firstValue)方法源码 12345void createMap(Thread t, T firstValue) &#123; // 创建一个ThreadLocalMap实例，并赋值给当前线程的实例变量threadLocals // 这里就是线程隔离的关键所在，每一个线程中的数据都是由线程独有的threadLocals变量存储的 t.threadLocals = new ThreadLocalMap(this, firstValue);&#125; ThreadLocalMap的构造器源码 123456789101112ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123; // 实例化Entry数组，长度为初始长度16 table = new Entry[INITIAL_CAPACITY]; // 计算key在数组中的位置 int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); // 创建Entry实例，并放在table的i下标位置 table[i] = new Entry(firstKey, firstValue); // 实际长度设置为1 size = 1; // 设置数组扩容阈值（len * 2 / 3） setThreshold(INITIAL_CAPACITY);&#125; 以上便是ThreadLocal达到线程隔离的基本解析，讲解的比较基础，其实就是JDK源码鉴赏，还有什么不懂的地方就自己去看源码吧。 延伸下 ThreadLocal的get()方法源码 12345678910111213public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings(\"unchecked\") T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125; 这段代码比较简单，这里就不在进行解释了，我们着重看一下最后一句setInitialValue()这个方法 1234567891011121314private T setInitialValue() &#123; T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value;&#125;protected T initialValue() &#123; return null;&#125; 会发现和set方法类似，只不过是将一个null当做value而已，所以我们在没给ThreadLocal设置值的情况下调用get方法，则会为其创建一个默认的null值并返回null。 留一个思考题 因为我们每个线程的ThreadLocal的key的hash值都是固定的，那么Thread的threadLocals变量的table中会有多少个非null元素呢？","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[]},{"title":"慎用ArrayList中的SubList","slug":"慎用ArrayList中的SubList","date":"2019-11-10T17:11:00.000Z","updated":"2019-12-19T16:51:55.997Z","comments":true,"path":"2019/11/11/慎用ArrayList中的SubList/","link":"","permalink":"http://luxiaowan.github.io/2019/11/11/慎用ArrayList中的SubList/","excerpt":"","text":"双十一了，大家都省了多少钱啊？ 题外话：此处交给大家一个查看商品历史价格的小方法： 在商品链接的域名后加上三个v就能查看到该商品的历史价格啦 🌰 http://shop.taobao.com/xxxx ↓ http://shop.taobaovvv.com/xxx 步入正题，为什么说我们在实际开发过程中要慎用ArrayList的subList呢？其实这也是阿里军规中的一条，原因其实很简单：不稳定！也许看到这里会觉得&quot;就是创建一个独立的新的SubList的实例，怎么会不稳定！&quot;，如果你是这么想的，那么恭喜你，这篇文章真的能够帮助到你，且往下看： 1. 看看SubList的set方法： 1234567891011121314151617public static void main(String[] args) &#123; List&lt;String&gt; sourceList = new ArrayList&lt;String&gt;() &#123; &#123; add(\"H\"); add(\"E\"); add(\"L\"); add(\"L\"); add(\"O\"); add(\"W\"); add(\"O\"); add(\"R\"); add(\"L\"); add(\"D\"); &#125; &#125;; List&lt;String&gt; subList = sourceList.subList(2, 5); System.out.println(\"sourceList: \" + sourceList); System.out.println(\"sourceList.subList(2, 5)得到: \" + subList); subList.set(1, \"cc\"); System.out.println(\"sourceList: \" + sourceList); System.out.println(\"subList: \" + subList); &#125;&#125; 上面代码的执行结果是什么？先不要看下面的答案，自己想一想。 答案 1234sourceList: [H, E, L, L, O, W, O, R, L, D]subList: [L, L, O]sourceList: [H, E, L, cc, O, W, O, R, L, D]subList: [L, cc, O] 哦吼~！答案和你自己想的有没有出入？奇妙吧，为什么修改了subList中的元素，会影响到sourceList？我们来看下ArrayList的subList方法都做了些什么： JDK源码 12345678/** * Returns a view of the portion of this list between the specified * &#123;@code fromIndex&#125;, inclusive, and &#123;@code toIndex&#125;, exclusive. */public List&lt;E&gt; subList(int fromIndex, int toIndex) &#123; subListRangeCheck(fromIndex, toIndex, size); return new SubList(this, 0, fromIndex, toIndex);&#125; 首先是检查我们的fromIndex和toIndex是否合法，然后调用ArrayList的内部类SubList创建一个SubList的实例。好像还真如我们之前想的一样，创建了一个独立的SubList的对象，没什么不对的，那我们来看一下SubList的构造器中都做了些什么吧。 1234567SubList(AbstractList&lt;E&gt; parent, int offset, int fromIndex, int toIndex) &#123; this.parent = parent; this.parentOffset = fromIndex; this.offset = offset + fromIndex; this.size = toIndex - fromIndex; this.modCount = ArrayList.this.modCount;&#125; 这是个什么鬼？ArrayList的实例对象(也就是parent)竟然作为参数传到了SubList中，SubList的偏移量为0+fromIndex，大小size为toIndex - fromIndex（也就是和String的substring方法一样，fromIndex到(toIndex -1)的数据集），修改次数modCount和ArrayList的modCount相等，那么我们猜测一下：SubList实例的变动，是否和ArrayList有关呢？ 我们看到subList方法的注释中有这么一句话：Returns a view of the portion of this list。难道SubList仅仅是ArrayList的一个被fromIndex和toIndex的区间视图？ 上面的例子中，subList调用了它的set方法，我们来看一下这个set方法内部逻辑是什么： 12345678910public E set(int index, E e) &#123; rangeCheck(index);// 下标校验 checkForComodification();// 校验合法性 // ***重点 // 根据偏移量和下标，获取ArrayList对象的elementData数组中下标为(offset + index)的元素 // offset是什么？从构造器中我们可以看到offset就是0 + fromIndex，也就是我们截取的起始下标，也就是SubList的set方法是直接在原ArrayList实例的内部数组上进行的操作 E oldValue = ArrayList.this.elementData(offset + index); ArrayList.this.elementData[offset + index] = e; return oldValue;&#125; 看到这里就一目了然了，怪不得我们修改了SubList的元素会影响到创建它的对象的值。所以在使用SubList的时候，如果需要修改SubList里面的值，一定要注意一下是否会影响到原List中的数据所涉及的业务，否则这个坑一旦踩上了，不太容易排查啊。 2. 再看看SubList的add方法 1234567891011121314151617public static void main(String[] args) &#123; List&lt;String&gt; sourceList = new ArrayList&lt;String&gt;() &#123; &#123; add(\"H\"); add(\"E\"); add(\"L\"); add(\"L\"); add(\"O\"); add(\"W\"); add(\"O\"); add(\"R\"); add(\"L\"); add(\"D\"); &#125; &#125;; List&lt;String&gt; subList = sourceList.subList(2, 5); System.out.println(\"sourceList: \" + sourceList); System.out.println(\"sourceList.subList(2, 5)得到: \" + subList); subList.add(\"cc\"); System.out.println(\"sourceList: \" + sourceList); System.out.println(\"subList: \" + subList); &#125;&#125; 上面代码的执行结果又是什么呢？如果我们稍微思考一下，大致能正确的分析出结果： 答案 1234sourceList: [H, E, L, L, O, W, O, R, L, D]subList: [L, L, O]sourceList: [H, E, L, L, O, cc, W, O, R, L, D]subList: [L, L, O, cc] 我们向subList中添加一个元素，原列表sourceList在toIndex的位置插入了subList中add的元素，也就是我们在SubList中新增一个元素，同时会将这个元素添加到原List中。 JDK源码 我们查看SubList的源码，发现并没有add(E e)方法，那我们调用的add(“cc”)是调用到哪里去了呢？我们查看SubList类的声明，可以看到它是继承了AbstractList抽象类，所以这里应该是调用了超类里的add(E e)方法， 12345/** AbstractList.java */public boolean add(E e) &#123; add(size(), e); return true;&#125; 这里可以看到是调用了add(int index, E element)方法进行数据新增的，然而SubList里面实现了这个方法，那么我们来看下SubList中的这个方法实现： 123456789101112public void add(int index, E e) &#123; // 校验下标是否越界 rangeCheckForAdd(index); // 校验原List是否有过修改 checkForComodification(); // parent即是在构造器中注入的原List parent.add(parentOffset + index, e); // 同步列表修改次数 this.modCount = parent.modCount; // 本列表的长度+1 this.size++;&#125; 由SubList的源码可以看出，SubList实例的add方法实际上就是在修改原List，包括SubList中所有的方法均是在parent列表上进行操作。 3. 奇葩操作，最坑的坑 仔细分析如下代码： 12345678910111213141516public static void main(String[] args) &#123; List&lt;String&gt; sourceList = new ArrayList&lt;String&gt;() &#123; &#123; add(\"H\"); add(\"E\"); add(\"L\"); add(\"L\"); add(\"O\"); add(\"W\"); add(\"O\"); add(\"R\"); add(\"L\"); add(\"D\"); &#125; &#125;; List&lt;String&gt; subList = sourceList.subList(2, 5); System.out.println(\"sourceList: \" + sourceList); System.out.println(\"subList: \" + subList); sourceList.add(\"cc\"); System.out.println(\"sourceList: \" + sourceList); System.out.println(\"subList: \" + subList);&#125; 这段代码的执行结果是什么？在不执行这段代码的情况下，是不是以为是下面的结果？ 1234sourceList: [H, E, L, L, O, W, O, R, L, D]subList: [L, L, O]sourceList: [H, E, L, L, O, W, O, R, L, D, cc]subList: [L, L, O] 如果你说对，就是这个，那你可就说错咯，实际上在执行到System.out.println(&quot;sourceList: &quot; + sourceList);这一句代码的时候整个程序的输出都是正常的，但在执行最后一句代码的时候，就会报错了，错误信息是： 123456789Exception in thread \"main\" java.util.ConcurrentModificationException at java.util.ArrayList$SubList.checkForComodification(ArrayList.java:1239) at java.util.ArrayList$SubList.listIterator(ArrayList.java:1099) at java.util.AbstractList.listIterator(AbstractList.java:299) at java.util.ArrayList$SubList.iterator(ArrayList.java:1095) at java.util.AbstractCollection.toString(AbstractCollection.java:454) at java.lang.String.valueOf(String.java:2994) at java.lang.StringBuilder.append(StringBuilder.java:131) at cc.kevinlu.sublist.SubListTest.main(SubListTest.java:31) 哦吼~！竟然报错了，我们可以看到是在ArrayList$SubList.checkForComodificatio方法中报的错，我们来看一下这个方法： 12345private void checkForComodification() &#123; // 比较原列表修改次数和SubList的修改次数是否相等 if (ArrayList.this.modCount != this.modCount) throw new ConcurrentModificationException();&#125; 这里抛出异常，说明这两个数是不相等的，那为什么会不相等呢？我们看SubList的add方法中有同步主、'子’列表的语句this.modCount = parent.modCount;，也就是说我们在修改subList的时候，会同步更新主列表的modCount，以保证主、'子’列表始终是一致的。 但是我们在修改主List的时候是不会去同步SubList的modCount的，我们输出SubList的实例实际上就是调用iterator方法，最终是调用了SubList的public ListIterator&lt;E&gt; listIterator(final int index)方法，该方法第一句就是调用checkForComodification方法检查modCount，这里自然就会报错咯！ 4. 填坑 既然有坑，就有填坑的办法，不可能一直把坑放在那，是吧。 如果既想修改subList，又不想影响到原list。那么可以创建一个机遇subList的拷贝: 123451.创建新的List： subList = Lists.newArrayList(subList);2.lambda表达式： sourceList.stream().skip(fromIndex).limit(size).collect(Collectors.toList()); 5. 总结 并不是说使用SubList一定不妥，文章开头我们也说的是慎用，所以，根据具体业务进行选择吧。","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[]},{"title":"Redis基本命令使用::zset篇","slug":"Redis基本命令使用—zset篇","date":"2019-11-08T04:55:00.000Z","updated":"2019-11-21T16:05:02.245Z","comments":true,"path":"2019/11/08/Redis基本命令使用—zset篇/","link":"","permalink":"http://luxiaowan.github.io/2019/11/08/Redis基本命令使用—zset篇/","excerpt":"","text":"Redis有6大基本类型，分别是字符串(String)、列表(List)、集合(set)、哈希结构(hash)、有序集合(zset)和基数(HyperLogLog) redis在线体验地址：http://try.redis.io/ 有序集合(ZSet) 介绍: Redis的有序集合和集合一样是一个简单的字符串集合，但该集合是有序的，且集合内的元素都是唯一的，也就是集合内不会出现重复元素，与集合唯一不同的是，有序集合中每一个元素都有一个double类型的score属性，Redis通过score的大小对元素进行排序的。有序集合中元素不能重复，但是元素的score值可以重复。 常用于做排行榜功能。 Redis中的集合是通过哈希表来实现的，所以获取集合中元素的时间复杂度为O(1)。 创建缓存 创建一个集合缓存，为集合新增元素 命令格式：zadd key score member [score member ...] 🌰 12345678zadd zk1 1 c 1 cc 1 ccc 2 cccc---创建有序集合zk1，元素为：key: c, score: 1key: cc, score: 1key: ccc, score: 1key: cccc, score: 2 创建一个元素或修改元素的score值（一次只能修改一个元素） 命令格式：zincrby key increment member 🌰 123456zincrby zk1 1 cc---1. 为集合zk1的元素cc的score进行+1操作2. 若集合zk1不存在，则创建3. 若元素cc不存在，则创建，且cc的score为1 查询缓存 查询缓存中元素个数 命令格式：zcard key 🌰 1234zcard zk1---查询集合zk1内的元素总个数 查询集合指定范围的元素 命令格式：zrange key start stop [withscores] 🌰 12345678910111213141. 返回元素名称zrange zk1 0 1---返回集合zk1中从下标0到下标1的元素名称，start和stop都是从0开始2. 返回元素和元素scorezrange zk1 0 -1 withscores---返回集合zk1中全部元素的名称和分数member1score1member2score2★常用于查询排行榜及分数 查询集合中某元素的下标（下标从0开始） 命令格式：zrank key member 🌰 1234zrank zk1 c---返回元素c在集合zk1中的下标 查询集合中某元素的分数 命令格式：zscore key member 🌰 12345zscore zk1 c---查询集合zk1中元素c的分数★常用于点赞数类别查询等 查询集合中指定范围的元素，按照score从大到小排序 命令格式：zrevrange key start stop [withscores] 🌰 1234zrevrange zk1 0 3 withscore---返回集合zk1中从1~4位元素，按照score从大到小 查询集合中某元素的排名 命令格式：zrevrank key member 🌰 12345zrevrank zk1 c---返回元素c在集合zk1中的排名★常用于名次查询 查询指定分数范围内的元素，可分页 命令格式：zrevrangebyscore key maxScore minScore [withscores] [limit offset count] 🌰 1234zrevrangebyscore zk1 3 2 withscores limit 0 1---分页返回集合zk1中分数从2~3的元素 查询指定成员区间内的成员 命令格式：zrangebylex key minChar maxChar [limit offset count] 🌰 1234zrangebylex zk1 - (c1 limit 0 12---返回从第一个元素到元素c1之间的位置 指令 是否必须 说明 ZRANGEBYLEX 是 指令 key 是 有序集合键名称 minChar 是 字典中排序位置较小的成员,必须以”[“(包含)开头,或者以”(“(不包含)开头,可使用”-“代替，&quot;-&quot;表示取最小值 maxChar 是 字典中排序位置较大的成员,必须以”[“(包含)开头,或者以”(“(不包含)开头,可使用”+”代替，&quot;+&quot;表示取最大值 limit 否 返回结果是否分页,指令中包含LIMIT后offset、count必须输入 offset 否 返回结果起始位置 count 否 返回结果数量 查询指定分数区间内的元素数 命令格式：zcount key min max 🌰 12345zcount zk1 1 2---返回score值为1~2的所有元素总数★计算排行榜中某一分数区间的数量 查询指定元素区间内的元素总数 命令格式：zlexcount key min max 🌰 1234zlexcount zk1 (c [cccc---查询c~cccc之间的元素总数，不包括c，但包括cccc 移除缓存元素 移除集合中指定的元素 命令格式：zrem key member [member ...] 🌰 1234zrem zk1 cc c1---移除集合zk1中的元素cc、c1 移除指定元素区间的所有成员 命令格式：zremrangebylex key min max 🌰 1234zremrangebylex zk1 [c (ccc---移除集合zk1中元素c到元素ccc之间的所有成员，包括c，但不包括ccc 移除指定排名区间所有成员 命令格式：zremrangebyrank key start stop 🌰 1234zremrangebyrank zk1 0 1---移除集合zk1中0~1下标的所有元素 移除指定分数区间所有成员 命令格式：zremrangebyscore key min max 🌰 1234zremrangebyscore zk1 0 1---移除集合zk1中score值为0~1的所有元素 特殊操作 计算多个集合的并集，并存入新的集合 命令格式：zunionstore destinationKey numkeys key[key ...] 🌰 1234zunionstore zku 2 zk1 zk2---合并集合zk1和zk2，将并集存入zku，集合zku中元素的score为所有参与计算的集合中相同的元素的score之和","categories":[{"name":"Redis","slug":"Redis","permalink":"http://luxiaowan.github.io/categories/Redis/"}],"tags":[]},{"title":"Java命令::jstat","slug":"Java命令—jstat","date":"2019-11-08T04:00:00.000Z","updated":"2019-11-10T15:57:22.417Z","comments":true,"path":"2019/11/08/Java命令—jstat/","link":"","permalink":"http://luxiaowan.github.io/2019/11/08/Java命令—jstat/","excerpt":"","text":"jstat是用于监控虚拟机运行状态信息的命令，可以显示虚拟机进程中的类装载、内存使用、GC情况、JIT编译等运行状态数据，能够在Linux上快速定位虚拟机性能问题。 jstat命令在jdk的bin目录下，目录中还有很多实用的命令 *以下分析是基于jdk1.8+ jstat命令格式： 123456789jstat -&lt;option&gt; &lt;pid&gt; [&lt;interval&gt; [&lt;count&gt;]]--- option: 需要查看的虚拟机信息 pid: Java程序进程号 本地虚拟机: pid 远程虚拟机: [protocol:][//] lvmid [@hostname[:port]/servername] interval: 监控间隔时间，可选，默认立刻执行一次 count: 监控次数，可选，默认无限次 option 说明 class 查看类装载、卸载数量、总空间及类装载所耗时间 gc 查看Java堆状况，包括Eden、survivor、老年代、永久代的容量 gcutil 类似于gc，主要输出各区域空间使用占比 gccause 同gc，会多输出每次gc的原因 gccapacity 同gc，但输出的主要是Java堆各个区域使用到的最大、最小空间 gcnew 查看新生代的使用情况 gcnewcapacity 同gcnew，输出内容主要关注新生代的最大、最小空间 gcold 查看老年代的使用情况 gcoldcapacity 同gcold，输出内容主要关注老年代的最大、最小空间 gcpermcapacity 输出永久代使用到的最大、最小空间 compiler 输出JIT编译器编译过的方法、耗时等信息 printcompilation 输出已经被JIT编译的方法 🌰 jstat -class : 显示加载class的数量及所占空间等信息 12345678910[root@master0 ~]# jstat -class 19080Loaded Bytes Unloaded Bytes Time 11512 22276.9 268 421.6 18.41 ---Loaded: 装载类的数量Bytes: 装载类所占用的字节数Unloaded: 卸载类的数量Bytes: 卸载类所占用的字节数Time: 装载和卸载类所花费的时间 jstat -gc : 显示gc的信息，查看gc的次数和时间 1234567891011121314151617181920212223，等于YGCT + FGCT[root@master0 ~]# jstat -gc 19080 1000 1 S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT 6656.0 6656.0 0.0 3761.4 94208.0 51469.2 73728.0 50832.0 - - - - 20207 92.896 37 7.833 100.729---[容量为字节]S0C: 年轻代第一个survivor区的总容量（survivor 0 capacity）S1C: 年轻代第二个survivor区的总容量（survivor 1 capacity）S0U: 年轻代第一个survivor区的已使用容量（survivor 0 using）S1U: 年轻代第二个survivor区的已使用容量（survivor 1 using）EC: 年轻代Eden区的总容量（Eden capacity）EU: 年轻代Eden区的已使用容量（Eden using）OC: 老年代的总容量（Old capacity）OU: 老年代已使用的容量（Old using）MC: Metaspace的总容量, jdk1.8+MU: Metaspace已使用的容量, jdk1.8+CCSC:压缩类空间容量, jdk1.8+CCSU:压缩类空间已使用的容量, jdk1.8+YGC: 服务启动至今年轻代gc的次数（young gc）YGCT: 服务启动至今年轻代gc使用的时间，秒（young gc time）FGC: 服务启动至今fullgc的次数FGCT: 服务启动至今fullgc使用的时间，秒GCT: 服务启动至今gc用的总时间，秒，等于YGCT + FGCT jstat -gcutil : 统计gc信息 123456789101112131415161718[root@master0 ~]# jstat -gcutil 19080 1000 3 S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 0.00 48.90 20.83 69.35 - - 20221 93.006 37 7.833 100.838 0.00 48.90 20.83 69.35 - - 20221 93.006 37 7.833 100.838 0.00 48.90 20.83 69.35 - - 20221 93.006 37 7.833 100.838---S0: 年轻代第一个survivor已使用容量比例S1: 年轻代第二个survivor已使用容量比例E: 年轻代Eden区已使用容量比例O: 老年代已使用容量比例M: 元空间已使用容量比例CCS: 压缩类空间已使用容量比例YGC: 服务启动至今年轻代gc次数YGCT: 服务启动至今年轻代gc所占用时间，秒FGC: 服务启动至今fullgc次数FGCT: 服务启动至今fullgc所占用时间，秒GCT: 服务启动至今gc总占用时间，秒，等于YGCT + FGCT jstat -gccause : 查看gc原因 12345678[root@master0 ~]# jstat -gccause 19080 S0 S1 E O M CCS YGC YGCT FGC FGCT GCT LGCC GCC 57.32 0.00 13.35 69.36 - - 20222 93.015 37 7.833 100.848 Allocation Failure No GC ---上述可以看到比-gcutil多处了一个LGCC和GCCLGCC: 最近一次gc发生的原因（last gc cause）GCC: 当前gc发生的原因 jstat -gccapacity : 查看虚拟机中对象的使用和容量大小 12345678910[root@master0 ~]# jstat -gccapacity 19080 NGCMN NGCMX NGC S0C S1C EC OGCMN OGCMX OGC OC MCMN MCMX MC CCSMN CCSMX CCSC YGC FGC 20480.0 323584.0 112640.0 7680.0 6656.0 96768.0 40448.0 647168.0 73728.0 73728.0 - - - - - - 20223 37---[容量单位为字节]NGC开头的表示：新生代空间容量OGC开头的表示：老年代空间容量MC开头的表示：元空间容量（Metaspace capacity）CCS开头的表示：类压缩空间 jstat -gcnew : 查看新生代gc情况 123456789[root@master0 ~]# jstat -gcnew 19080 S0C S1C S0U S1U TT MTT DSS EC EU YGC YGCT 7680.0 6656.0 0.0 6354.4 15 15 7680.0 96768.0 95634.9 20223 93.023---[容量单位为字节]TT: 老年化阈值，也可以理解为对象持有次数，就是在被移动到老年代之前，在新生代中存活的次数MTT: 最大老年化阈值DSS: Survivor区所需空间大小 jstat -gcnewcapacity : 查看新生代空间容量 123456789101112[root@master0 ~]# jstat -gcnewcapacity 19080 NGCMN NGCMX NGC S0CMX S0C S1CMX S1C ECMX EC YGC FGC 20480.0 323584.0 112128.0 107520.0 7168.0 107520.0 7168.0 322560.0 97280.0 20225 37---[容量单位为字节]MN表示：最小MX表示：最大NGC开头表示：新生代空间总容量S0C开头：新生代第一个survivor区容量S1C开头：新生代第二个survivor区容量EC开头：新生代Eden区容量 jstat -gcold : 查看老年代gc情况 1234567[root@master0 ~]# jstat -gcold 19080 1000 MC MU CCSC CCSU OC OU YGC FGC FGCT GCT - - - - 73728.0 51144.0 20234 37 7.833 100.943 - - - - 73728.0 51144.0 20234 37 7.833 100.943---[容量单位为字节] jstat -gcoldcapacity : 查看老年代容量 1234[root@master0 ~]# jstat -gcoldcapacity 19080 100 2 OGCMN OGCMX OGC OC YGC FGC FGCT GCT 40448.0 647168.0 73728.0 73728.0 20235 37 7.833 100.949 40448.0 647168.0 73728.0 73728.0 20235 37 7.833 100.949 jstat -gcpermcapacity : 查看perm中对象的信息和容量 1jdk1.8+以上去除了该命令，如果你当前使用的是jdk1.7-，那么自行谷歌吧 jstat -compiler : 查看虚拟机实时编译的信息 1234567891011[root@master0 ~]# jstat -compiler 19080Compiled Failed Invalid Time FailedType FailedMethod 2737 0 0 41.90 0 ---Compiled: 编译任务执行数量Failed: 编译任务执行失败数量Invalid: 编译任务执行失效数量Time: 编译任务消耗的时间FailedType: 最后一个编译失败任务的类型FailedMethod: 最后一个编译失败的任务所在类及方法 jstat -printcompilation : 查看虚拟机已经编译过的方法 123456789[root@master0 ~]# jstat -printcompilation 19080Compiled Size Type Method 2737 1562 1 sun/misc/FloatingDecimal dtoa---Compiled: 编译任务的数量Size: 方法生成字节码的大小（单位：字节）Type: 编译类型Method: 类名和方法名用来标识编译的方法","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[]},{"title":"Redis基本命令使用::list篇","slug":"Redis基本命令使用—list篇","date":"2019-10-31T04:55:00.000Z","updated":"2019-10-31T17:23:56.890Z","comments":true,"path":"2019/10/31/Redis基本命令使用—list篇/","link":"","permalink":"http://luxiaowan.github.io/2019/10/31/Redis基本命令使用—list篇/","excerpt":"","text":"Redis有6大基本类型，分别是字符串(String)、列表(List)、集合(set)、哈希结构(hash)、有序集合(zset)和基数(HyperLogLog) redis在线体验地址：http://try.redis.io/ 列表(List) 介绍: Redis的列表就是一个简单的字符串列表，我们可以往列表的头部和尾部添加新数据，要可以根据下标修改下标对应的值，列表是按照插入顺序有序的（按插入顺序倒序，类似于栈），并且列表可以出现重复数据。可以做消息队列，不过需要注意的是可能需要消息去重(后面有更牛的)。 创建缓存 创建一个列表缓存 命令格式：lpush key value [value ...] 🌰 1234lpush c1 1 2 3 4 5 6 7 8 9---将1~9放入c1列表中，此时列表中存储顺序为9 8 7 6 5 4 3 2 1 向列表左侧新增值 命令格式：lpush key value [value ...] 🌰 1234lpush c1 10---将10放入到c1列表头部，此时列表中存储顺序为10 9 8 7 6 5 4 3 2 1 向列表右侧新增值 命令格式：rpush key value [value ...] 🌰 1234rpush c1 0---将0放入到c1列表尾部，此时列表中存储顺序为10 9 8 7 6 5 4 3 2 1 0 以上两个命令很容易理解，lpush—&gt;left push, rpush—&gt;right push 在列表指定元素前/后插入数据 命令格式：linsert key BEFORE|AFTER pivot value 🌰 123456789101112131415161718192021221) linsert c1 after 0 -1---将-1插入到元素值0之前，此时列表中存储顺序为10 9 8 7 6 5 4 3 2 1 0 -12)linsert c1 before 0 1---将1插入到元素值0之前，此时列表中存储顺序为10 9 8 7 6 5 4 3 2 1 1 0 -13)linsert c1 before 1 3---将3插入到元素值1之前，此时列表中存储顺序为10 9 8 7 6 5 4 3 2 3 1 1 0 -14)linsert c1 after 1 4---将4插入到元素值1之后，此时列表中存储顺序为10 9 8 7 6 5 4 3 2 3 1 4 1 0 -15)linsert c1 after 99 100---失败** 由3、4操作可以得出结论，当执行linsert指令时，列表是从头部开始进行遍历，直到查询到与pivot元素相同的值时停止，根据AFTER、BEFORE选择是插入到元素后还是元素前，所以划重点：若列表中存在pivot的重复元素，linsert只会以第一个遍历到的元素为准** 由5可以得出结论，若指定的pivot元素不存在于列表中，则不进行任何设置 通过索引下标设置值 命令格式：lset key index value 🌰 12345671)lset c1 0 21---将下标为0的元素修改为21，此时列表中存储顺序为21 9 8 7 6 5 4 3 2 3 1 4 1 0 -12)lset c1 99 99---ERR index out of range,说明通过lset命令，不能越界修改元素 向已存在的列表头部添加元素 命令格式：lpushx key value [value ...] 🌰 12345671)lpushx c1 22 23 24---将22、23、24添加到列表c1头部，列表变为：24 23 22 21 9 8 7 6 5 4 3 2 3 1 4 1 0 -12)lpushx c2 1 2 3---因为列表c2不存在，所以设置失败，此时使用lrange查看c2会返回空 向已存在的列表尾部添加元素 命令格式：rpushx key value [value ...] 🌰 12345671)rpushx c1 -2 -3---将-2、-3添加到列表c1尾部，列表变为：24 23 22 21 9 8 7 6 5 4 3 2 3 1 4 1 0 -1 -2 -32)rpushx c2 1 2 3---因为列表c2不存在，所以设置失败，此时使用lrange查看c2会返回空 截取列表 命令格式：ltrim key start end 🌰 123ltrim c1 4 16---列表下标从0开始，截取5~17位的元素，列表c1变为：9 8 7 6 5 4 3 2 3 1 4 1 0 查看缓存 查看列表内所有元素 命令格式：lrange key 0 -1 🌰 1234lrange c1 0 -1---获取列表c1的所有元素 查看列表某一范围内的元素 命令格式：lrange key start end 🌰 1234lrange c1 1 3---查看列表c1中弟2~4位上的元素 弹出列表头部元素 命令格式：lpop key 🌰 1234lpop c1---弹出列表c1的头部元素9，此时列表c1变为：8 7 6 5 4 3 2 3 1 4 1 0，头部的9已经没有了，是不是很适合做消息队列 弹出列表尾部元素 命令格式：rpop key 🌰 1234rpop c1---弹出c1的尾部元素0，此时列表c1变为：8 7 6 5 4 3 2 3 1 4 1，尾部的0已经没有了，是不是很适合做消息队列👀 弹出列表头部元素，若当前列表内无元素，则阻塞，直到获取到或达到超时时间 命令格式：blpop key [key ...] timeout timeout单位为***秒*** 🌰 12345blpop c1 c2 c3 10---弹出列表c1或列表c2/c3的头部元素，只要c1、c2、c3有一个列表中有元素被弹出，则结束阻塞若c1、c2、c3均有元素，则返回第一个满足弹出条件的列表，然后结束阻塞 弹出列表尾部元素，若当前列表内无元素，则阻塞，直到获取到或达到超时时间 命令格式：brpop key [key ...] timeout timeout单位为***秒*** 🌰 12345blpop c1 c2 c3 10---弹出列表c1或列表c2/c3的尾部元素，只要c1、c2、c3有一个列表中有元素被弹出，则结束阻塞若c1、c2、c3均有元素，则返回第一个满足弹出条件的列表，然后结束阻塞 获取列表指定位置的元素 命令格式：lindex key index 🌰 12345671)lindex c1 2---返回列表c1中下标为2的元素，仅仅返回数据，不弹出，时间复杂度O(1)2)lindex c1 -1---返回列表最后一个元素。列表元素下标-1代表列表中最后一个元素，所以列表是可以通过负数下标从后往前遍历 弹出一个列表中的最后一个元素到另外一个列表头部，并返回这个元素——无阻塞 命令格式：rpoplpush source_key destination_key 🌰 1234rpoplpush c1 c2---弹出列表c1的尾部元素插入到列表c2的头部，若c1为空，则返回nil，但不插入到c2中，是不是更适合做队列 弹出一个列表中的最后一个元素到另外一个列表头部，并返回这个元素——阻塞 命令格式：brpoplpush source_key destination_key timeout timeout单位为***秒*** 🌰 1234brpoplpush c3 c2 10---弹出列表c3的尾部元素插入到列表c2的头部，若c3为空，则阻塞等到列表c3中有值，否则等到了10秒后结束阻塞返回nil，是不是更适合做阻塞队列 其他命令 删除指定范围内等于某个值的所有元素 命令格式：lrem key index element 🌰 1234567891011121314151)lrem c1 -2 3---移除列表c1中，从倒数第二个元素到列表头部范围内所有的32)lrem c1 3 2---移除列表c1中，从第四位元素到尾部范围内所有的23)lrem c1 0 1---移除列表c1中所有的14)lrem c1 -1 4---移除列表c1中所有的4 查看列表长度 命令格式：llen key 🌰 1234llen c1---查看列表c1的总长度，若c1不存在，则返回0，不会报错，记住，若列表不存在也不会报错","categories":[{"name":"Redis","slug":"Redis","permalink":"http://luxiaowan.github.io/categories/Redis/"}],"tags":[]},{"title":"Redis基本命令使用::set篇","slug":"Redis基本命令使用—set篇","date":"2019-10-31T04:55:00.000Z","updated":"2019-11-10T15:57:22.418Z","comments":true,"path":"2019/10/31/Redis基本命令使用—set篇/","link":"","permalink":"http://luxiaowan.github.io/2019/10/31/Redis基本命令使用—set篇/","excerpt":"","text":"Redis有6大基本类型，分别是字符串(String)、列表(List)、集合(set)、哈希结构(hash)、有序集合(zset)和基数(HyperLogLog) redis在线体验地址：http://try.redis.io/ 集合(Set) 介绍: Redis的集合就是一个简单的字符串集合，该集合是无序的，且集合内的元素都是唯一的，也就是集合内不会出现重复元素。Redis中的集合是通过哈希表来实现的，所以获取集合中元素的时间复杂度为O(1)。 创建缓存 创建一个集合缓存，为集合新增元素 命令格式：sadd key member [member ...] 🌰 1234sadd s1 1 2 3 0 -1 redis mongodb zookeeper---创建集合s1,元素为：1 2 3 0 -1 redis mongodb zookeeper 移除元素 随机移除集合中的一个元素并返回这个元素 命令格式：spop key 🌰 1234spop s1---移除之后，集合s1中就没有这个元素了 指定移除集合中一个或多个元素 命令格式：srem key member [member ...] 🌰 1234srem s1 -1 0 99---将元素-1、0、99从集合s1中移除，仅移除集合中存在的元素 查询集合 查询集合中元素个数 命令格式：scard key 🌰 1234scard s1---返回集合s1中元素总个数 返回集合中所有元素 命令格式：smembers key 🌰 1234smembers s1---返回集合s1中所有的元素 随机返回集合中的一个或多个元素 命令格式：srandmember key [count] 🌰 1234srandmember s1 10---随机返回集合s1中的10个元素，若不指定数量，则默认返回一个元素 迭代集合中的元素 命令格式：sscan key cursor [MATCH pattern] [COUNT count] 🌰 1234sscan s1 0 match re* count 1---迭代集合中re开头的所有元素，每次返回1个 判断元素是否存在于集合中 命令格式：sismember key member 🌰 1234sismember s1 99---若99存在于s1中，则返回1，不存在则返回0 多集合之间操作 查看多个集合的差集 命令格式：sdiff key [key ...] 🌰 1234sdiff s1 s2 s3---返回集合s1相对于s2、s3的差集，也就是只返回s1中所有不存在于s2、s3中的所有元素 多个集合的差集存储到指定集合中 命令格式：sdiffstore destination key [key ...] 🌰 1234sdiffstore ds1 s1 s2 s3---将s1中不存在于s2、s3中的元素存储到集合ds1中 查看多个集合的并集，去重 命令格式：sunion key [key ...] 🌰 1234sunion s1 s2 s3---将集合s1、s2、s3的元素合并去重后返回，所有元素均唯一 多个集合的并集存储到指定集合中 命令格式：sunionstore destination key [key ...] 🌰 1234sunionstore us1 s1 s2 s3---集合s1、s2、s3的并集存储到集合us1中，并返回集合us1中的元素个数 将一个集合中的某元素移动到另一个集合中 命令格式：smove source destination member 🌰 1234smove s1 s2 -2---将s1中的元素-2移动到集合s2中，若s2不存在，则自动创建 查看多个集合的交集 命令格式：sinter key [key ...]（intersection） 🌰 123456sinter s1 s3---返回集合s1和集合s3的交集，也就是两个集合中都存在的数据实际应用：查看两个人的共同好友；微信里查看和好友的共同群 多个集合的交集存储到指定集合中 命令格式：sinterstore destination key [key ...] 🌰 1234sinterstore is1 s1 s3---将s1和s3的交集元素存储到集合is1中","categories":[{"name":"Redis","slug":"Redis","permalink":"http://luxiaowan.github.io/categories/Redis/"}],"tags":[]},{"title":"Redis基本命令使用::hash篇","slug":"Redis基本命令使用—hash篇","date":"2019-10-30T15:55:00.000Z","updated":"2019-10-31T16:05:56.510Z","comments":true,"path":"2019/10/30/Redis基本命令使用—hash篇/","link":"","permalink":"http://luxiaowan.github.io/2019/10/30/Redis基本命令使用—hash篇/","excerpt":"","text":"Redis有6大基本类型，分别是字符串(String)、列表(List)、集合(set)、哈希结构(hash)、有序集合(zset)和基数(HyperLogLog) redis在线体验地址：http://try.redis.io/ 哈希(Hash) 介绍: Redis hash 是一个存储多个键值对的映射表，适用于存储对象的属性，比如存储用户信息、用户Session信息等。在实际项目中使用的频率比较多，之前主要用于存储用户基本信息、用户临时订单信息、产品信息等。 创建缓存 创建一个缓存 命令格式：hset key field value 🌰 1234hset cc name 'cc'---将cc的name属性的值设置为cc 批量创建缓存 命令格式：hmset key filed value [field value ...] 🌰 1234hmset cc name 'cc' age 19 avatar 'a.png' status 1---设置cc对象的name、age、avatar、status属性的值 设置一个key不存在field的value，若field已存在则不设置 命令格式：hsetnx key field value 🌰 1234hsetnx cc name 'yy'---若对象cc的name属性不存在，则设置cc的name属性值为yy 查看缓存 查看key下所有属性+值 命令格式：hgetall key 🌰 12345678hgetall cc---获取对象cc的所有属性，返回数据格式： field1 value1 field2 value2 查看key下所有的值 命令格式：hvals key 🌰 12345hvals cc---返回cc对象的所有属性的值，仅返回值，不返回属性名称等同于 ”hmget key 所有field“ 命令 查看key下所有的field名称 命令格式：hkeys key 🌰 1234hkeys cc---返回对象cc的所有属性名 查看key的某一field的值 命令格式：hget key field 🌰 1234hget cc name---返回对象cc的name属性的值，若对象无此属性，则返回nil 查看key的多个field的值 命令格式：hmget key field [field ...] 🌰 1234hmget cc name age status---返回对象cc的name、age、status属性 迭代对象的所有属性(适用于大对象) 命令格式：hscan key course [MATCH pattern] [COUNT num] 🌰 1hscan cc 0 MATCH *e COUNT 1 查看对象的属性数 命令格式：hlen key 🌰 1234hlen cc---返回对象cc的属性数量 其他命令 删除一个/多个属性 命令格式：hdel key field [field ...] 🌰 1234hdel cc name age---删除对象cc的name、age属性 查看对象属性是否存在 命令格式：hexists key field 🌰 1234hexists cc name---对象cc若存在属性name，则返回1，不存在则返回0","categories":[{"name":"Redis","slug":"Redis","permalink":"http://luxiaowan.github.io/categories/Redis/"}],"tags":[]},{"title":"Redis基本命令使用::string篇","slug":"Redis基本命令使用—string篇","date":"2019-10-29T15:55:00.000Z","updated":"2019-10-30T16:43:50.550Z","comments":true,"path":"2019/10/29/Redis基本命令使用—string篇/","link":"","permalink":"http://luxiaowan.github.io/2019/10/29/Redis基本命令使用—string篇/","excerpt":"","text":"Redis有6大基本类型，分别是字符串(String)、列表(List)、集合(set)、哈希结构(hash)、有序集合(zset)和基数(HyperLogLog) redis在线体验地址：http://try.redis.io/ 字符串(String) 介绍: 字符串是Redis中最基本的数据类型，数据以二进制的形式存储于内存中，所以Redis的字符串可以是任何形式的数据，比如JPEG图像、序列化的Ruby对象等。 字符串最大可存储512MB的数据，但一般一个字符串容量过大，会直接影响存储和查询的效率。字符串有两种编码方式：raw和embstr，根据字符串长度自动选择使用哪一种编码，目前最新版的长度是44个字节，字符串长度小于等于44个字节，则使用embstr编码，大于44个字节则使用raw编码，两种编码方式这里就不作详解，有兴趣的可以谷歌一下。 创建缓存 创建一个缓存 命令格式：set key value 🌰 1234set cc 'niubility'---创建一个key为cc，值为niubility的缓存 批量创建缓存 命令格式：mset key value [key value ...] 🌰 1234mset cc1 1 cc2 2 cc3 3---创建三个缓存，key:value分别为cc1:1, cc2:2, cc3:3 命令格式：msetnx key value [key value ...] 🌰 12345msetnx cc1 11 cc2 22 cc5 5 cc6 6---此命令只会将尚不存在的key值创建到缓存中，已经存在的key则忽略* cc1、cc2已存在于缓存中，所以不会创建/更新成功，cc5、cc6不存在于缓存中，会创建 创建一个带过期时间的缓存 命令格式：setex key time value 🌰 1234setex cc1 10 234---设置key:value为cc1:234且过期时间为10秒的缓存 设置一个不存在key的value，若key已存在则不设置 命令格式：setnx key value 🌰 1234setnx cc5 12---若cc5的key不存在于缓存中，则创建key:value为cc5:12的缓存，否则不执行创建 组合创建一个缓存(缓存过期时间、是否覆盖) 命令格式：set key value [EX|PX time] [NX|XX] 解析： 1234567 EX：表明过期时间为秒 PX：表明过期时间为毫秒 NX：若key不存在则执行，否则不执行，与XX相反 XX：若key存在则执行，否则不执行，与NX相反 🌰 1234 set cc1 123 EX 20 XX --- 若cc1已存在则创建过期时间为20秒的key:value = cc1:123 将key设置为新值的同时返回原值 命令格式：getset key value 🌰 1234getset cc1 1---若key不存在，则返回nil 读取缓存 读取一个key的缓存值 命令格式：get key 🌰 1234get cc1---读取key=cc1的值 批量读取一批数据 命令格式：mget key [key ...] 🌰 1mget cc1 cc2 cc3 截取字符串并返回 命令格式：getrange key start end 🌰 1234getrange cc1 1 12---字符串下标以0开始，若start超出字符串长度或key不存在，则返回空字符串 其他操作 将value加1( value必须为整数 )[ 可用于阅读量、点赞数等简单统计类的功能应用 ] 命令格式：incr key 🌰 1234incr cc1---每次调用均对value进行+1操作 给value加上某个数( num必须为整数 ) 命令格式：incrby key num 🌰 1234incrby cc1 100---给cc1的值加上100 给value加上某个浮点数 命令格式：incrbyfloat key num 🌰 1incrbyfloat cc1 0.2 给value减1 命令格式：decr key 🌰 1234decr cc1---每次调用均对value进行-1操作 给value减去某个数( num必须为整数 ) 命令格式：descby key num 🌰 1234decrby cc1 100---给cc1的值减去100 查看value的长度 命令格式：strlen key 🌰 1strlen cc1 目前先整理这些，都是一些基础的命令，随后再写一篇Java中使用Jedis操作字符串的随笔。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://luxiaowan.github.io/categories/Redis/"}],"tags":[]},{"title":"Redis数据在内存中是如何存储的","slug":"Redis数据在内存中是如何存储的","date":"2019-10-29T08:50:00.000Z","updated":"2019-10-29T15:02:17.269Z","comments":true,"path":"2019/10/29/Redis数据在内存中是如何存储的/","link":"","permalink":"http://luxiaowan.github.io/2019/10/29/Redis数据在内存中是如何存储的/","excerpt":"","text":"","categories":[{"name":"Redis","slug":"Redis","permalink":"http://luxiaowan.github.io/categories/Redis/"}],"tags":[]},{"title":"MySQL索引什么时候用hash","slug":"MySQL索引什么时候用hash","date":"2019-10-28T11:06:00.000Z","updated":"2019-10-29T14:10:04.447Z","comments":true,"path":"2019/10/28/MySQL索引什么时候用hash/","link":"","permalink":"http://luxiaowan.github.io/2019/10/28/MySQL索引什么时候用hash/","excerpt":"","text":"MySQL索引是在面试中常被问到的知识点，常用的两种索引方法有Hash和B+Tree，树的结构我们改天再扯，今天说收Hash。 为什么使用hash Hash索引可以根据数据的hash值直接定位到索引数据的存储位置，就相当于我知道了数组的下标，然后根据下标去取数据，这个效率可以说是最高的了，使用hash就是为了如此。 支持hash的存储引擎 目前支持hash的引擎有MEMORY(这里需要谷歌)，其他的引擎都通过各自的方式去支持hash方法。如InnoDB有一套自适应hash算法，内部实现还是采用了BT的方式，可以理解为BT索引的索引 InnoDB中hash索引支持的开启/关闭 hash索引虽然非常快速，但是在InnoDB中确实支持的不是很好，并且索引的具体创建是由引擎决定的(创建后存在于内存中)，非DBA可控，所以一般情况下建议关闭hash支持，使用BT也能够满足性能要求。 ​ set global innodb_adaptive_hash_index=off/on hash的使用场景 hash使用场景比较局限 hash索引仅适用于‘=’、‘&lt;=&gt;’和‘in’操作，所以hash仅仅适用于精确查找。 不适用于查询排序，因为hash后的数据并不会像原数据一样保持有序。 不适用于模糊查询，也就是不能使用like关键字。 既然不支持排序，也肯定不支持范围查询咯 解决hash冲突 不论hash的算法多么精确，当数据量大的时候都有可能发生hash碰撞，解决hash碰撞的方法有很多，比如再hash、链表叠加等，MySQL采用的是链表叠加的方式，也就是类似于HashMap解决hash碰撞的方法。所以在发生hash碰撞过多的情况下，使用hash索引会影响查询性能。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/categories/MySQL/"}],"tags":[]},{"title":"Spring-Session和Redis实现Session共享","slug":"Spring-Session和Redis实现Session共享","date":"2019-10-24T11:30:00.000Z","updated":"2019-10-24T15:18:45.220Z","comments":true,"path":"2019/10/24/Spring-Session和Redis实现Session共享/","link":"","permalink":"http://luxiaowan.github.io/2019/10/24/Spring-Session和Redis实现Session共享/","excerpt":"","text":"需求 现在大部分服务都以集群负载均衡的方式部署，几乎很难再遇到单点部署的项目，因为大家都要保证最基本的HA，说到HA，第一要考虑的就是各系统之间的Session共享的问题，如何解决呢？负载均衡当前使用Nginx 分析 不同的POD之间如果需要达到数据共享的目的，那么则需要使用同一个存储媒介，一开始想到使用MySQL来存储登录session，但是每次请求都去MySQL中查询数据，开销还是非常大的；然后最近使用MongoDB比较嗨，想着用MongoDB，但是MongoDB查询起来也不方便，况且我们这个Session也不是量级很大的数据集，最终采用了内存级的Redis来解决这个问题。 使用Redis的基本操作是将jsessionId为key，用户信息为value，使用jedis或者redisTemplate来操作Redis的读写行为。但是这种方式侵入了业务代码，并不是最优解，查了部分资料之后，发现spring-session.jar包中有一个非常有特色的注解@EnableRedisHttpSession，可以不需要侵入业务代码就能使用redis实现session共享的问题。 我们看一下@EnableRedisHttpSession的源码是怎么说的： 将此注释添加到一个单独的类上，该类必须加上@Configuration注解。使用方式在注释里也给出了demo代码。 实现 修改pom.xml文件，引入我们需要的jar包 123456789101112&lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 修改application.yml文件，配置redis信息 1234spring.redis.database=0spring.redis.host=localhostspring.redis.port=6379spring.redis.password=123456 创建配置类 1234567import org.springframework.context.annotation.Configuration;import org.springframework.session.data.redis.config.annotation.web.http.EnableRedisHttpSession;@Configuration@EnableRedisHttpSessionpublic class RedisHttpSessionConfig &#123;&#125; 到此，我们使用Redis实现Session共享的所有配置和代码都已经写完了，可以看到我们没有侵入到任何业务代码中，从头到尾也很简单。 扩展 1234@EnableGemFireHttpSession@EnableSpringHttpSession@EnableMongoHttpSession@EnableJdbcHttpSession","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://luxiaowan.github.io/categories/SpringBoot/"}],"tags":[]},{"title":"Java简单操作MongoDB","slug":"Java简单操作MongoDB","date":"2019-10-24T02:16:00.000Z","updated":"2019-10-24T13:20:12.762Z","comments":true,"path":"2019/10/24/Java简单操作MongoDB/","link":"","permalink":"http://luxiaowan.github.io/2019/10/24/Java简单操作MongoDB/","excerpt":"","text":"前面已经掌握了mongo最基本的一些命令，对各个命令也都实操过，理解各命令的意思，也对mongo有了最基本的理解，但大部分猿还是想使用Java去连接mongo，串串也不例外 在pom.xml中加入mongodb-java-driver.jar的依赖 Maven项目依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt; &lt;version&gt;3.0.4&lt;/version&gt;&lt;/dependency&gt; SpringBoot项目依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-mongodb&lt;/artifactId&gt;&lt;/dependency&gt; 连接mongo 认证连接 12345678// 创建验证信息，根据加密方式选择MongoCredential内对应的加密方式List&lt;MongoCredential&gt; credentials = new ArrayList&lt;&gt;();MongoCredential credential = MongoCredential.createCredential(\"admin\", \"test1\", \"admin\".toCharArray());credentials.add(credential);// 创建mongo服务地址ServerAddress serverAddress = new ServerAddress(\"localhost\", 27017);MongoClient mongoClient = new MongoClient(serverAddress, credentials); 无需认证连接 1MongoClient client = new MongoClient(\"localhost\", 27017); 然后就可以通过MongoClient的实例方法对mongo进行相关操作了 创建数据库连接 1MongoDatabase db = client.getDatabase(\"test1\"); mongo特性是不管数据库事先是否存在，都可以正常创建数据库连接，不会像MySQL一样报错，连接成功后，在mongo服务器上执行show dbs，会发现仍然查不到我们连接的这个数据库，这是正常情况，只有在数据库中有数据的时候，才会查得出来，client.getDatabase(&quot;test&quot;) === use test命令 创建集合 使用数据库连接实例方法创建一个空的集合 1234db.createCollection(\"base_info\");--- 命令：db.createCollection(\"base_info\") 直接向创建的集合中插入数据 12345MongoCollection&lt;Document&gt; collection = db.getCollection(\"base_info\");collection.insertOne(new Document(\"name\", \"lxl\"));--- 命令：db.base_info.insert(&#123;\"name\": \"lxl\"&#125;) 以上两种方式均可创建一个集合，区别在于第一种方式创建的是空集合， 删除集合 1234collection.drop();--- 命令：db.base_info.drop() 对集合的CRUD 新增数据 单条新增 12Document document = new Document().append(\"name\",\"cc\").append(\"age\",30).append(\"location\",\"SZ\");collection.insertOne(document); 批量新增 123456List&lt;Document&gt; documentList = new ArrayList&lt;&gt;();for (int i = 0; i &lt; 1000; i++) &#123; Document doc = new Document().append(\"name\", \"cc\" + i).append(\"age\", new Random().nextInt(100)).append(\"location\", \"SZ\"); documentList.add(doc);&#125;collection.insertMany(documentList); insertOne(Document)方法每次插入一条数据 insertMany(List)方法批量插入数据，并且可以通过参数InsertManyOptions设置是否排序 删除数据 删除第一条匹配的数据 12345Bson condition = Filters.eq(\"age\", 99);collection.deleteOne(condition);--- 命令：db.base_info.deleteOne(&#123;\"age\": 99&#125;) 删除所有匹配数据 12345Bson condition = Filters.eq(\"age\", 99);collection.deleteMany(condition);--- 命令：db.base_info.deleteMany(&#123;\"ag\": 99&#125;) 查询数据 查询返回第一条匹配的数据 1234Bson condition = Filters.eq(\"age\", 16);FindIterable&lt;Document&gt; vals = collection.find(condition);Document document = vals.first();System.out.println(document.toJson()); 通过调用FindIterable的实例方法first()取第一条数据 查询返回所有匹配数据 12345Bson condition = Filters.eq(\"age\", 16);FindIterable&lt;Document&gt; vals = collection.find(condition);for (Document val : vals) &#123; System.out.println(val.toJson());&#125; find()方法返回所有匹配数据 分页查询 分页查询是我们日常开发中经常用到的功能，尤其是mongo这种量级较大的存储，分页使用limit()和skip()两个方法来实现，limit指定查询的条数，skip进行分页，参数为从第几条开始，需要使用当前页码和分页条数进行计算(pageNo - 1) * pageSize 12345Bson condition = Filters.lt(\"age\", 2);FindIterable&lt;Document&gt; vals = collection.find(condition).limit(10).skip(10).sort(Sorts.descending(\"age\"));for (Document val : vals) &#123; System.out.println(val.toJson());&#125; limit(10): 每页取10条数据 skip(10): 从第11条开始查询，起始位置为0 sort(Sorts.descending(“age”)): 以列age倒序 对应MySQL：select * from base_info where age &lt; 2 order by age desc limit 7, 7 更新数据 更新第一条匹配数据中的某些字段 123Bson condition = Filters.eq(\"age\", 15);Document document = new Document(\"$set\", new Document(\"location\", \"XZ\"));collection.updateOne(condition, document); 注意这里有一个$set，这个指令是必须的，相对应的指令还有$inc 替换第一条匹配数据全部内容 123Bson condition = Filters.eq(\"age\", 15);Document document = new Document(\"location\", \"XZ\");collection.updateOne(condition, document); 没有$set指令,则表示使用参数document替换掉第一条匹配到的数据 第一条匹配的数据中指定字段数量+1 123Bson condition = Filters.eq(\"location\", \"XZ\");Document document = new Document(\"$inc\", new Document(\"age\", 1));collection.updateOne(condition, document); 更新所有匹配数据 123Bson condition = Filters.eq(\"age\", 15);Document document = new Document(\"$set\", new Document(\"location\", \"XZ\"));collection.updateMany(condition, document); 全部代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import org.bson.Document;import org.bson.conversions.Bson;import com.mongodb.MongoClient;import com.mongodb.client.FindIterable;import com.mongodb.client.MongoCollection;import com.mongodb.client.MongoDatabase;import com.mongodb.client.model.Filters;import com.mongodb.client.model.Sorts;/** * @author: cc */public class MongoConnectTest &#123; public static void main(String[] args) &#123; MongoClient client = new MongoClient(\"localhost\", 27017); try &#123; /* // 创建验证信息，根据加密方式选择MongoCredential内对应的加密方式 List&lt;MongoCredential&gt; credentials = new ArrayList&lt;&gt;(); MongoCredential credential = MongoCredential.createCredential(\"admin\", \"test\", \"\".toCharArray()); credentials.add(credential); // 创建mongo服务地址 ServerAddress serverAddress = new ServerAddress(\"localhost\", 27017); MongoClient mongoClient = new MongoClient(serverAddress, credentials);*/ MongoDatabase db = client.getDatabase(\"test1\"); // 创建集合 // 方式1 // db.createCollection(\"base_info2\"); // 方式2 MongoCollection&lt;Document&gt; collection = db.getCollection(\"base_info\"); // collection.insertOne(new Document(\"name\", \"lxl\")); // 删除集合 // collection.drop(); // 新增数据 // 单条新增 /*Document document = new Document().append(\"name\", \"cc\").append(\"age\", 30).append(\"location\", \"SZ\"); collection.insertOne(document); // 批量新增 List&lt;Document&gt; documentList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 1000; i++) &#123; Document doc = new Document().append(\"name\", \"cc\" + i).append(\"age\", new Random().nextInt(100)) .append(\"location\", \"SZ\"); documentList.add(doc); &#125; collection.insertMany(documentList);*/ // 删除数据 // 删除第一条匹配数据 /*Bson condition = Filters.eq(\"age\", 99); collection.deleteOne(condition); // 删除所有匹配数据 collection.deleteMany(condition);*/ // 修改数据 // 修改第一条匹配数据 /*Bson condition = Filters.eq(\"age\", 15); Document document = new Document(\"$set\", new Document(\"location\", \"XZ\")); collection.updateOne(condition, document); collection.updateMany(condition, document);*/ // 年龄+1 /*Bson condition = Filters.eq(\"location\", \"XZ\"); Document document = new Document(\"$inc\", new Document(\"age\", 1)); collection.updateOne(condition, document);*/ // 查询返回第一条匹配数据 Bson condition = Filters.lt(\"age\", 2); FindIterable&lt;Document&gt; vals = collection.find(condition).sort(Sorts.descending(\"age\")).limit(7).skip(7); // Document document = vals.first(); // System.out.println(document.toJson()); for (Document val : vals) &#123; System.out.println(val.toJson()); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://luxiaowan.github.io/categories/MongoDB/"}],"tags":[{"name":"Java与MongoDB","slug":"Java与MongoDB","permalink":"http://luxiaowan.github.io/tags/Java与MongoDB/"}]},{"title":"MongoDB监控、分片及备份恢复","slug":"MongoDB监控、分片及备份恢复","date":"2019-10-23T16:00:00.000Z","updated":"2019-10-24T13:20:12.764Z","comments":true,"path":"2019/10/24/MongoDB监控、分片及备份恢复/","link":"","permalink":"http://luxiaowan.github.io/2019/10/24/MongoDB监控、分片及备份恢复/","excerpt":"","text":"","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://luxiaowan.github.io/categories/MongoDB/"}],"tags":[{"name":"监控","slug":"监控","permalink":"http://luxiaowan.github.io/tags/监控/"}]},{"title":"MongoDB基本指令","slug":"MongoDB基本指令","date":"2019-10-22T16:00:00.000Z","updated":"2019-10-24T13:20:12.763Z","comments":true,"path":"2019/10/23/MongoDB基本指令/","link":"","permalink":"http://luxiaowan.github.io/2019/10/23/MongoDB基本指令/","excerpt":"","text":"查看所有的db show dbs 切换db user &lt;db_name&gt; 查看当前所在db名称 db 删除db**(必须在要删除的db中操作)** db.dropDatabase() 查看db下所有的集合 show tables show collections 创建集合 db.createCollection(&quot;abc&quot;) db.createCollection(&quot;def&quot;, {capped: true, autoIndexId: true, size: 1024, max: 100}) 往一张不存在的集合中插入一条数据，会自动创建集合 db.test.insert({title: 123}) 删除集合 db.&lt;collection_name&gt;.drop() 例：db.abc.drop() 插入文档 db.&lt;collection_name&gt;.insert({title: 1234}) doc=({title: 12345, name: &quot;MongoDB指南&quot;}) db.&lt;collection_name&gt;.insert(doc) db.&lt;collection_name&gt;.save({name: &quot;MongoDB简单指令&quot;}}) doc2=({name: &quot;MongoDB从入门到放弃&quot;}) db.&lt;collection_name&gt;.save(doc2) doc3=({_id: &quot;edrftgyhjkgjhfgv2ryuoio&quot;, name: &quot;MongoDB从入门到放弃&quot;}) db.&lt;collection_name&gt;.save(doc3)// 若_id对应值的数据已经存在，则更新这条数据，否则新增一条数据 更新文档 db.&lt;collection_name&gt;.update({title: 1234}, {$set:{title: &quot;4321&quot;}}) 格式： 123456789db.&lt;collection_name&gt;.update( &lt;where&gt;,// 相当于MySQL的where &lt;update&gt;,// 相当于MySQL的update语句的set，需要跟一些指令：$,$inc,$set &#123; upsert: true,// true：如果不存在记录，则新增；false相反，默认 multi: true,// true：只更新第一条匹配的记录；false相反，全部更新， 默认 writeConcern: &lt;document&gt;// 异常级别 &#125;) 案例 12345678910111213&gt; 1. 只更新第一条记录：&gt; db.col.update( &#123; \"count\" : &#123; $gt : 1 &#125; &#125; , &#123; $set : &#123; \"test2\" : \"OK\"&#125; &#125; );&gt; 2. 全部更新：&gt; db.col.update( &#123; \"count\" : &#123; $gt : 3 &#125; &#125; , &#123; $set : &#123; \"test2\" : \"OK\"&#125; &#125;,false,true );&gt; 3. 只添加第一条：&gt; db.col.update( &#123; \"count\" : &#123; $gt : 4 &#125; &#125; , &#123; $set : &#123; \"test5\" : \"OK\"&#125; &#125;,true,false );&gt; 4. 全部添加进去:&gt; db.col.update( &#123; \"count\" : &#123; $gt : 5 &#125; &#125; , &#123; $set : &#123; \"test5\" : \"OK\"&#125; &#125;,true,true );&gt; 5. 全部更新：&gt; db.col.update( &#123; \"count\" : &#123; $gt : 15 &#125; &#125; , &#123; $inc : &#123; \"count\" : 1&#125; &#125;,false,true );&gt; 6. 只更新第一条记录：&gt; db.col.update( &#123; \"count\" : &#123; $gt : 10 &#125; &#125; , &#123; $inc : &#123; \"count\" : 1&#125; &#125;,false,false );&gt; db.&lt;collection_name&gt;.save() 调用save指令一般需要指定_id 删除文档 db.&lt;collection_name&gt;.remove({title: &quot;4321&quot;}, true) 格式 1234db.&lt;collection_name&gt;.remove( &lt;where&gt;,// 相当于MySQL的where &lt;justOne&gt;// true：只删除一条匹配条件的数据；false：匹配条件的数据全部删除，默认) 删除集合中所有数据 db.&lt;collection_name&gt;.remove({}) 新函数： db.&lt;collection_name&gt;.deleteMany({}) db.&lt;collection_name&gt;.deleteOne({title: &quot;12345&quot;}) 查询文档 普通查询 格式： 12345&gt; db.&lt;collection_name&gt;.find(&gt; &lt;where&gt;, // 查询条件&gt; &lt;colName&gt;// 返回字段名称&gt; )&gt; 创建一个集合，插入三条数据 1db.user.insert([&#123;name: \"cc\", age: \"29\", gender: 1&#125;, &#123;name: \"ccc\", age: \"30\", gender: 2&#125;, &#123;name: \"c\", age: \"28\", gender: 1&#125;]) 查询集合中全部数据 普通显示：db.user.find() 格式化显示：db.user.find().pretty() 查询name=&quot;c&quot;的信息 db.user.find({name: &quot;c&quot;}).pretty() 查询只返回第一个匹配到的数据 db.user.findOne({name: &quot;c&quot;}) AND查询 db.user.find({key: value, key: value}) And查询即是在where条件里面用逗号&quot;,&quot;分隔 栗子： 123456&gt; db.user.find(&#123;name:\"cc\", gender:1&#125;)&gt; &gt; ---&gt; 等同于MySQL：&gt; select * from user where name = \"c\" AND gender = 1&gt; OR查询 db.user.find({$or:[{key:value}, {key:value}]}) 栗子： 12345678&gt; db.user.find(&#123;&gt; $or:[&#123;name:\"c\"&#125;, &#123;gender:2&#125;]&gt; &#125;)&gt; &gt; ---&gt; 等同于MySQL：&gt; select * from user where name = \"c\" OR gender = 2&gt; AND和OR组合查询 db.user.find({key:value, $or:[{key:value}, {key:value}]}) 栗子： 123456&gt; db.user.find(&#123;gender:1, $or:[&#123;name: \"c\"&#125;, &#123;age: \"28\"&#125;]&#125;)&gt; &gt; ---&gt; 等同于MySQL：&gt; select * from user where gender = 1 AND (name = \"c\" OR age = \"28\")&gt; 运算符 运算符 格式 案例 MySQL对应语句 等于 {key:value}{key:{$eq:value}} db.user.find({age:“29”})db.user.find({age:{$eq:“29”}}) where age = “29” 大于 {key:{$gt: value}} db.user.find({age:{$gt:“30”}}) where age &gt; “30” 小于 {key:{$lt: value}} db.user.find({age:{$lt: “30”}}) where age &lt; “30” 大于等于 {key:{$gte: value}} db.user.find({age:{$gte:“30”}}) where age &gt;= “30” 小于等于 {key:{$lte: value}} db.user.find({age:{$lte:“30”}}) where age &lt;= “30” 不等于 {key:{ne: value}} db.user.find({age:{$ne:“29”}}) where age != “30” 模糊查询 查询age包含0的：db.user.find({age:/0/}) 查询age以2开头的：db.user.find({age:/^2/}) 查询age以8结束的：db.user.find({age:/8$/}) 分页查询 格式： 123456&gt; db.&lt;collection_name&gt;.find().limit(Number).skip(Number)&gt; &gt; ---&gt; limit(Number)表示查询多少条数据&gt; skip(Number)表示从第几条开始查询&gt; 查询一条数据 第一种方法：db.user.findOne({}) 第二种方法：db.user.find({}).limit(1) 从第二条数据开始查询一条数据 db.user.find().limit(1).skip(2) 查询排序 格式： 1234567&gt; db.&lt;collection_name&gt;.find().sort(&#123;&lt;key_name&gt;:-1/1&#125;)&gt; &gt; ---&gt; &lt;key_name&gt;：排序字段&gt; -1：倒序&gt; 1：正序&gt; 按照年龄倒序 db.user.find().sort({age:-1}) 按照年龄倒序、性别正序 db.user.find().sort({age:-1, gender:1})","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://luxiaowan.github.io/categories/MongoDB/"}],"tags":[{"name":"基本指令","slug":"基本指令","permalink":"http://luxiaowan.github.io/tags/基本指令/"}]},{"title":"MongoDB基础应用","slug":"MongoDB基础应用","date":"2019-10-22T16:00:00.000Z","updated":"2019-10-24T13:20:12.764Z","comments":true,"path":"2019/10/23/MongoDB基础应用/","link":"","permalink":"http://luxiaowan.github.io/2019/10/23/MongoDB基础应用/","excerpt":"","text":"索引 说明：索引是为了加快查询速度，可以对集合中的一列或多列设置索引。 – 无索引：扫描整个集合的文档，查找符合条件的文档 – 有索引：查找索引，根据索引取出文档数据 创建索引 db.&lt;collection_name&gt;.createIndex(keys, option) 栗子： 123456789101. 单索引db.user.createIndex(&#123;age:-1&#125;)---age:索引列名-1:倒序索引1:正序索引2. 多索引(复合索引)db.user.createIndex(&#123;age:1, gender:-1&#125;) 可选参数 参数 类型 说明 background Boolean 指定创建索引时是否阻塞集合的其他操作。true:后台执行，不阻塞；false:阻塞，默认 unique Boolean 指定索引是否为唯一索引。true:唯一索引；false:不唯一，默认 name String 索引名称，默认为字段名+索引顺序 v indexversion 索引版本号，默认为当前mongo的版本号 weights Integer 1~99999之间，值越大权重越大 expireAfterSeconds Integer 指定集合生存时间。秒级，TTL sparse Boolean 指定是否忽略不存在的字段。true:不查出不包含查询字段的文档；false:查询所有文档，默认 栗子： 12345&gt; db.user.createIndex(&#123;name:1&#125;, &#123;background: true, unique: true, name: \"idx_user_name\", v: 1, weights: 99, sparse: true&#125;)&gt; &gt; ---&gt; 后台不阻塞集合的方式创建一个name列正序,版本号为1,权重99,忽略无name字段的文档的唯一索引idx_user_name&gt; 查看所有索引 db.&lt;collection_name&gt;.getIndexes() 栗子： db.user.getIndexes() 重建索引 方法1：db.&lt;collection_name&gt;.reIndex() 方法2：先删除原索引，然后再创建 12db.&lt;collection_name&gt;.dropIndex(&lt;idx_name&gt;)db.&lt;collection_name&gt;.createIndex(...) 删除索引 删除指定名称的索引： db.user.dropIndex(&lt;idx_name&gt;) 删除集合中所有索引 db.user.dropIndexes() 说明：只会删除自建的索引，集合中_id列的索引不会被删除 聚合查询 12345678910111213141516171819202122232425创建集合并插入数据：db.agg.save([&#123; title: 'MongoDB Overview', description: 'MongoDB is no sql database', by_user: 'runoob.com', url: 'http://www.runoob.com', tags: ['mongodb', 'database', 'NoSQL'], likes: 100&#125;,&#123; title: 'NoSQL Overview', description: 'No sql database is very fast', by_user: 'runoob.com', url: 'http://www.runoob.com', tags: ['mongodb', 'database', 'NoSQL'], likes: 10&#125;,&#123; title: 'Neo4j Overview', description: 'Neo4j is no sql database', by_user: 'Neo4j', url: 'http://www.neo4j.com', tags: ['neo4j', 'database', 'NoSQL'], likes: 750&#125;]) 格式： 12345678910&gt; db.&lt;collection_name&gt;.aggregate(condition)&gt; ---&gt; condition:&gt; [&#123;$group:&#123;_id:\"$&lt;key&gt;\", num_tutorial:&#123;$&lt;fun_expression&gt;:\"$&lt;key&gt;\"&#125;&#125;&#125;]&gt; $group:一个组&gt; _id:组合列，类同于MySQL的group by后面的字段，默认_id的列会在查询结果中显示&gt; num_tutorial:输出的列名&gt; $&lt;fun_expression&gt;:聚合表达式&gt; $&lt;key&gt;:运算的列名&gt; 关键字：aggregate 说明：聚合查询就是求和、最大、最小、最前、最后、平均数的统称，类似于MySQL的count()、sum()、avg() 栗子： 12345&gt; db.user.aggregate([&#123;$group: $&#123;_id:\"$gender\", num_tutorial:&#123;$sum:1&#125;&#125;&#125;])&gt; &gt; ---&gt; 等同于MySQL：select gender, count(1) from user group by gender&gt; 聚合表达式 表达式 描述 案例 $sum 计算总和 db.agg.aggregate([{$group:{_id:&quot;$by_user&quot;, num_tutorial:{$sum:&quot;$likes&quot;}}}])—等同于MySQL：select by_user, sum(likes) from agg group by by_user $avg 计算平均值 db.agg.aggregate([{$group:{_id:&quot;$by_user&quot;, num_tutorial:{$avg:&quot;$likes&quot;}}}])—等同于MySQL：select by_user, avg(likes) from agg group by by_user $min 获取集合中指定列的最小值记录 db.agg.aggregate([{$group:{_id:&quot;$by_user&quot;, num_tutorial:{$min:&quot;$like&quot;}}}])—等同于MySQL：select by_user, min(likes) from agg group by by_user $max 获取集合中指定列的最大值记录 db.agg.aggregate([{$group:{_id:&quot;$by_user&quot;, num_tutorial:{$max:&quot;$likes&quot;}}}])—等同于MySQL：select by_user, max(likes) from agg group by by_user $push 在结果文档中插入值到数组 db.agg.aggregate([{$group:{_id:&quot;$by_user&quot;, num_tutorial:{$push:&quot;$url&quot;}}}])—等同于MySQL：查出所有数据，然后取字段url写入同一个列表中，url不去重，然后输出 $addToSet 在结果文档中插入值到一个数组中，但不创建副本 db.agg.aggregate([{$group:{_id:&quot;$by_user&quot;, num_tutorial:{$addToSet:&quot;$url&quot;}}}])—等同于MySQL：查出所有数据，然后取字段url写入同一个列表中，url去重，然后输出 $first 根据资源文档的排序获取第一个文档数据 db.agg.aggregate([{$group:{_id:&quot;$by_user&quot;, num_tutorial:{$first:&quot;$title&quot;}}}]) $last 根据资源文档的排序获取最后一个文档数据 db.agg.aggregate([{$group:{_id:&quot;$by_user&quot;, num_tutorial:{$last:&quot;$title&quot;}}}]) 管道函数 说明：管道函数类似于Linux系统中的管道操作，将上一步的运算结果作为下一步的输入值，最终达到理想计算结果的运算方式 函数 说明 $project 指定需要输出的列，默认显示_id，格式：{$project:{by_user:1, title:1, url:1, _id:0不显示id}}, $limit 限制查询返回的文档数，格式：{$limit: 1},只返回一个文档 $skip 跳过指定数量的文档，返回之后的所有文档，格式：{$skip: 1},从第二个文档开始输出 $match 条件筛选，格式：{match: {likes: {gte: 10}}} $group 聚合条件，格式：{group: {_id: &quot;by_user&quot;, count: {$sum: -1}}} $sort 排序，格式：{$sort: {likes: -1}}，-1:倒序; 1:正序 $unwind 将文档中的某一个数组类型字段拆分成多条，每条包含数组中的一个值 $geoNear 输出接近某一地理位置的有序文档。 栗子： 123456&gt; db.agg.aggregate(&#123;$match: &#123;likes: &#123;$gte: 10&#125;&#125;&#125;, &#123; $project: &#123;_id: 0, title: 1, by_user: 1, likes: 1&#125;&#125;, &#123;$limit: 5&#125;, &#123;$skip: 1&#125;, &#123;$sort: &#123;likes: -1&#125;&#125;)&gt; &gt; ---&gt; 等同于MySQL: &gt; select title, by_user, likes from agg where likes &gt;= 10 order by likes desc limit 1,5&gt;","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://luxiaowan.github.io/categories/MongoDB/"}],"tags":[{"name":"基础应用","slug":"基础应用","permalink":"http://luxiaowan.github.io/tags/基础应用/"}]},{"title":"MongoDB连接报错java.lang.NoSuchFieldError ACKNOWLEDGED","slug":"MongoDB连接报错java.lang.NoSuchFieldError-ACKNOWLEDGED","date":"2019-10-22T16:00:00.000Z","updated":"2019-10-24T13:20:12.764Z","comments":true,"path":"2019/10/23/MongoDB连接报错java.lang.NoSuchFieldError-ACKNOWLEDGED/","link":"","permalink":"http://luxiaowan.github.io/2019/10/23/MongoDB连接报错java.lang.NoSuchFieldError-ACKNOWLEDGED/","excerpt":"","text":"BUG描述 使用SpringBoot整合MongoDB时，正要运行代码连接mongo，就赤红赤红的报了个错： 12345Exception in thread \"main\" java.lang.NoSuchFieldError: ACKNOWLEDGED at com.mongodb.MongoClientOptions$Builder.&lt;init&gt;(MongoClientOptions.java:960) at com.mongodb.MongoClient.&lt;init&gt;(MongoClient.java:155) at com.mongodb.MongoClient.&lt;init&gt;(MongoClient.java:145) at com.example.demo.mongo.MongoConnectTest.main(MongoConnectTest.java:15) 这一下就傻眼了，对于刚接触mongo的人来说，是很懵圈的，大脑知识库中没有这个异常信息的解决办法，只能谷歌了，没想到有那么多人遇到过这个问题 BUG解决 在Stack Overflow上找到一个帖子： https://stackoverflow.com/questions/13593614/mongodb-java-lang-nosuchfielderror 其中jyemin的回答可以说是直击要害了，顺利的解决了这个问题，我把截图贴上，以防帖子被删 其实就是在工程中引入了多个版本不同的mongo-java-driver，所以导致程序混乱，只要保留自己真正使用的那个版本，其他的都删除即可 我的配置： 将2.7.1版本的依赖删除就可以正常运行了","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://luxiaowan.github.io/categories/MongoDB/"}],"tags":[{"name":"MongoDB错误记录","slug":"MongoDB错误记录","permalink":"http://luxiaowan.github.io/tags/MongoDB错误记录/"}]},{"title":"Git Stash用法","slug":"git stash命令","date":"2019-10-21T16:00:00.000Z","updated":"2019-10-24T13:20:12.765Z","comments":true,"path":"2019/10/22/git stash命令/","link":"","permalink":"http://luxiaowan.github.io/2019/10/22/git stash命令/","excerpt":"","text":"创建仓库 git init echo 123 &gt; test.txt git add . git commit -m “add test file” git remote add origin git@git.xx.xx.xx:xxx/xxx.git git push origin master 以上内容比较简单，就不作详细讲述，如果上面的内容看不懂，下面的请放弃 使用git stash暂存 随意修改test.txt文件的内容，比如：画个心形，你正热火朝天修改中…… 这时领导走过来拍了拍你的肩膀说到：”小伙子啊，创建个朕.txt文件提交上来，内容就写我还想再活五百年，test.txt文件这一版不作修改“。 你面露笑容的回答：“好的，没问题！”，内心却是：“MMP,MMP,MMP……” 辛辛苦苦修改的test.txt文件怎么办？眼瞅着就要完成了，难道复制出去，然后等解决领导需求后再粘贴回来？low不low？肯定不low啊，一个文件而已，这种方法很简单，*但是当你实际项目中修改了几十个文件的时候呢？*挨个儿复制出去？累傻小子呐？！！！！！ git stash命令帮你解决问题，stash是存储的意思，也就是将当前工作区内的所有东西都存储起来，然后工作区所有文件恢复到修改之前的状态(并不是最新状态，不会和仓库中进行自动同步，需要你自己去pull)，然后你就可以继续完成任务了。(在执行git stash之前需要先执行git add命令) stash可以进行多次操作，每次操作都会将当前工作区的文件情况暂存起来，stash是类栈存储，每次stash的序号都为0，此次之前stash的序号会自动+1 使用git stash pop取出 git stash pop取出栈顶元素，也就是序号为0的那个，即最近一次执行git stash保存的内容。pop之后，暂存列表中就会自动将其清除掉，这个时候你再执行git stash list会发现毛都没有 这个时候可能就会纳闷了，我保存了好几次，但是我这次是想使用最开始stash的那份内容，怎么办？一直pop，直到最后一次？当然不行，上面刚讲过pop之后暂存列表中就没有stash的信息了，已经被pop出去的就找不回来了，等于是自杀式攻击，那咋整呢？ git stash apply stash@{序号}可以将指定序号的stash内容弹出到工作区，此时工作区里文件的状态就和stash@{序号}里的一致了，但是这个命令无法将stash记录从暂存列表中删除，仅仅只是将文件恢复而已 git stash drop stash@{序号}来丢弃暂存列表中的记录，可以配合apply使用 查看暂存记录中的信息 查看暂存列表：git stash list查看当前stash的列表 查看暂存内容：git stash show stash@{序号}查看指定序号的stash的内容 git stash save ‘msg’ 等于是在stash的时候打了个标签，妖娆！！！！","categories":[{"name":"Git","slug":"Git","permalink":"http://luxiaowan.github.io/categories/Git/"}],"tags":[]},{"title":"Hash索引相关","slug":"Hash索引相关","date":"2019-10-12T15:42:43.000Z","updated":"2019-10-21T15:48:05.043Z","comments":true,"path":"2019/10/12/Hash索引相关/","link":"","permalink":"http://luxiaowan.github.io/2019/10/12/Hash索引相关/","excerpt":"","text":"hash索引结构使用方式较为局限，仅适用于=、IN和&lt;=&gt;三种，但是由于通过hash可以直接查找到具体的值，而不用像BT那样每次都从root节点开始遍历，所以在通常情况下，hash的查找效率要比BT高。 hash的缺陷： 1. hash不能进行范围查找 值在计算hash后，并不能保证计算后的hash值和计算前的大小排列一样，所以hash不适用于范围查找 2. hash不能进行排序查询 值计算后的hash值无法保证与原值大小顺序一样，所以无法进行排序 3. 组合索引不能使用部分字段查询 组合索引的hash值是所有索引字段的值组合在一起进行计算的，若仅使用部分字段进行查询的话，计算出的hash值基本不会与索引的hash值相同 4. hash在出现大量值碰撞的时候，性能会降低 hash出现大量的值相等的时候，需要进行表扫描以进行精确匹配，效率较低","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[{"name":"Java基础知识","slug":"Java基础知识","permalink":"http://luxiaowan.github.io/tags/Java基础知识/"}]},{"title":"Char和varchar简单介绍","slug":"char与varchar简单介绍","date":"2019-10-12T04:10:43.000Z","updated":"2019-10-21T16:19:50.476Z","comments":true,"path":"2019/10/12/char与varchar简单介绍/","link":"","permalink":"http://luxiaowan.github.io/2019/10/12/char与varchar简单介绍/","excerpt":"","text":"1. 数据长度 1) char(最大长度255个字节) 长度固定（字段存入数据长度始终等于字段长度） 2) varchar(最大长度65535个字节) 可变长度，存入数据长度为N个字节，则实际使用了N+1(255以上长度则+2)个字节的空间，多出来的1字节是用来存储数据实际长度。 存入数据对比 存入&quot;ab&quot; char查询出&quot;ab&quot; varchar查询出&quot;ab&quot; 存入&quot;ab &quot;，ab后面有两个空格， char查询出&quot;ab&quot; varchar查询出&quot;ab &quot; 解析： ​ 在入库时，数据库会自动在ab后面添加两位空格，让入库的数据长度保证等于4 char(4) ，这种操作很容易理解，但是如果你去数据表里面去查这条数据，会发现表中数据并没有空格，所以可以推断出这里是逻辑追加，所以在查询数据的时候会并不会出现引擎自动添加的空格。 2. 实操(技术一定要**实操**) 1）无空格数据 先创建一张表 1create table cv(c char(4), v varchar(4)); 插入数据 1insert into cv values(&quot;ab&quot;, &quot;ab&quot;); 查询数据 1select concat(&apos;(&apos;, c, &apos;)&apos;) AS c, concat(&apos;(&apos;, v, &apos;)&apos;) AS v from cv; 结果 结果中char和varchar均查出来为无空格的ab ####2）有空格数据 插入数据 1insert into cv values(&quot;ab &quot;, &quot;ab &quot;); 查询数据 1select concat(&apos;(&apos;, c, &apos;)&apos;) AS c, concat(&apos;(&apos;, v, &apos;)&apos;) AS v from cv; 结果 结果中可以看出，char类型将数据后面的空格自动去掉了，varchar则保留了所有的空格","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/categories/MySQL/"}],"tags":[]},{"title":"MySQL使用查询缓存","slug":"MySQL使用查询缓存","date":"2019-10-11T05:01:23.000Z","updated":"2019-10-21T15:48:59.282Z","comments":true,"path":"2019/10/11/MySQL使用查询缓存/","link":"","permalink":"http://luxiaowan.github.io/2019/10/11/MySQL使用查询缓存/","excerpt":"","text":"查询服务是否已开启缓存 执行show variables like '%query_cache%';查看缓存状态 Variable_name: query_cache_type为缓存状态，ON表示开启，OFF表示关闭 开启/关闭使用查询缓存 修改my.cnf文件进行开启和关闭 [mysqld]中添加/修改： query_cache_size = 20M query_cache_type = ON/OFF **修改完成后重启MySQL服务：service mysql restart/systemctl mysql restart ** 查询缓存使用情况 执行show status like 'qcache%';查询缓存使用情况 因为本机MySQL未开启查询缓存，所以此处和使用相关的属性均为0 属性解释: 属性 释义 Qcache_free_blocks 缓存中相邻内存块的个数。数目大说明可能有碎片。FLUSH QUERY CACHE会对缓存中的碎片进行整理，从而得到一个空闲块。 Qcache_free_memory 缓存中空闲内存大小 Qcache_hits 缓存命中次数，命中一次就+1 Qcache_inserts 查询次数，命中次数/查询次数=缓存命中率 Qcache_lowmem_prunes 缓存出现内存不足并且必须要进行清理以便为更多查询提供空间的次数，如果数字不断增长，就可能碎片非常严重，或者内存很少，通过Qcache_free_blocks、Qcache_free_memory来分析具体情况 Qcache_not_cached 不适合进行缓存的查询的数量 Qcache_queries_in_cache 当前缓存的查询(和响应)的数量 Qcache_total_blocks 缓存中块的数量","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/categories/MySQL/"}],"tags":[]},{"title":"Java基础面试题","slug":"蚂蚁Java基础面试题","date":"2019-10-10T15:21:56.000Z","updated":"2019-10-21T15:49:27.313Z","comments":true,"path":"2019/10/10/蚂蚁Java基础面试题/","link":"","permalink":"http://luxiaowan.github.io/2019/10/10/蚂蚁Java基础面试题/","excerpt":"","text":"一 map怎么实现hashcode和equals,为什么重写equals必须重写hashcode 使用过concurrent包下的哪些类，使用场景等等。 concurrentHashMap怎么实现？concurrenthashmap在1.8和1.7里面有什么区别 CountDownLatch、LinkedHashMap、AQS实现原理 线程池有哪些RejectedExecutionHandler,分别对应的使用场景 多线程的锁？怎么优化的？偏向锁、轻量级锁、重量级锁？ 组合索引？B+树如何存储的？ 为什么缓存更新策略是先更新数据库后删除缓存 OOM说一下？怎么排查？哪些会导致OOM? OSI七层结构，每层结构都是干什么的？ java的线程安全queue需要注意的点 死锁的原因，如何避免 二 jvm虚拟机老年代什么情况下会发生gc，给你一个场景，一台4核8G的服务器，每隔两个小时就要出现一次老年代gc，现在有日志，怎么分析是哪里出了问题 数据库索引有哪些？底层怎么实现的？数据库怎么优化？ 数据库的事务，四个性质说一下，分别有什么用，怎么实现的？ 服务器如何负载均衡，有哪些算法，哪个比较好，一致性哈希原理，怎么避免DDOS攻击请求打到少数机器 volatile讲讲 哪些设计模式？装饰器、代理讲讲？ redis集群会吗？ mysql存储引擎 事务隔离级别 不可重复度和幻读，怎么避免，底层怎么实现（行锁表锁） 三 项目介绍 分布式锁是怎么实现的 MySQL有哪几种join方式，底层原理是什么 Redis有哪些数据结构？底层的编码有哪些？有序链表采用了哪些不同的编码？ Redis扩容，失效key清理策略 Redis的持久化怎么做，aof和rdb，有什么区别，有什么优缺点。 MySQL数据库怎么实现分库分表，以及数据同步？ 单点登录如何是实现？ 谈谈SpringBoot和SpringCloud的理解 未来的技术职业怎么规划？ 为什么选择我们公司？","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://luxiaowan.github.io/tags/面试/"},{"name":"阿里","slug":"阿里","permalink":"http://luxiaowan.github.io/tags/阿里/"}]},{"title":"Linux配置JDK环境","slug":"Linux配置JDK环境","date":"2019-01-15T04:12:00.000Z","updated":"2020-03-24T05:09:48.450Z","comments":true,"path":"2019/01/15/Linux配置JDK环境/","link":"","permalink":"http://luxiaowan.github.io/2019/01/15/Linux配置JDK环境/","excerpt":"","text":"1、下载jdk压缩包 https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 2、解压 tar -zxvf jdk-8u191-linux-x64.tar.gz /opt/jdk 3、设置环境变量 vim ~/.bashrc 内容： 12345export JAVA_HOME=/opt/jdkexport CLASSPATH=$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$PATH:$JAVA_HOME/bin 4、立即设置环境变量 source ~/.bashrc","categories":[{"name":"Linux","slug":"Linux","permalink":"http://luxiaowan.github.io/categories/Linux/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/tags/Java/"}]},{"title":"Linux配置maven环境","slug":"Linux配置maven环境","date":"2019-01-15T03:00:00.000Z","updated":"2020-03-24T05:06:04.033Z","comments":true,"path":"2019/01/15/Linux配置maven环境/","link":"","permalink":"http://luxiaowan.github.io/2019/01/15/Linux配置maven环境/","excerpt":"","text":"0、先设置JDK环境 &lt; 设置 &gt; 1、下载Maven包 wget -c http://mirror.bit.edu.cn/apache/maven/maven-3/3.6.0/binaries/apache-maven-3.6.0-bin.tar.gz 2、解压 tar -zxvf apache-maven-3.6.0-bin.tar.gz /opt/ 3、设置环境变量 vim ~/.bashrc 内容： 123export M2_HOME=/opt/apache-maven-3.6.0export PATH=$PATH:$M2_HOME/bin 4、立即设置环境变量 source ~/.bashrc","categories":[{"name":"Linux","slug":"Linux","permalink":"http://luxiaowan.github.io/categories/Linux/"}],"tags":[{"name":"Maven","slug":"Maven","permalink":"http://luxiaowan.github.io/tags/Maven/"}]}]}