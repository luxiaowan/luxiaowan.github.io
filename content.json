{"meta":{"title":"串一串","subtitle":"断舍离","description":null,"author":"cc","url":"http://luxiaowan.github.io","root":"/"},"pages":[{"title":"关于本人","date":"2019-10-21T15:26:09.000Z","updated":"2019-10-21T15:28:30.066Z","comments":true,"path":"about/index.html","permalink":"http://luxiaowan.github.io/about/index.html","excerpt":"","text":"串一串 浪人 非常懒，偶尔写一下 文笔很烂，写的不好 凑合看吧"},{"title":"推荐书单","date":"2019-10-21T15:38:29.000Z","updated":"2019-10-21T15:56:47.928Z","comments":true,"path":"books/index.html","permalink":"http://luxiaowan.github.io/books/index.html","excerpt":"","text":"书名 购买地址 JavaScript权威指南（第6版） JavaScript高级程序设计 Java编程思想（第4版） java并发编程实战 Netty权威指南（第2版） Spring Boot实战 Spring微服务实战 Word Excel PPT 2016入门与提高 编程珠玑（第2版） 操作系统真象还原 大型网站系统与Java中间件实践 高性能JavaScript 高性能MySQL（第3版） 机器学习 极简思维 技术运营 人性的弱点 设计模式解析（第2版） 设计模式之禅 深度思维 深入理解Java虚拟机 深入浅出Node.js 深入浅出React和Redux 移动Web前端高效开发实战 亿级流量网站架构核心技术 原则 怎样管精力就怎样过一生 重构改善既有代码的设计 Excel高效办公：数据处理与分析（修订版） 项目管理艺术 周鸿祎自述 我的互联网方法论 奇点临近 高效能人士的七个习惯 三板斧：阿里巴巴管理之道 掘金移动互联：跨境电商如何挑战海外市场 微信思维 系统之美:决策者的系统思考 思考，快与慢 创新者的窘境 微服务设计 Scrum敏捷软件开发 管理的实践"},{"title":"标签","date":"2019-10-21T15:35:35.000Z","updated":"2019-10-21T15:35:54.071Z","comments":true,"path":"tags/index.html","permalink":"http://luxiaowan.github.io/tags/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2019-10-21T15:42:43.000Z","updated":"2019-10-21T15:57:01.607Z","comments":true,"path":"links/index.html","permalink":"http://luxiaowan.github.io/links/index.html","excerpt":"","text":"本人还没友链，孤独小客"},{"title":"分类","date":"2019-10-21T15:29:38.000Z","updated":"2019-10-21T15:32:44.295Z","comments":true,"path":"categories/index.html","permalink":"http://luxiaowan.github.io/categories/index.html","excerpt":"","text":""},{"title":"个人项目","date":"2019-10-21T15:54:25.000Z","updated":"2019-10-21T15:57:51.353Z","comments":true,"path":"repository/index.html","permalink":"http://luxiaowan.github.io/repository/index.html","excerpt":"","text":"https://github.com/luxiaowan"}],"posts":[{"title":"Kafka生产者","slug":"Kafka生产者","date":"2020-04-16T16:46:00.000Z","updated":"2020-04-16T20:18:28.216Z","comments":true,"path":"2020/04/17/Kafka生产者/","link":"","permalink":"http://luxiaowan.github.io/2020/04/17/Kafka生产者/","excerpt":"","text":"简 在kafka中把产生消息的一方称为生产者（Producer），尽管消息的产生非常简单，但是消息的发送过程比较复杂 发送消息从创建一个ProducerRecord对象开始，此类是kafka中的一个核心类，表示kafka需要发送的K-V键值对，记录了要发送的topic、partition、key、value、timestamp等 12345678public class ProducerRecord&lt;K, V&gt; &#123; private final String topic; private final Integer partition; private final Headers headers; private final K key; private final V value; private final Long timestamp;&#125; 在发送ProducerRecord的时候需要将对象序列化为字节数组，便于在网络上传输，之后消息达到分区器，若发送过程中指定了分区号，也就是partition，则在发送消息的时候将使用指定的分区，若发送过程中未制定分区，则根据topic和cluster中的partition数量顺序选择一个分区进行发送，分区选择器由接接口org.apache.kafka.clients.producer.Partitioner的实现类指定。 org.apache.kafka.clients.producer.KafkaProducer 1234567private int partition(ProducerRecord&lt;K, V&gt; record, byte[] serializedKey, byte[] serializedValue, Cluster cluster) &#123; Integer partition = record.partition(); return partition != null ? partition : partitioner.partition( record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);&#125; org.apache.kafka.clients.producer.internals.DefaultPartitioner 123456789101112131415161718192021// 选取分区public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if (keyBytes == null) &#123; // 顺序index int nextValue = nextValue(topic); List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic); if (availablePartitions.size() &gt; 0) &#123; // 取模 int part = Utils.toPositive(nextValue) % availablePartitions.size(); return availablePartitions.get(part).partition(); &#125; else &#123; // no partitions are available, give a non-available partition return Utils.toPositive(nextValue) % numPartitions; &#125; &#125; else &#123; // hash the keyBytes to choose a partition return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &#125;&#125; ProducerRecord内关联的时间戳timestamp，如果用户未指定，则使用KafkaProducer内的time的时间作为时间戳，但是kafka最终使用的时间戳取决于topic配置的时间戳类型： topic为CreateTime，则消息记录中的时间戳由broker使用 topic为LogAppendTime，则消息记录中的时间戳会在追加到日志中时由broker重写 消息被放在一个记录批次里ProducerBatch，这个批次的所有消息都会被发送到相同的topic和partition上，由一个FutureRecordMetadata负责发送。 broker收到消息后会返回一个响应，如果发送正常的话，会返回一个RecordAppendResult对象，包含了topic、partition、offset、时间戳等信息，发送失败则会将失败的消息记录下来，然后后续重试发送。 org.apache.kafka.clients.producer.KafkaProducer 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788private Future&lt;RecordMetadata&gt; doSend(ProducerRecord&lt;K, V&gt; record, Callback callback) &#123; TopicPartition tp = null; try &#123; throwIfProducerClosed(); // first make sure the metadata for the topic is available ClusterAndWaitTime clusterAndWaitTime; try &#123; clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs); &#125; catch (KafkaException e) &#123; if (metadata.isClosed()) throw new KafkaException(\"Producer closed while send in progress\", e); throw e; &#125; long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs); Cluster cluster = clusterAndWaitTime.cluster; // 序列化key byte[] serializedKey; try &#123; serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key()); &#125; catch (ClassCastException cce) &#123; throw new SerializationException(\"Can't convert key of class \" + record.key().getClass().getName() + \" to class \" + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() + \" specified in key.serializer\", cce); &#125; // 序列化value byte[] serializedValue; try &#123; serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value()); &#125; catch (ClassCastException cce) &#123; throw new SerializationException(\"Can't convert value of class \" + record.value().getClass().getName() + \" to class \" + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() + \" specified in value.serializer\", cce); &#125; // 决定要发送的partition int partition = partition(record, serializedKey, serializedValue, cluster); tp = new TopicPartition(record.topic(), partition); // 设置header setReadOnly(record.headers()); Header[] headers = record.headers().toArray(); int serializedSize = AbstractRecords.estimateSizeInBytesUpperBound(apiVersions.maxUsableProduceMagic(), compressionType, serializedKey, serializedValue, headers); ensureValidRecordSize(serializedSize); // 设置消息时间戳 long timestamp = record.timestamp() == null ? time.milliseconds() : record.timestamp(); log.trace(\"Sending record &#123;&#125; with callback &#123;&#125; to topic &#123;&#125; partition &#123;&#125;\", record, callback, record.topic(), partition); // producer callback will make sure to call both 'callback' and interceptor callback Callback interceptCallback = new InterceptorCallback&lt;&gt;(callback, this.interceptors, tp); // 事务 if (transactionManager != null &amp;&amp; transactionManager.isTransactional()) transactionManager.maybeAddPartitionToTransaction(tp); // 发送消息，见下方代码 RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, headers, interceptCallback, remainingWaitMs); if (result.batchIsFull || result.newBatchCreated) &#123; log.trace(\"Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch\", record.topic(), partition); this.sender.wakeup(); &#125; return result.future; &#125; catch (ApiException e) &#123; log.debug(\"Exception occurred during message send:\", e); if (callback != null) callback.onCompletion(null, e); // 记录错误信息 this.errors.record(); this.interceptors.onSendError(record, tp, e); return new FutureFailure(e); &#125; catch (InterruptedException e) &#123; this.errors.record(); this.interceptors.onSendError(record, tp, e); throw new InterruptException(e); &#125; catch (BufferExhaustedException e) &#123; this.errors.record(); this.metrics.sensor(\"buffer-exhausted-records\").record(); this.interceptors.onSendError(record, tp, e); throw e; &#125; catch (KafkaException e) &#123; this.errors.record(); this.interceptors.onSendError(record, tp, e); throw e; &#125; catch (Exception e) &#123; this.interceptors.onSendError(record, tp, e); throw e; &#125;&#125; org.apache.kafka.clients.producer.internals.RecordAccumulator 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public RecordAppendResult append(TopicPartition tp, long timestamp, byte[] key, byte[] value, Header[] headers, Callback callback, long maxTimeToBlock) throws InterruptedException &#123; appendsInProgress.incrementAndGet(); ByteBuffer buffer = null; if (headers == null) headers = Record.EMPTY_HEADERS; try &#123; Deque&lt;ProducerBatch&gt; dq = getOrCreateDeque(tp); synchronized (dq) &#123; if (closed) throw new KafkaException(\"Producer closed while send in progress\"); RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq); if (appendResult != null) return appendResult; &#125; byte maxUsableMagic = apiVersions.maxUsableProduceMagic(); int size = Math.max(this.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(maxUsableMagic, compression, key, value, headers)); log.trace(\"Allocating a new &#123;&#125; byte message buffer for topic &#123;&#125; partition &#123;&#125;\", size, tp.topic(), tp.partition()); // 申请一个缓冲区，将消息数据写入到缓冲区中 buffer = free.allocate(size, maxTimeToBlock); synchronized (dq) &#123; if (closed) throw new KafkaException(\"Producer closed while send in progress\"); RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq); if (appendResult != null) &#123; return appendResult; &#125; MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic); // 将消息分批处理 ProducerBatch batch = new ProducerBatch(tp, recordsBuilder, time.milliseconds()); FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, headers, callback, time.milliseconds())); dq.addLast(batch); incomplete.add(batch); // 清空缓冲区 buffer = null; return new RecordAppendResult(future, dq.size() &gt; 1 || batch.isFull(), true); &#125; &#125; finally &#123; if (buffer != null) free.deallocate(buffer); appendsInProgress.decrementAndGet(); &#125;&#125; 消息发送类型 简单发送 kafka最简单的消息发送是只指定topic和key及value，分区及时间戳均使用默认值，send()方法会返回一个Future&lt;RecordMetadata&gt;对象，如果不需要关心返回值，则可以忽略这个返回值，否则必须关注此值，方法返回的异常信息可能有InterruptedException(发送线程中断异常)，BufferExhaustedException(缓冲区已满)，SerializationException(序列化异常) 123ProducerRecord&lt;String,String&gt; record = new ProducerRecord&lt;&gt;(\"cc_test\",\"cc\",\"chuanchuan\");producer.send(record); 同步发送 第一种简单发送方式的前提是我们不在意发送的结果，但是我们在正常的情况下都会等待broker的反馈。我们从发送的源码中看到send()方法返回的Future&lt;RecordMetadata&gt;对象，我们可以调用Future的get()方法阻塞主线程等待broker的响应，如果返回错误，则我们调用get()方法的时候会抛出异常，如果没发生异常，则顺利获取到RecordMetadata对象，使用该对象查看消息的详细信息：topic、key和value的序列化后的大小、offset、partition。 生产者发送过程中一般会出现两类错误：一类可以通过重试解决，一类无法通过重试解决。比如连接错误、无Leader错误等都可以通过重试来实现，而消息过大这类错误KafkaProducer会直接抛出异常，不会重试，因为不管重试多少次都是消息过大。 1234567ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(\"cc_test\", \"cc\", \"chuanchuan\");try&#123; RecordMetadata rm = producer.send(record).get(); System.out.println(rm.offset());&#125; catch(Exception e) &#123; log.error(\"occur error\", e);&#125; 异步发送 消息同步发送会造成同一时间只能有一条消息\u0013\u0013在发送中，在其有返回之前，其他的消息都需要一直等待，这样会造成消息堵塞滞后，无法让kafka发挥更大的效益，若一个消息发送需要20ms，发送五十条消息就需要1s，如果我们使用异步这种方式，那么发送五十条可能只需要30ms，甚至更少。异步发送的原理是在我们调用send()方法时传入一个接口org.apache.kafka.clients.producer.Callback的实现类的对象，由ProducerBatch的私有方法completeFutureAndFireCallbacks完成回调 12345678910ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(\"cc_test\", \"cc\", \"chuanchuan\");producer.send(record, );class CcProducerCallback implements Callback &#123; public void onCompletion(RecordMetadata metadata,Exception exception)&#123; if(exception != null)&#123; exception.printStackTrace(); &#125; &#125;&#125; org.apache.kafka.clients.producer.internals.ProducerBatch 1234567891011121314151617181920212223private void completeFutureAndFireCallbacks(long baseOffset, long logAppendTime, RuntimeException exception) &#123; produceFuture.set(baseOffset, logAppendTime, exception); // execute callbacks for (Thunk thunk : thunks) &#123; try &#123; // 发生异常 if (exception == null) &#123; RecordMetadata metadata = thunk.future.value(); if (thunk.callback != null) thunk.callback.onCompletion(metadata, null); &#125; else &#123; // 正常 if (thunk.callback != null) thunk.callback.onCompletion(null, exception); &#125; &#125; catch (Exception e) &#123; log.error(\"Error executing user-provided callback on message for topic-partition '&#123;&#125;'\", topicPartition, e); &#125; &#125; produceFuture.done();&#125; 分区机制 kafka对于数据的读写是以partition为粒度的，partition可以分布在不同的broker上，每个节点都可以独立的实现消息的读写，并且能够通过新增新的broker来提升kafka集群的吞吐量，partition部署在多个broker来实现负载均衡。 kafka的分区策略其实指的就是Producer将消息发送到哪个分区的算法，kafka提供了默认的分区策略，同时也支持我们自定义分区策略，所有的策略都实现于接口org.apache.kafka.clients.producer.Partitioner 1234567891011121314151617181920public interface Partitioner extends Configurable, Closeable &#123; /** * 提供消息信息计算partition * * @param topic topic名称 * @param key key名称 * @param keyBytes key序列化字节数组 * @param value value值 * @param valueBytes value序列化字节数组 * @param cluster 集群 */ public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster); /** * 关闭partitioner */ public void close();&#125; 消息发送到哪一个partition上涉及到分区选择机制，主要有顺序、随机、按key分配、自定义分配等方式，具体的实现方法就是public int partition()。 顺序轮询 顺序分配就是消息均匀的发送给每一个partition，每个partition存储一次消息，kafka的默认策略。 随机策略 随机策略可以先计算出topic的总的partition数，然后使用ThreadLocalRandom.current().nextInt()方法来获取一个小于分区总数的随机值，随机策略会导致消息分布不均匀。虽然是随机的，但是单个分区内也是有序的。 策略代码 12List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);return ThreadLocalRandom.current().nextInt(partitions.size()); key分配策略 这个策略也叫做 key-ordering策略，kafka中每条消息都会有自己的key，一旦消息被定义了 key，那么你就可以保证同一个key的所有消息都进入到相同的partition里面，因为每个partition下的消息处理都是有顺序的，所以这个策略也被称为按消息键保序策略 策略代码 123List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);// Math.abs()的原因是hashCode可能是负数return Math.abs(key.hashCode()) % partitions.size(); 自定义分配策略 自由发挥吧，只要实现Partitioner接口就成了 application.properties 12# org.apache.kafka.clients.producer.ProducerConfig类中定义了各类参数配置信息spring.kafka.properties.partitioner.class=cc.kevinlu.springboot.kafka.partitioners.CcPartitioner CcPartitioner 12345678910111213141516171819202122232425262728package cc.kevinlu.springboot.kafka.partitioners;import java.util.Map;import org.apache.kafka.clients.producer.Partitioner;import org.apache.kafka.common.Cluster;import lombok.extern.slf4j.Slf4j;@Slf4jpublic class CcPartitioner implements Partitioner &#123; @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; if (log.isDebugEnabled()) &#123; log.debug(\"&#123;&#125;------------&#123;&#125;\", topic, cluster.availablePartitionsForTopic(topic).size()); &#125; // 永远都打到partition 0上 return 0; &#125; @Override public void close() &#123; &#125; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; &#125;&#125;","categories":[{"name":"Kafka","slug":"Kafka","permalink":"http://luxiaowan.github.io/categories/Kafka/"}],"tags":[]},{"title":"Git切换回某个commit","slug":"Git切换回某个commit","date":"2020-04-16T15:45:00.000Z","updated":"2020-04-16T16:00:43.558Z","comments":true,"path":"2020/04/16/Git切换回某个commit/","link":"","permalink":"http://luxiaowan.github.io/2020/04/16/Git切换回某个commit/","excerpt":"","text":"原由 commit之后忘了push，然后就revert HEAD了，导致本地的代码丢失了刚修改的内容 第一步 1git log 使用该命令查看commit记录 格式commit commit_id，比如commit bc208f03c3bb341dfc56533d9ea196b6d347ff34中，bc208f03c3bb341dfc56533d9ea196b6d347ff34就是commit_id，每一次commit的id都是全局唯一的 第二步 1git reset --hard $&#123;commit_id&#125; 若想切换回jmm这次的commit，则语句为git reset --hard 81fc9404e8186d132c799ffaf62e652a4c8c98f0 总结 git操作要慎重，不过即使出了问题也有恢复的小技巧","categories":[{"name":"Git","slug":"Git","permalink":"http://luxiaowan.github.io/categories/Git/"}],"tags":[]},{"title":"Kafka初探","slug":"Kafka初探","date":"2020-04-15T18:51:00.000Z","updated":"2020-04-16T16:00:26.695Z","comments":true,"path":"2020/04/16/Kafka初探/","link":"","permalink":"http://luxiaowan.github.io/2020/04/16/Kafka初探/","excerpt":"","text":"基本名词 消息：kafka中的数据单元称为消息，也可以叫记录，相当于MySQL表中的一条记录 批次：为了提高效率，kafka可以一次性写入一批数据(消息)，批次指的就是一组消息 主题：相当于MySQL的表，一个主题(Topic)代表着一类消息，kafka使用主题对消息分类 分区：分区(partition)归属于主题，一个主题可以划分为若干个分区，分区可以分布在不同的broker上，也可以在同一个broker上，使用分区来实现kafka的伸缩性。主题的单个分区上的消息是有序的，但是不同分区上的消息无法保证有序。 生产者：向主题发布消息的客户端称为生产者，生产者用于不断的向主题发送消息 消费者：订阅主题消息的客户端称为消费者，消费者用于处理生产者生产的消息 消费者群组：生产者与消费者的关系是一对多，比如一个客服对应多个咨询者，消费者群组就是由一批消费者组成的 偏移量：偏移量(Consumer Offset)是一种源数据，是一个单向递增的整数标识，用于记录消费者发生重平衡时的位置，以便用来恢复数据 broker：一个独立的服务器被称为broker，broker接收来自生产者的消息，并为消息设置偏移量，并提交消息持久化到磁盘 broker集群：多个broker组成一个集群，保证kafka的高可用，每个集群中都有一个broker充当集群Leader的角色 副本：kafka中消息的备份又称为副本(Replica)，副本的数量是可配置的，类型有Leader和Follower两种，Leader对外提供服务，Follower辅助 重(chong)平衡(ReBalance)：若消费者组内某个消费者宕了，其他存活的消费者自动重新分配订阅主题分区，kafka高可用的必备能力 关系介绍 Topic&amp;Partition Topic是kafka中给消息分类的标记，一个消息必定属于一个Topic，一个Topic可以包括一个或多个Partition，Partition又可以有多个副本，副本又可以分配在不同的broker上。 Partition内部是有序的，Partition之间是无序的 内部存储是以append-log的方式不断进行log文件尾部追加，文件读写是在磁盘上是顺序的，效率极高，媲美内存操作，每一条log对应一个offset，可以把offset理解为一个数组的下标，通过这个下标就可以读取对应的消息数据，Partition只负责为消息分配offset，消费者具体由哪个offset开始消费消息完全由消费者自己控制，也就是kafka服务端只负责提供数据，消费者自己控制消息消费进度。 kafka虽然是可以持久化消息，并且不删除已经被消费过的消息，但消息也不是被永久存储在磁盘上的，为了防止磁盘长期被消息写入数据日积月累，kafka提供两种旧数据淘汰策略： 开启数据清理：log.cleaner.enable=true，默认关闭状态 基于时间：log.retention.hours=168，单位：小时；log.retention.ms=100，单位：毫秒；log.retention.minutes，单位：分钟 基于文件大小：log.retention.bytes=1073741824，单位：字节 Consumer&amp;Consumer Group 一个消费者组由一个或多个消费者组合而成，每一条消息只会被同一个group中的一个消费者消费，但是不同group中的消费者可以同时消费同一条消息，保证了 消息队列中的消息只被消费一次；kafka是发布订阅模式的消息队列，这里订阅的是消费者组，而不是特定的一个消费者实例。 kafka支持离线处理和实时处理，所以我们可以使用Hadoop进行离线处理，也可以使用Storm这种实时流处理系统进行实时处理，还可以将数据实时的同步到其他的数据中心，前提是这些消费者处于不同的消费者组。 可以测试一下： 12345678910111213141516171819202122# 1. 创建一个topickafka-topics --create --zookeeper localhost:2181 --replication-factor 3 --partition 1 --topic cc_topic# 2. 启动一个Producerkafka-console-producer --broker-list localhost:9092 --topic cc_topic# 3. 启动五个Consumer，1~3号放cc_group_1，4~5放cc_group_2kafka-console-consumer --bootstrap-server localhost:9092 --group cc_group_1 --from-beginning --topic cc_topickafka-console-consumer --bootstrap-server localhost:9092 --group cc_group_1 --from-beginning --topic cc_topickafka-console-consumer --bootstrap-server localhost:9092 --group cc_group_1 --from-beginning --topic cc_topickafka-console-consumer --bootstrap-server localhost:9092 --group cc_group_2 --from-beginning --topic cc_topickafka-console-consumer --bootstrap-server localhost:9092 --group cc_group_2 --from-beginning --topic cc_topic# 4. 发送一条消息123# 5. 查看消费者cc_group_1和cc_group_2各自收到123这条消息一次 Consumer ReBalance Consumer ReBalance是通过Zookeeper实现，kafka保证了同一个消费者组中只能有一个消费者消费某条消息，其实kafka保证的是在稳定状态下每一个消费者都只会消费一个或多个Partition的消息，而某一Partition的消息仅会被一个消费者消费，这样设计的优势是每个消费者不用跟所有的broker进行通信，减少了通信开销，劣势是同一个消费组内的消费者不能均匀消费，而且单个Partition内部的数据是有序的，所以对于单个消费者来说，其消费的消息是有序的。 Consumer &lt; Partition：会出现某些Consumer消费多个Partition的数据 Consumer &gt; Partition：会出现某些Consumer没有可消费的Partition Consumer = Partition：一个Consumer消费一个Partition，均匀 特性 高吞吐、低延迟：kafka处理消息的速度非常快，每秒几乎可以处理几十万条消息，并且最低延迟只有几毫秒 高伸缩性：每个topic都能有多个partition，每个partition又可以分布在不同的broker上 高并发：能够同时支持数千个客户端进行读写 容错性：允许集群中的某些节点失败，某个节点宕机，kafka仍然可用继续提供服务 持久性、可靠性：kafka的消息存储是基于Zookeeper的，Zookeeper是可以将消息持久化到磁盘上，并且支持数据备份，所以kafka是一个非常可靠的可持久化消息中间件 速度快：kafka采用零拷贝的模式实现数据的快速移动，避免了内核空间和用户空间的频繁切换，kafka可以批量发送数据，从生产者到文件系统到消费者；数据压缩可以通过有效的数据压缩减少IO次数，并且采用顺序读写的方式避免寻址造成的消耗。总结起来就是零拷贝、顺序读写、数据压缩、分批发送。 消息队列 点对点（一对一）：一个生产者所生产的消息只会被一个消费者进行消费，不会同时被多个消费者消费 发布订阅（一对多、多对多）：一个或多个生产者所生产的消息会被多个消费者同时消费 架构体系 一个kafka集群包含若干个Producer、若干Consumer group、若干broker和Zookeeper集群组成，kafka通过Zookeeper管理Partition，选举Leader，以及在Consumer发生变化时通过Zookeeper进行ReBalance。Producer将消息push到Partition，Consumer通过pull将消息从Partition拉取到本地。 API kafka目前提供五类常用的API，主要有Producer、Consumer、Stream、Connect、Admin API： Producer API：允许App作为Producer将消息发送到kafka集群的一个或多个topic上 Consumer API：允许App作为Consumer从kafka集群上的一个或多个topic拉取消息 Stream API：允许App作为流处理器，从一个或多个topic中消费输入流并转化为输出流 Connector API：允许将现有的应用程序或存储系统连接到kafka的topic，充当Producer或Consumer Admin API：允许管理和检查topic、broker和kafka的其他内容 重要配置参数 kafka的参数配置文件是server.properties broker.id：每个broker都有一个唯一标识，就像是MySQL表中的主键ID，默认值是0，这个值在kafka集群中必须是唯一不可重复的，值随意设置。 port：kafka broker的默认端口是9092，若未指定port参数，则就是9092，修改port参数可以是任意端口，但是最好不要低于1024，不然就需要管理员权限启动了 zookeeper.connect：设置broker源数据的Zookeeper地址，参数的值可以设置一个或多个，多个zk通过逗号分隔，比如zk1:port,zk2:port,zk3:port，不同的kafka集群可以使用同一个zk集群，可以通过指定zk的具体path来区分每个kafka的使用，比如kafka cluster1使用zk:port/path1，kafka cluster2使用zk:port/path2。 zookeeper.connection.timeout.ms：设置broker连接Zookeeper的超时时间，单位是毫秒 log.dirs：kafka把所有的消息都保存在本地磁盘上，保存的日志地址通过该参数指定，可以指定多个存储目录，通过逗号分隔，例如/home/kafka/1,/home/kafka/2,/home/kafka/3 auto.create.topic.enable：默认为true，允许随意的创建topic，参数为true时，使用Producer往一个不存在的topic发送消息时会自动创建topic、使用Consumer从一个不存在的topic拉取消息时自动创建topic、主动创建topic、当任意一个客户端向topic发送元数据请求时。此值建议在生产上设置为false，topic由人工进行分配，防止生产环境出现各种乱七八糟的topic。 topic相关参数 num.partitions：主题拥有的Partition数量，若在创建topic的时候未指定分区数量，则使用该参数的值，默认为1。在运行过程中，分区数量可以增加不能减少，在创建时可以通过--partition指定个数 default.replication.factor：kafka消息的默认副本数，默认为1，只有在自动创建topic的时候才有效，在创建时可以通过--replication-factor指定 log.cleaner.enable：是否开启日志清理功能，默认为true，清理方式有时间和日志文件大小两种方式 log.retention.hours：设置kafka消息保存的时间，默认为168个小时，还可以通过log.retention.ms和log.retention.minutes来设置清理时间的毫秒和分钟时间 log.retention.bytes：设置topic的每个Partition所能保存的数据量，比如若一个topic有10个Partition，此参数的值为1G，那么该topic的最大存储容量为8G，topic的容量随着Partition的增加而增加。 log.segment.bytes：设置日志文件的最大的容量大小。当消息到达broker时，会被追加到日志文件中，但是如果日志片段的当前大小加上新接收消息的打小后超过了该参数设置的值，则将新消息和后续的消息写入到一个新的日志文件中。该参数的值越小，分割的文件就越多，磁盘的写入效率就越低。 log.segment.ms：除了待日志文件大小超值后重新分配新文件之外，还可以通过日志创建时间来控制消息日志文件的生命周期，可以和log.segment.bytes同时设置，哪一个先达标使用哪一个策略，比如bytes设置为1G，ms设置为1小时，若30分钟内文件容量已达1G，则后续消息写入到新的日志文件中，若1小时内日志文件尚未达到1G，则也分配新的日志文件记录后续的消息。 log.retention.check.interval.ms：检查日志段以查看是否可以根据保留策略删除它们的时间间隔，单位：毫秒 message.max.bytes：该参数限定broker可接收的单个消息的大小，默认是1MB，如果Producer发送的消息大于此值，则broker会直接拒绝并返回错误。该参数指定的是压缩后的消息大小，消息的实际大小可能大于此值。","categories":[{"name":"Kafka","slug":"Kafka","permalink":"http://luxiaowan.github.io/categories/Kafka/"}],"tags":[]},{"title":"Redis的穿透、击穿和雪崩","slug":"Redis的穿透、击穿和雪崩","date":"2020-04-13T14:23:00.000Z","updated":"2020-04-16T15:41:38.711Z","comments":true,"path":"2020/04/13/Redis的穿透、击穿和雪崩/","link":"","permalink":"http://luxiaowan.github.io/2020/04/13/Redis的穿透、击穿和雪崩/","excerpt":"","text":"前言 不论是我们在使用Redis还是准备面试，都逃不掉一块我们必须要考虑到的内容，也是使用Redis不精细的话必定会遇到的问题，就是缓存穿透、击穿和雪崩，这三个问题严重情况下，会使服务无法继续正常使用。从名字上来看，好像雪崩最严重，眼前突然浮现出电影《攀登者》里面的画面=。= 缓存穿透 缓存穿透是指访问一个DB和Redis中必定不存在的key，如果不对这类请求进行过滤拦截的话，请求每次都会穿过Redis直接打到DB上，并且我们一般是缓存中没数据的时候去DB中取，取出来之后再放到缓存中，但这类请求所需要的数据在DB中也不存在，所以即使请求打到DB上，最终缓存中还是没有数据，在这类请求特别多的情况下，DB很快就会被拖垮，引起服务异常。 解决方案 布隆过滤器：我们可以在做事务型处理之后，将需要缓存的key放到布隆过滤器中，但是由于布隆过滤器只能保证可能存在，所以在使用过程中还是会有穿透的可能性存在，但概率极小，所以不用过多担心 短期null：此方案是在DB中查询不到数据的时候，就往Redis中设置一个短时间内就会过期的null值，比如30秒，1分钟等，不过时间还是要根据自己的业务性质来定。为什么要给不存在的key在缓存中设置一个null值？其实不一定是null，只要团队约定一个特殊字符即可，因为我们到数据库里取不出来数据，缓存里取个null（或者nil），也就代表了这个key不存在与数据库中 缓存雪崩 大量的key在同一时刻同时失效，这些key并不一定是设置了相同的时间，也可能是凑巧时间累计在一起了，恰巧大量的针对这些失效的key的请求在同一时间大量的打了进来，这时缓存全部未命中，所有的请求都透传到DB上，引起DB压力瞬间扩大数倍，极易导致DB因负载过高而崩溃，危害极大。 解决方案 加锁：对访问的key进行加锁，同时只放一个请求透传到DB，从一定程度上缓解了DB的压力，这也是缓存击穿的一种解决方案 队列：所有的请求全部塞入到队列中，依次打到DB上，这种方式能解决DB的压力，但是会给请求处理效率带来一些延迟 随机过期时间：给key设置过期时间时，在原过期时间的基础上加一个随机时间，比如3000毫秒以内的随机数，这样过期时间重复或者累计重复的可能性降低了很多，不太容易引起大量的key同时失效，并且成本较低。 缓存击穿 缓存雪崩是说的大量的key，缓存击穿说的是某一个热点key，也就是在某些时间点会被超高的并发访问。key在某个时间点过期的时候，恰好在对这个Key有大量的并发请求过来，缓存中无法命中则会把请求全部打到DB端，如此大量的请求可能会瞬间把DB压爆。 解决方案 临时加锁：对key加上互斥锁，若缓存中命中不了的时候，先给这个key设置一个锁（SETNX），锁的过期时间要非常简短，只有加锁成功的线程才透传到DB，加载完数据后set到缓存中，并释放锁 12345678910111213function get(key): var v = redis.get(key) if v eq null: if setnx(key_lock, val, timeout): var v = db.get(key) redis.set(key, v, timeout) release(key_lock) return v else: sleep(30) return get(key) else: return v; 同步锁：对于热点key，set缓存的时候同时set一个针对这个key的监视key，监视key的过期时间一定要小于被监视的key，每次获取缓存数据的时候都获取一下这个监视key，并判断监视key是否过期了，如果过期了，则重置一下key的过期时间，并重新设置这个监视key 12345678910111213141516171819function get(key): var v = redis.get(key) if v eq null: if setnx(key_lock, val, timeout): var v = db.get(key) redis.set(key, v, timeout) redis.set(key_monitor, now() + timeout - time, timeout - time) release(key_lock) return v else: sleep(30) return get(key) else: var vm = redis.get(key_monitor) if now() - vm lt 10: redis.expire(key ,timeout) reids.expire(key_monitor, now() + timeout - time) else: return v 总结 若是想更好的使用Redis，那么穿透、击穿、雪崩必定是要慎重考虑的东西，解决方案有多种，应根据自己的实际业务做更优选择","categories":[{"name":"Redis","slug":"Redis","permalink":"http://luxiaowan.github.io/categories/Redis/"}],"tags":[]},{"title":"Java内存模型JMM","slug":"Java内存模型JMM","date":"2020-04-12T17:11:00.000Z","updated":"2020-04-12T17:13:19.959Z","comments":true,"path":"2020/04/13/Java内存模型JMM/","link":"","permalink":"http://luxiaowan.github.io/2020/04/13/Java内存模型JMM/","excerpt":"","text":"","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[]},{"title":"查看字节码Javap","slug":"查看字节码javap","date":"2020-04-12T16:45:00.000Z","updated":"2020-04-12T16:51:46.838Z","comments":true,"path":"2020/04/13/查看字节码javap/","link":"","permalink":"http://luxiaowan.github.io/2020/04/13/查看字节码javap/","excerpt":"","text":"代码 12345678910111213141516171819public class JavaP &#123; public static void main(String[] args) &#123; String s1 = \"Hello\"; String s2 = \"Hel\" + \"lo\"; String s3 = new String(\"Hello\"); String s4 = \"Hel\" + new String(\"lo\"); String s5 = s3.intern(); int i1 = 0; int i2 = 1; System.out.println(s1 == s2); System.out.println(s1 == s3); System.out.println(s3 == s4); System.out.println(s2 == s4); System.out.println(s1 == s2); System.out.println(s2 == s5); System.out.println(i1 == i2); &#125;&#125; 查看字节码 1javap -v JavaP.class 字节码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566Constant pool:（常量池）#2 = String #36 // Hello#3 = Class #37 // java/lang/String#5 = Class #39 // java/lang/StringBuilder#7 = String #40 // Hel#9 = String #42 // lo&#123; stack=4, locals=8, args_size=1 0: ldc #2 // String Hello （加载常量池的#2号字符串压入栈） 2: astore_1 (将上一步加载的引用变量赋值给变量1，即s1【参照LocalVariableTable的slot数值】) 3: ldc #2 // String Hello （加载常量池的#2号字符串压入栈） 5: astore_2 (将上一步加载的引用变量赋值给变量2，即s2【参照LocalVariableTable的slot数值】) 6: new #3 // class java/lang/String (创建#3号的实例：java/lang/String) 9: dup （复制上一步创建的对象的引用压入栈） 10: ldc #2 // String Hello （加载常量池的#2号字符串压入栈） 12: invokespecial #4 // Method java/lang/String.\"&lt;init&gt;\":(Ljava/lang/String;)V (调用String的init方法，将上一步加载入栈的引用作为参数传入init方法) 15: astore_3 （将上一步返回的引用变量赋值给变量3，即s3【参照LocalVariableTable的slot数值】) 16: new #5 // class java/lang/StringBuilder（创建#5号的实例：java/lang/StringBuilder） 19: dup （复制上一步创建的对象引用压入栈） 20: invokespecial #6 // Method java/lang/StringBuilder.\"&lt;init&gt;\":()V （调用StringBuilder的init方法） 23: ldc #7 // String Hel （加载常量池#7号字符串压入栈） 25: invokevirtual #8 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder;（调用StringBuilder的append方法，将上一步加载的引用作为参数传入append方法） 28: new #3 // class java/lang/String （创建#3号的实例：java/lang/String） 31: dup （复制上一步创建的对象的引用压入栈） 32: ldc #9 // String lo （加载常量池#9号字符串压入栈） 34: invokespecial #4 // Method java/lang/String.\"&lt;init&gt;\":(Ljava/lang/String;)V （调用String的init方法，将上一步加载入栈的引用作为参数传入init方法） 37: invokevirtual #8 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder;（调用StringBuilder的append方法，将上一步加载的引用作为参数传入append方法） 40: invokevirtual #10 // Method java/lang/StringBuilder.toString:()Ljava/lang/String;（调用StringBuilder的toString方法） 43: astore 4 （将上一步返回的引用赋值给变量4，即s4【参照LocalVariableTable的slot数值】） 45: aload_3 （加载变量3压入栈） 46: invokevirtual #11 // Method java/lang/String.intern:()Ljava/lang/String; （调用String的intern方法） 49: astore 5 （将上一步返回的引用赋值给变量5，即s5【参照LocalVariableTable的slot数值】） 51: iconst_0 （将数字0压入栈） 52: istore 6 （将上一步加载的数字赋值给变量6，即i1【参照LocalVariableTable的slot数值】） 54: iconst_1 （将数字1压入栈） 55: istore 7 （将上一步加载的数字赋值给变量7，即i2【参照LocalVariableTable的slot数值】） 57: getstatic #12 // Field java/lang/System.out:Ljava/io/PrintStream; （调用System的静态方法out） 60: aload_1 （加载变量1，即s1） 61: aload_2 （加载变量2，即s2） 62: if_acmpne 69（比较s1和s2是否不相等，如果不相等，跳转到63行指令，如果相等则继续下一步，if_acmpne和if_icmpne，a表示对象引用比较，I表示数字比较；if_acmpeq和if_acmpne，eq【equal】表示相等，ne【not equal】） 65: iconst_1 （将数字1压入栈） 66: goto 70 （跳转到64行指令） 69: iconst_0 （将数字0压入栈） 70: invokevirtual #13 // Method java/io/PrintStream.println:(Z)V …… 156: getstatic #12 // Field java/lang/System.out:Ljava/io/PrintStream; 159: iload 6 （加载变量6，即i1） 161: iload 7 （加载变量7，即i2） 163: if_icmpne 170（比较i1和i2） 166: iconst_1 167: goto 171 170: iconst_0 171: invokevirtual #13 // Method java/io/PrintStream.println:(Z)V …… 150: return LocalVariableTable: Start Length Slot Name Signature 0 151 0 args [Ljava/lang/String; 3 148 1 s1 Ljava/lang/String; 6 145 2 s2 Ljava/lang/String; 16 135 3 s3 Ljava/lang/String; 45 106 4 s4 Ljava/lang/String; 51 100 5 s5 Ljava/lang/String; 54 121 6 i1 I 57 118 7 i2 I&#125;","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[]},{"title":"GC日志分析","slug":"GC日志分析","date":"2020-04-12T16:40:00.000Z","updated":"2020-04-12T16:46:14.671Z","comments":true,"path":"2020/04/13/GC日志分析/","link":"","permalink":"http://luxiaowan.github.io/2020/04/13/GC日志分析/","excerpt":"","text":"VM参数 1、在控制台打印出每次GC信息： 1-XX:+PrintGCDetails 2、在发生OOM时打印出堆栈信息： 1-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/Users/chuan/ OOM代码 发生OOM测试程序代码，也可以自行编写，运行VM参数（-Xms1m -Xmx1m -XX:+PrintGCDetails -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/Users/chuan/） 123456789101112131415161718192021222324public class OOMDemo &#123; private static final int K = 1024; private static final int M = K * K; private static final int G = K * M; private static final int ALIVE_OBJECT_SIZE = 32 * M; public static void main(String[] args) &#123; int length = ALIVE_OBJECT_SIZE / 64; ObjectOf64Bytes[] array = new ObjectOf64Bytes[length]; for (long i = 0; i &lt; G; i++) &#123; array[(int) (i % length)] = new ObjectOf64Bytes(); &#125; &#125;&#125;class ObjectOf64Bytes &#123; long placeholder0; long placeholder1; long placeholder2; long placeholder3; long placeholder4; long placeholder5;&#125; Minor GC日志 [GC --[PSYoungGen: 447K-&gt;447K(1024K)] 709K-&gt;956K(1536K), 0.0031459 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 下面来解析这一段信息，其实很简单，不用硬记，按顺序从左到右： GC：表示这是发生了GC PSYoungGen：GC类型，此处表示新生代内存发生了GC； 447K-&gt;447K：-&gt;前面的447K表示GC前新生代内存占用，-&gt;后面的447K表示GC后新生代内存占用（此处是分毫未被回收，那你执行个骡子，浪费 0.0031459秒）； (1024K)：1024K表示新生代总共大小，看得出来是1M，可以回头看一下VM参数配置； 709K-&gt;956K：709K表示GC前JVM堆内存使用，956表示GC后JVM堆内存使用（硌老子的，GC后还变多了）； (1536K):1536K表示JVM堆内存总大小； 0.0031459 secs：本次GC耗时（看得出还是很快的）； user：GC用户耗时； sys：GC系统耗时； real：GC实际耗时。 Full GC日志 [Full GC [PSYoungGen: 447K-&gt;197K(1024K)] [ParOldGen: 508K-&gt;494K(512K)] 956K-&gt;692K(1536K), [Metaspace: 3179K-&gt;3179K(1056768K)], 0.0069504 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] Full GC：GC类型； [PSYoungGen: 447K-&gt;197K(1024K)] ：Young区，447K表示GC前Young区内存占用，197K表示GC后Young区内存占用，1024K表示Young区总大小； [ParOldGen: 508K-&gt;494K(512K)] ：Old区，508K表示GC前Old区内存占用，494K表示GC后Old区内存占用，512K表示Old区总大小； 956K-&gt;692K(1536K)：956K表示GC前JVM堆内存使用，692K表示GC后JVM堆内存使用，1536K表示JVM堆内存总大小； [Metaspace: 3179K-&gt;3179K(1056768K)]：元空间，3179K表示GC前后元空间的使用，1056768K表示元空间的总大小（使用VM参数 -XX:-UseCompressedClassPointers -XX:MetaspaceSize=50M指定元空间的总大小）； 0.0069504 secs：本次GC耗时（看得出还是很快的）； user：GC用户耗时； sys：GC系统耗时； real：GC实际耗时。 规律 [名称:GC前内存-&gt;GC后内存(总内存)] 12345678HeapPSYoungGen total 2560K, used 60K [0x00000000ffd00000, 0x0000000100000000, 0x0000000100000000)eden space 2048K, 2% used [0x00000000ffd00000,0x00000000ffd0f038,0x00000000fff00000)from space 512K, 0% used [0x00000000fff00000,0x00000000fff00000,0x00000000fff80000)to space 512K, 0% used [0x00000000fff80000,0x00000000fff80000,0x0000000100000000)ParOldGen total 7168K, used 736K [0x00000000ff600000, 0x00000000ffd00000, 0x00000000ffd00000)object space 7168K, 10% used [0x00000000ff600000,0x00000000ff6b8128,0x00000000ffd00000)Metaspace used 3328K, capacity 4112K, committed 4352K, reserved 8192K 解析上述： PSYoungGen total = eden space + from space，内存地址eden+from+to ParOldGen GC前后老年代的内存变化","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[]},{"title":"MySQL连接查询","slug":"MySQL连接查询","date":"2020-04-12T15:36:00.000Z","updated":"2020-04-12T17:08:48.838Z","comments":true,"path":"2020/04/12/MySQL连接查询/","link":"","permalink":"http://luxiaowan.github.io/2020/04/12/MySQL连接查询/","excerpt":"","text":"左连接 左连接基本格式为A left join B on A.key=B.key，比如以下语句： 1select * from A left join B on A.id=B.id; 如果A、B表的数据结构为： table A table B id, name id, name 1, xiaolu 1, xiaolu 2, chuanchuan 3, chuanchuan 这时执行上述语句会得出如下结果： A.id A.name B.id B.name 1 Xiaolu 1 Xiaolu 2 chuanchuan null null 由上述结果可以看出，左连接是以左表为坐标，首先将A表中所有的数据列出来，然后根据on的匹配条件查出B表中的数据并将数据列在A表数据后面，如果在B表中没有与A表中匹配的数据，则显示为null，查询出的总数据数为A表中的数据条目个数。 右连接 右连接基本格式为A right join B on A.key=B.key，比如以下语句： 1select * from A right join B on A.id=B.id; 如果A、B表的数据结构为： table A table B id, name id, name 1, xiaolu 1, xiaolu 2, chuanchuan 3, chuanchuan 这时执行上述语句会得出如下结果： B.id B.name A.id A.name 1 xiaolu 1 xiaolu null null 3 chuanchuan 由上述结果可以看出，左连接是以左表为坐标，首先将B表中所有的数据列出来，然后根据on的匹配条件查出A表中的数据并将A表的数据列在B表的前面，如果在A表中没有与B表中匹配的数据，则显示为null，查询出的总数据数为B表中的数据条目个数。 内链接 内连接基本格式为A inner join B on A.key=B.key，比如以下语句： 1select * from A inner join B on A.id=B.id; 如果A、B表的数据结构为： table A table B id, name id, name 1, xiaolu 1, xiaolu 2, chuanchuan 3, chuanchuan 这时执行上述语句会得出如下结果： B.id B.name A.id A.name 1 xiaolu 1 xiaolu","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/categories/MySQL/"}],"tags":[]},{"title":"MySQL联合索引底层数据结构","slug":"MySQL联合索引底层数据结构","date":"2020-04-12T07:36:00.000Z","updated":"2020-04-12T10:39:05.585Z","comments":true,"path":"2020/04/12/MySQL联合索引底层数据结构/","link":"","permalink":"http://luxiaowan.github.io/2020/04/12/MySQL联合索引底层数据结构/","excerpt":"","text":"前言 了解MySQL索引结构的基本都知道索引BTree类型是用B+树的数据结构，单列索引的结构我们很容易理解，二级索引的每个叶子节点只存储主键关键字外的一个数据，查询起来也很容易在非叶子节点进行大小值判断，最终找到叶子节点 对于多列组合索引，存储结构也是B+树，那么非叶子节点和叶子节点都存储的是什么内容？ 二级组合索引 对于组合索引，需要遵循断桥原则(最左匹配原则)，例如(a, b,)可以满足a，a、b，我们根据这个原则反推一下二级组合索引的存储规则： 叶子节点应该是线性排列，并且每个节点的数据排列顺序和创建索引字段的顺序一致 叶子节点排列顺序应该是先按照a进行排序，排序完成后再按照b进行排序，所以应该是a是全局有序，b是a中有序，如果列数更多的情况下，下一列都相对于前列有序。 非叶子节点存储完整的索引关键字信息，排列规则和叶子节点一致 整体查询使用二分法 根据上述推断，我们基本可以判定二级组合索引的数据结构图了 上面我们进行了规则反推，也根据反推总结出了简单的组合索引数据结构图，那么我们来验证一下上述推论： 因为索引遵循断桥原则，B+树是顺序且极限二分查找的方式进行遍历，所以在进行B+树遍历的时候从左到右进行匹配，并且我们创建索引的目的是为了提升查询效率，如果每个节点中数据和索引的字段顺序不一致，各执己见，那么在查询的时候还要判断当前节点的某位数据对应了索引的哪个字段，效率会更低，所以推论1是可信可靠的。 根据图中叶子节点的数据可以看出，所有的数据都是按照列A进行排序的1、2、3、4，B列的顺序为1、2、1、5、1、5，B列全局是无序的，这就尴尬了，如果我们仅按照列B去查询，在索引中匹配的时候岂不是很麻烦，或者说压根儿就无法匹配，这也就明白了为什么仅使用到列B的时候不会走索引了，那仅使用A的时候可以走索引是因为列A在索引树中相对于全局是有序的，所以可以根据列A进行二分查找和定位。由此可见推论2也是可靠的。 B+树的非叶子节点存储的是索引关键字的数据信息，并且根据推论2的结果可以验证推论3也是正确的。 B+树是使用二分法进行查找的，所以推论4是正确的。 总结 我们根据B+树的特性、索引的断桥原则和单列索引存储特性三个方面反推组合索引的数据存储结构，并验证了我们的推论，这仅仅是串一串的推论，可能不靠谱，哈哈~","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/categories/MySQL/"}],"tags":[{"name":"索引","slug":"索引","permalink":"http://luxiaowan.github.io/tags/索引/"}]},{"title":"Redis和MySQL分布式双写一致性","slug":"Redis和MySQL分布式双写一致性","date":"2020-04-12T06:37:00.000Z","updated":"2020-04-12T14:50:23.244Z","comments":true,"path":"2020/04/12/Redis和MySQL分布式双写一致性/","link":"","permalink":"http://luxiaowan.github.io/2020/04/12/Redis和MySQL分布式双写一致性/","excerpt":"","text":"前言 一个MySQL服务和一个Redis服务，用户的数据存储持久化在MySQL中，缓存在Redis中，有请求的时候从Redis中获取缓存的用户数据，有修改则同时修改MySQL和Redis中的数据。现在问题是：不论是先保存到MySQL还是Redis，都面临着此成功彼失败的情况，那么如何保证MySQL与Redis中的数据一致？ 数据一致性 数据一致性主要出现在使用不同存储组件的情况下，存储组件之间无法直接通信，所以不能相互之间实现数据交换，但是使用第三方来单独操作各存储组建时，有极大的可能造成各存储组件之间数据不一致。 数据一致性分为强一致性和最终一致性，强一致性的情况是不论何时访问哪一个存储组件，所得到的数据都是一样的，最终一致性的情况是可以在某短时间内访问不同存储组件所得到的数据允许不一样。我们实际的生产开发中，基本都是遵循最终一致性。 MySQL和Redis一致性 Redis一般用于存储热点数据，MySQL存储所有数据。但是在更新缓存这方面有多种方案： 先删除缓存，再更新数据库 先更新缓存，再更新数据库 先更新数据库，在更新缓存 先更新数据库，再删除缓存 先更新数据库，再给缓存设置过期时间 使用Canal中间件 先删除缓存，再更新数据库（不可取） 123def method(): deleteRedis() updateDB() 第一步先删除缓存，第二步再更新数据库，如果在第一步之后，其他线程发生了数据库读操作，然后读到的旧数据又set到了Redis中，然后第二步执行，最终Redis和MySQL中的数据出现了不一致 先更新缓存，再更新数据库（不可取） 123def method(): updateRedis() updateDB() 第一步先更新缓存，第二步更新数据库，如果更新Redis成功，但是更新数据库失败，则两者的数据又出现了不一致 先更新数据库，在更新缓存（不可取） 123def method(): updateDB() updateRedis() 两个线程同时对一条数据进行操作，在线程B先于线程A更新缓存成功，则造成了缓存中的数据低于MySQL一个版本 ####先更新数据库，再删除缓存（可取） 123def method(): updateDB() deleteRedis() 两个线程同时操作一条数据，甭管哪一个先操作成功，最终都会删除掉缓存中的数据，然后由查询将最新数据set到缓存中，但是在并发量大的情况下，由于缓存被删除了两次，可能会造成缓存击穿 先更新数据库，再给缓存设置过期时间（推荐） 123def method(): updateDB() expireRedis() 更新完数据库之后，并不立即删除缓存，而是给缓存设置一个过期时间，由缓存自行过期，使用这个方案的前提是能允许数据可以很短时间内的不一致，并且推荐 使用Canal中间件（可取） Canal是阿里巴巴开源的一款数据库同步工具，他可以将自己伪装成一个MySQL Slave节点，将MySQL的binlog文件同步到Canal，然后由Canal进行下一步处理，比如存入MQ、Redis等，但是可能会存在些许延迟。后续讨论 强一致性 如果想要求数据强一致性，就只用MySQL，不要用Redis，以免出现不一致的情况","categories":[],"tags":[]},{"title":"Consul心跳机制","slug":"Consul心跳机制","date":"2020-04-11T16:26:00.000Z","updated":"2020-04-11T18:25:09.519Z","comments":true,"path":"2020/04/12/Consul心跳机制/","link":"","permalink":"http://luxiaowan.github.io/2020/04/12/Consul心跳机制/","excerpt":"","text":"前言 我们知道Eureka是通过Client向Server发送renew通知来续命，属于是&quot;去中心化&quot;的设计，而Consul是&quot;中心化&quot;设计，Consul的心跳由Server端发起 Consul心跳 Client在注册到Consul Server的时候(ConsulServiceRegistry#register)，会将客户端的注册信息全部发送给注册中心(接口：/v1/agent/service/register)，其中主要信息包括服务id、name、ip、port、health-check-url等，所以Consul Server才会知道向Client的哪个接口发送心跳。 1234567891011121314151617181920212223242526&#123; id='consul-demo-7702', name='consul-demo', tags=[ secure=false ], address='192.168.0.107', meta=null, port=7702, enableTagOverride=null, check=Check&#123; script='null', interval='10s', ttl='null', http='http://192.168.0.107:7702/health', method='null', header=&#123; &#125;, tcp='null', timeout='null', deregisterCriticalServiceAfter='null', tlsSkipVerify=null, status='null' &#125;, checks=null&#125; 这里check.http是值是我们在项目的properties文件中配置的： 123456server.port=7702spring.application.name=consul-demospring.cloud.consul.host=127.0.0.1spring.cloud.consul.port=8500# 心跳接口spring.cloud.consul.discovery.health-check-path=/health Spring Cloud Consul的心跳接口默认为actuator包中的/actuator/health，所以如果我们既没设置自定义的心跳接口，也没依赖actuator包，那么Consul Server就会在我们注册的Service上显示service checks fail 我们查看Consul Server控制台，发现会控制台健康检查语句agent: Check is now critical: check=service:consul-demo-client-18090，正常的健康检查语句是agent: Check status updated: check=service:consul-demo-7702 status=passing 默认情况下Consul会每隔10秒，通过一个HTTP接口/health来检测节点的健康情况。 如果健康检测失败，那服务实例就会被标记成critical，可以通过在检查定义中指定超时字段来配置自定义HTTP检查超时值，检查的输出限制在大约4KB，大于此值的响应将被截断，会被认为健康检查未通过。 spring.cloud.consul.discovery.prefer-ip-address参数决定上报给注册中心的健康接口是IP还是hostname 健康检查接口创建源码 ConsulAutoRegistration#createCheck 1234567891011121314151617181920212223242526272829303132public static NewService.Check createCheck(Integer port, HeartbeatProperties ttlConfig, ConsulDiscoveryProperties properties) &#123; NewService.Check check = new NewService.Check(); if (StringUtils.hasText(properties.getHealthCheckCriticalTimeout())) &#123; check.setDeregisterCriticalServiceAfter( properties.getHealthCheckCriticalTimeout()); &#125; if (ttlConfig.isEnabled()) &#123; check.setTtl(ttlConfig.getTtl()); return check; &#125; Assert.notNull(port, \"createCheck port must not be null\"); Assert.isTrue(port &gt; 0, \"createCheck port must be greater than 0\"); // 若自定义了spring.cloud.consul.discovery.health-check-url if (properties.getHealthCheckUrl() != null) &#123; check.setHttp(properties.getHealthCheckUrl()); &#125; else &#123; // 自定义了spring.cloud.consul.discovery.health-check-path或默认 check.setHttp(String.format(\"%s://%s:%s%s\", properties.getScheme(), properties.getHostname(), port, properties.getHealthCheckPath())); &#125; // spring.cloud.consul.discovery.health-check-headers check.setHeader(properties.getHealthCheckHeaders()); // 设置健康检查频率spring.cloud.consul.discovery.health-check-interval，字符串，要加上单位\"5s\" check.setInterval(properties.getHealthCheckInterval()); // 设置健康检查超时时间spring.cloud.consul.discovery.health-check-timeout，字符串，要加上单位\"5s\" check.setTimeout(properties.getHealthCheckTimeout()); check.setTlsSkipVerify(properties.getHealthCheckTlsSkipVerify()); return check;&#125; 图中Tags一栏有一个secure=false，这个是由客户端返回给Server，这个标识是检测健康检查接口是否为https协议 1234567891011121314151617public static List&lt;String&gt; createTags(ConsulDiscoveryProperties properties) &#123; List&lt;String&gt; tags = new LinkedList&lt;&gt;(properties.getTags()); if (!StringUtils.isEmpty(properties.getInstanceZone())) &#123; tags.add(properties.getDefaultZoneMetadataName() + \"=\" + properties.getInstanceZone()); &#125; if (!StringUtils.isEmpty(properties.getInstanceGroup())) &#123; tags.add(\"group=\" + properties.getInstanceGroup()); &#125; // 检查请求schema是否为https tags.add(\"secure=\" + Boolean.toString(properties.getScheme().equalsIgnoreCase(\"https\"))); return tags;&#125;","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://luxiaowan.github.io/categories/SpringBoot/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://luxiaowan.github.io/tags/SpringCloud/"}]},{"title":"Consul是什么","slug":"Consul是什么","date":"2020-04-11T07:32:00.000Z","updated":"2020-04-11T15:37:22.399Z","comments":true,"path":"2020/04/11/Consul是什么/","link":"","permalink":"http://luxiaowan.github.io/2020/04/11/Consul是什么/","excerpt":"","text":"前言 Consul是一个基于CP的轻量级分布式高可用的系统，提供服务发现、健康检查、K-V存储、多数据中心等功能，不需要再依赖其他组件(Zk、Eureka、Etcd等)。 服务发现：Consul可以提供一个服务，比如api或者MySQL之类的，其他客户端可以使用Consul发现一个指定的服务提供者，并通过DNS和HTTP应用程序可以很容易的找到所依赖的服务。 健康检查：Consul客户端提供相应的健康检查接口，Consul服务端通过调用健康检查接口检测客户端是否正常 K-V存储：客户端可以使用Consul层级的Key/Value存储，比如动态配置,功能标记,协调,领袖选举等等 多数据中心：Consul支持开箱即用的多数据中心 架构介绍 看得出一个Consul集群是由N个Server和M个Client节点组成的。 Client节点：使用的client模式，该模式下可以接收服务的注册请求，但是会把请求转发给Server节点，自身不做处理，并且不持久化在本地 Server节点：使用server模式，处理注册请求，将注册信息持久化到本地，用作故障恢复。 Server节点分为Leader和Follower两个身份，Leader负责监控Follower，同步注册信息给所有的Follower，一个集群中只能有一个Leader Server之间通过RPC消息通信，Follower不会主动发起RPC请求，只会有Leader或选举时的Candidate主动发起 Follower节点接收到RPC请求后，会将请求转发给Leader节点，由Leader节点处理后进行相应的ACK，请求分为事务型和非事务型，非事务型的请求由Leader节点直接响应，事务型的请求 集群一般推荐3或5个节点比较合适，因为Raft选举时，4和3、5和6的结果是一样的 Gossip是什么 从架构图中发现有一个Gossip，一个DC中涵盖了两个Gossip池，LAN池和WAN池，为什么会有Gossip，因为Consul是建立在Serf基础之上的，Gossip由Serf提供，Gossip是一个去中心化的协议。Consul中用Gossip维护节点关系，告知当前节点集群中还有哪些节点，其他节点的身份，是Follower还是Leader。 LAN Gossip：局域网内唯一，LAN池是用于局域网内的节点消息广播，Consul的Client和Server节点全部都在LAN池中，LAN池中的客户端可以自动发现服务器，不需要进行过多的配置，LAN池能保证快速可靠的消息传播，比如Leader选举。 WAN Gossip：WAN是全局唯一的，无论属于哪一个DC，所有Server应该加入到WAN中，由WAN提供信息让Server节点可以执行跨数据中心的请求。 工作原理 Producer启动之后，会向Consul注册中心发送一个POST请求，上报自己的id、name、ip、port、健康检查接口、心跳频率等信息 Consul接收到Producer上报的信息之后，根据上报的心跳频率和健康检查接口对Producer进行健康检查，检验Producer是否健康 Consumer启动之后，会从Consul拉取Producer的列表缓存在本地，后续的请求都会从本地选举发出，使用RestTemplate发出请求的时候，每次都会从Consul同步一下服务者信息。 Leader选举 一个DC可以有多个Server，但是只能有一个Leader，Leader基于Raft算法进行选举，在Leader选举过程中，整个集群都无法对外提供服务。 节点的身份有Follower、Candidate、Leader三种，所有的节点在初始化的时候都为Follower，节点加入到LAN Gossip池后，由Raft协议的Leader-Follower模式进行Leader选举。 概念 Leader：集群中仅有一个，处理客户端所有的请求，遇到事务型的请求时会在本地处理后再生成同步日志，由Gossip通知到各个Follower节点进行同步 Follower：所有节点的初始状态，正常集群中可以有多个Follower，不处理任何请求也不发送任何请求，只响应来自Leader和Candidate的请求，当接收到客户端发来的请求时会自动将请求转发给Leader节点处理 Candidate：Follower超过选举器时都未收到来自Leader的心跳时，自动转换身份为Candidate，并根据Raft算法执行新一轮的Leader选举 Election Time(选举超时时间)：每一个节点都维护着自己的选举计时器，这个计时器的值需要大于心跳间隔，Follower收到Leader的心跳请求后会重置选举计时器，如果这个计时器归零了，则将节点身份转换为Candidate，并向其他节点发送投票。设置选举计时器主要是为了防止因为网络抖动等问题而引起心跳消息丢失，不然可能一旦心跳丢失了就立刻进入选举 Heart time(心跳超时时间)：Leader向Follower发送心跳的时间间隔 Term(任期)：任期是一个全局递增的数字，没进行一次选举，任期数就+1，每个节点都记录该值 Raft算法 Raft是一个共识算法，也就是当大多数对某个事情都赞同的情况下执行该事情，主要为解决分布式一致性的问题。Raft算法是从Paxos的理论演变而来，Raft把问题分解成领导选举、日志复制、安全和成员变化 领导选举：集群中必须存在一个Leader节点 日志复制：Leader节点接收并处理客户端的请求，然后将这些请求序列化成日志再同步到集群中的其他节点 安全性：已经被Raft状态机记录过的数据，就不能被再次输入到Raft状态机中 Leader选举过程 选举过程 在节点刚开始启动时，初始状态是Follower状态。一个Follower状态的节点，只要一直收到来自Leader或者Candidate的正确RPC消息的话，将一直保持在Follower状态。Leader节点通过周期性的发送心跳请求（一般使用带有空数据的AppendEntries RPC来进行心跳）来维持着Leader节点状态。每个Follower同时还有一个选举超时（Election timeout）定时器，如果在这个定时器超时之前都没有收到来自Leader的心跳请求，那么Follower将认为当前集群中没有Leader了，将发起一次新的选举。 发起选举时，Follower将递增它的任期号然后切换到Candidate状态。然后通过向集群中其它节点发送RequestVote RPC请求来发起一次新的选举，一个节点将保持在该任期内的Candidate状态下。 选举过程中可能遇到的问题 该Candidate节点收到超过半数以上集群中其它节点的投票赢得选举 如果Candidate节点收到了集群中半数以上节点的投票，那么此Candidate节点将成为新的Leader。每个节点在一个任期中只能给一个节点投票，而且遵守“先来后到”的原则，这样就保证每个任期最多只有一个节点会赢得选举成为leader。 收到任期号比当前节点任期号不一致的请求 比当前节点任期号小：说明当前集群已经进入了下一轮选举，则自动拒绝收到的请求，继续保持在Candidate状态 比当前节点任期号大：说明集群中已经存在了Leader，节点从Candidate切换到Follower 选举后没有任何一个节点成为Leader 本次选举未选举出Leader，则将集群中的任期号+1，再次进行选举 SpringCloud使用Consul 安装并启动Consul Server 1./consul agent -dev 创建SpringCloud项目 服务提供方 12345server.port=7702spring.application.name=consul-demospring.cloud.consul.host=127.0.0.1spring.cloud.consul.port=8500spring.cloud.consul.discovery.health-check-path=/health 12345@GetMapping(\"/health\")public String health() &#123; System.out.println(\"----health check----\"); return \"hello consul\";&#125; 服务调用方 12server.port=18090spring.application.name=consul-demo-client 123456789101112@LoadBalanced@Beanpublic RestTemplate restTemplate() &#123; return new RestTemplate();&#125;@Resourceprivate RestTemplate restTemplate;@RequestMapping(\"/hello\")public String hello() &#123; return restTemplate.getForObject(\"http://consul-demo/health\", String.class);&#125; 两个项目启动后，查看服务注册情况：http://127.0.0.1:8500/ 访问接口测试：http://127.0.0.1:18090/hello","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://luxiaowan.github.io/categories/SpringBoot/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://luxiaowan.github.io/tags/SpringCloud/"}]},{"title":"Eureka集群搭建","slug":"Eureka集群搭建","date":"2020-04-11T05:16:00.000Z","updated":"2020-04-11T07:17:46.106Z","comments":true,"path":"2020/04/11/Eureka集群搭建/","link":"","permalink":"http://luxiaowan.github.io/2020/04/11/Eureka集群搭建/","excerpt":"","text":"Eureka集群 Eureka是基于AP的分布式服务注册中心，集群中所有的Server节点都互为对方的备份，可以把所有的节点都看作是Master节点，也可以把所有的节点都看作是Slave节点，任一节点接收到新的服务注册请求后，都会在registry完成之后同步给其他的Server节点，续约操作renew和registry的逻辑一样。 当集群中一个Server节点宕机之后，Client会把自动切换到存活的节点，由于集群中的所有Server之间是相互同步的，所以各个Server节点之间的信息是相同的，除非是在一个Client刚注册到Server1还未同步给其他节点时宕机了，会造成信息不一致的情况，但当节点将续约请求发送给其他节点时，如果这个节点没有这个Client的信息，则会重新进行registry，并同步给集群中的其他节点。 因为Eureka集群是基于AP的，所以只要有一个Server节点可用，那么整个集群就是可用的，每一个Server节点都有一个server_id属性，当节点宕机后，会选择server_id值靠的最近的Server接收宕机节点的请求。 搭建集群 Eureka Server搭建很简单，最主要的就是节点配置文件的内容 Eureka Server peer1 123456789101112server: port: 9001 #服务端口eureka: instance: hostname: peer1 client: register-with-eureka: false #是否将eureka自身作为应用注册到eureka注册中心 fetch-registry: false #为true时，可以启动，但报异常：Cannot execute request on any known server serviceUrl: defaultZone: http://peer2:9002/eureka/,http://peer3:9003/eureka/ server: enable-self-preservation: false peer2 123456789101112server: port: 9002 #服务端口eureka: instance: hostname: peer2 client: register-with-eureka: false #是否将eureka自身作为应用注册到eureka注册中心 fetch-registry: false #为true时，可以启动，但报异常：Cannot execute request on any known server serviceUrl: defaultZone: http://peer1:9001/eureka/,http://peer3:9003/eureka/ server: enable-self-preservation: false peer3 123456789101112server: port: 9003 #服务端口eureka: instance: hostname: peer3 client: register-with-eureka: false #是否将eureka自身作为应用注册到eureka注册中心 fetch-registry: false #为true时，可以启动，但报异常：Cannot execute request on any known server serviceUrl: defaultZone: http://peer1:9001/eureka/,http://peer2:9002/eureka/ server: enable-self-preservation: false Eureka Client 123456server.port=7080spring.application.name=eureka-healtheureka.client.service-url.defaultZone=http://127.0.0.1:9001/eureka/eureka.instance.health-check-url-path=/cc/healtheureka.instance.lease-renewal-interval-in-seconds=5eureka.instance.lease-expiration-duration-in-seconds=15 在客户端的defaultZone不论设置集群中的哪一个节点或哪几个节点，整个集群的所有节点都会有这个Client的注册信息，这就是因为Server节点的replicate功能。 当我们关闭掉其他Server节点，只留一个可用节点的时候，注册中心仍然可以提供服务。但如果我们在Client的defaultZone只设置一个节点的信息的话，那么在这个节点宕机之后，Client就找不到可以继续renew的节点了，最终整个集群都收不到这个Client的续约，在lease-expiration-duration-in-seconds之后，集群会将这个Client从服务列表移除，这损失可就太大了。 所以应该把Server和Client节点的defaultZone要设置齐全。Client把所有Server节点都设置进来，Server节点也把所有的Server节点都设置进来。 集群参数 12345678# Eureka Server启动时，从远程Eureka Server读取不到注册信息时，多长时间不允许Client访问，默认5分钟eureka.server.wait-time-in-ms-when-sync-empty# Eureka Server 集群节点更新频率，单位：毫秒，默认10分钟eureka.server.peer-eureka-nodes-update-interval-ms# 初始化实例信息到Eureka服务端的间隔时间，单位为秒，默认40秒eureka.client.initial-instance-info-replication-interval-seconds# 更新实例信息的变化到Eureka服务端的间隔时间，单位为秒，默认30秒eureka.client.instance-info-replication-interval-seconds","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://luxiaowan.github.io/categories/SpringBoot/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://luxiaowan.github.io/tags/SpringCloud/"}]},{"title":"Eureka心跳检测","slug":"Eureka心跳检测","date":"2020-04-10T13:18:00.000Z","updated":"2020-04-10T18:45:18.617Z","comments":true,"path":"2020/04/10/Eureka心跳检测/","link":"","permalink":"http://luxiaowan.github.io/2020/04/10/Eureka心跳检测/","excerpt":"","text":"前言 注册中心的心跳机制有两种形式：客户端主动上报和客户端被动响应。Eureka属于是主动上报类型的，Client通过renew机制频繁的向Server发送消息，通知Server它还活着，不要将其从服务列表中剔除，但是我们renew仅仅是监控Client是否存活，并不会去检测Client依赖的服务是否存活 从图中我们发现Client123和Client456两个客户端均依赖了第三方组件，并且MySQL同时宕机了。 Client123使用了Eureka自带的renew机制，renew最基础的就是调一下Server的/apps/{appName}/{instanceId}?status=&amp;lastDirtyTimestamp=接口，正常情况下Client启动后的status为UP，所以只要Client自身服务不出问题，永远都是UP，默认的指示器是CompositeHealthIndicator，默认的管理器为EurekaHealthCheckHandler； Client456通过扩展HealthIndicator接口和HealthCheckHandler接口，然后来自定义需要监控的内容 默认健康监控组件 在类DiscoveryClient#getHealthCheckHandler方法中选择需要使用的健康管理器 1234567891011121314151617public HealthCheckHandler getHealthCheckHandler() &#123; HealthCheckHandler healthCheckHandler = this.healthCheckHandlerRef.get(); if (healthCheckHandler == null) &#123; if (null != healthCheckHandlerProvider) &#123; healthCheckHandler = healthCheckHandlerProvider.get(); &#125; else if (null != healthCheckCallbackProvider) &#123; healthCheckHandler = new HealthCheckCallbackToHandlerBridge(healthCheckCallbackProvider.get()); &#125; if (null == healthCheckHandler) &#123; healthCheckHandler = new HealthCheckCallbackToHandlerBridge(null); &#125; this.healthCheckHandlerRef.compareAndSet(null, healthCheckHandler); &#125; return this.healthCheckHandlerRef.get();&#125; 方法调用流程图 自定义健康监控 自定义监控组件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556@Componentpublic class HealthPolicyBean implements InitializingBean &#123; @Resource private RedisTemplate&lt;String, Object&gt; redisTemplate; /** * 调度线程池 */ private ScheduledExecutorService scheduled = Executors.newScheduledThreadPool(1); /** * 数据库健康情况 */ public static boolean dbHealth = true; /** * Redis健康情况 */ public static boolean redisHealth = true; /** * MongoDB健康情况 */ public static boolean mongoHealth = true; @Override public void afterPropertiesSet() throws Exception &#123; // 创建调度器 ThreadPoolExecutor heartbeatExecutor = new ThreadPoolExecutor(1, 1, 0, TimeUnit.SECONDS, new SynchronousQueue&lt;&gt;(), new ThreadFactoryBuilder().setNameFormat(\"redis-HeartbeatExecutor-%d\").setDaemon(true).build()); TimedSupervisorTask task = new TimedSupervisorTask(\"redis-heartbeat\", scheduled, heartbeatExecutor, 10, TimeUnit.SECONDS, 100, new RedisTimer()); scheduled.schedule(task, 10, TimeUnit.SECONDS); &#125; /** * 监控Redis状态 */ protected class RedisTimer implements Runnable &#123; @Override public void run() &#123; try &#123; List&lt;RedisClientInfo&gt; clientList = redisTemplate.getClientList(); if (clientList == null || clientList.isEmpty()) &#123; HealthPolicyBean.redisHealth = false; &#125; else &#123; HealthPolicyBean.redisHealth = true; &#125; &#125; catch (Exception e) &#123; HealthPolicyBean.redisHealth = false; &#125; &#125; &#125;&#125; 自定义HealthIndicator 12345678910111213141516/** * Cc健康指示器 */@Componentpublic class CcHealthIndicator implements HealthIndicator &#123; @Override public Health health() &#123; if (HealthPolicyBean.dbHealth &amp;&amp; HealthPolicyBean.redisHealth &amp;&amp; HealthPolicyBean.mongoHealth) &#123; // 当所有组件都正常时才返回UP return new Health.Builder(Status.UP).build(); &#125; else &#123; return new Health.Builder(Status.DOWN).build(); &#125; &#125;&#125; 自定义HealthCheckHandler 1234567891011121314151617/** * Cc健康管理器 */@Componentpublic class CcHealthCheckHandler implements HealthCheckHandler &#123; @Autowired private CcHealthIndicator ccHealthIndicator; @Override public InstanceInfo.InstanceStatus getStatus(InstanceInfo.InstanceStatus currentStatus) &#123; if (ccHealthIndicator.health().getStatus().equals(Status.UP)) &#123; return InstanceInfo.InstanceStatus.UP; &#125; return InstanceInfo.InstanceStatus.DOWN; &#125;&#125; 方法调用流程图 区别 我们打开Redis服务，启动Eureka Server、Client123和Client456。 Redis运行中 Redis正常运行时，两个服务都处于正常情况 Redis停止 将Redis服务停掉，等待一个renew周期后，服务状态发生变化，使用默认HealthCheckHandler的CUSER-SERVICE的status仍然为UP，而我们自定义HealthCheckHandler的EUREKA-HEALTH服务的status已经变成了DOWN，符合正常要求。 总结 在实际的生产工作中，尽量不要使用默认的HealthCheckHandler，不然就算是我们项目的MySQL、Redis、MongoDB、MQ都挂掉了，只要项目的进程还存活，那么status就很大的可能是UP，但实际上项目已经无法正常提供服务了，会给我们的项目带来很大的麻烦。","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://luxiaowan.github.io/categories/SpringBoot/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://luxiaowan.github.io/tags/SpringCloud/"},{"name":"Eureka","slug":"Eureka","permalink":"http://luxiaowan.github.io/tags/Eureka/"}]},{"title":"Eureka服务注册详解","slug":"Eureka服务注册详解","date":"2020-04-10T03:00:00.000Z","updated":"2020-04-10T14:41:02.989Z","comments":true,"path":"2020/04/10/Eureka服务注册详解/","link":"","permalink":"http://luxiaowan.github.io/2020/04/10/Eureka服务注册详解/","excerpt":"","text":"前言 服务注册与发现是Spring Cloud Eureka的核心功能，首先我们需要一个Eureka Server，然后再来一个Eureka Client，那么Client的服务是怎么自动注册到Server的呢？我们都知道SpringBoot是约定大于配置的一个框架，自动配置是在启动的时候扫描/META-INF/spring.factories文件中EnableAutoConfiguration下的所有的*AutoConfiguration类，看一下eureka-client包下的spring.factories文件内容 我们主要关注两个类EurekaClientAutoConfiguration和EurekaDiscoveryClientConfiguration 在类EurekaClientAutoConfiguration的定义上我们可以看到这个类是在EurekaDiscoveryClientConfiguration初始化完成之后再进行初始化的，这不是重点，重点来看EurekaClientAutoConfiguration 123456@AutoConfigureAfter(name = &#123; \"org.springframework.cloud.autoconfigure.RefreshAutoConfiguration\", \"org.springframework.cloud.netflix.eureka.EurekaDiscoveryClientConfiguration\", \"org.springframework.cloud.client.serviceregistry.AutoServiceRegistrationAutoConfiguration\" &#125;)public class EurekaClientAutoConfiguration &#123;&#125; 注册服务 EurekaClientAutoConfiguration类的主要功能是配置EurekaClient。其中有个关键的内部类RefreshableEurekaClientConfiguration： 123456789101112131415161718192021222324252627@Configuration(proxyBeanMethods = false)@ConditionalOnRefreshScopeprotected static class RefreshableEurekaClientConfiguration &#123; @Autowired private ApplicationContext context; @Autowired private AbstractDiscoveryClientOptionalArgs&lt;?&gt; optionalArgs; @Bean(destroyMethod = \"shutdown\") @ConditionalOnMissingBean(value = EurekaClient.class, search = SearchStrategy.CURRENT) @org.springframework.cloud.context.config.annotation.RefreshScope @Lazy public EurekaClient eurekaClient(ApplicationInfoManager manager, EurekaClientConfig config, EurekaInstanceConfig instance, @Autowired(required = false) HealthCheckHandler healthCheckHandler) &#123; ApplicationInfoManager appManager; if (AopUtils.isAopProxy(manager)) &#123; appManager = ProxyUtils.getTargetObject(manager); &#125; else &#123; appManager = manager; &#125; CloudEurekaClient cloudEurekaClient = new CloudEurekaClient(appManager, config, this.optionalArgs, this.context); cloudEurekaClient.registerHealthCheck(healthCheckHandler); return cloudEurekaClient; &#125;&#125; 这个类被@ConditionalOnRefreshScope标注了，因为在spring-cloud-context包的spring.factories中配置了RefreshAutoConfiguration，且@ConditionalOnRefreshScope的实例化取决于RefreshAutoConfiguration 123456789@Target(&#123; ElementType.TYPE, ElementType.METHOD &#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@ConditionalOnClass(RefreshScope.class)@ConditionalOnBean(RefreshAutoConfiguration.class)// 重点@ConditionalOnProperty(value = \"eureka.client.refresh.enable\", havingValue = \"true\", matchIfMissing = true)@interface ConditionalOnRefreshScope &#123;&#125; 既然RefreshableEurekaClientConfiguration类被实例化了，那么里面的EurekaClient也同样被实例化了，在eurekaClient()方法中返回的是CloudEurekaClient类的实例，那么关键就是这个类了。 CloudEurekaClient继承自DiscoveryClient，并且在构造器中是直接调了父类的构造器去处理具体逻辑 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455DiscoveryClient(ApplicationInfoManager applicationInfoManager, EurekaClientConfig config, AbstractDiscoveryClientOptionalArgs args, Provider&lt;BackupRegistry&gt; backupRegistryProvider, EndpointRandomizer endpointRandomizer) &#123; try &#123; // 创建调度线程池，只给heartbeat和cacheRefresh使用，所以核心线程池为2即可 scheduler = Executors.newScheduledThreadPool(2, new ThreadFactoryBuilder() .setNameFormat(\"DiscoveryClient-%d\") .setDaemon(true) .build()); heartbeatExecutor = new ThreadPoolExecutor( 1, clientConfig.getHeartbeatExecutorThreadPoolSize(), 0, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;(), new ThreadFactoryBuilder() .setNameFormat(\"DiscoveryClient-HeartbeatExecutor-%d\") .setDaemon(true) .build() ); cacheRefreshExecutor = new ThreadPoolExecutor( 1, clientConfig.getCacheRefreshExecutorThreadPoolSize(), 0, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;(), new ThreadFactoryBuilder() .setNameFormat(\"DiscoveryClient-CacheRefreshExecutor-%d\") .setDaemon(true) .build() ); &#125; catch (Throwable e) &#123; throw new RuntimeException(\"Failed to initialize DiscoveryClient!\", e); &#125; // 从远程获取所有的服务 if (clientConfig.shouldFetchRegistry() &amp;&amp; !fetchRegistry(false)) &#123; fetchRegistryFromBackup(); &#125; // 在所有后台任务启动之前调用并执行预注册处理程序 if (this.preRegistrationHandler != null) &#123; this.preRegistrationHandler.beforeRegistration(); &#125; if (clientConfig.shouldRegisterWithEureka() &amp;&amp; clientConfig.shouldEnforceRegistrationAtInit()) &#123; try &#123; if (!register() ) &#123; throw new IllegalStateException(\"Registration error at startup. Invalid server response.\"); &#125; &#125; catch (Throwable th) &#123; logger.error(\"Registration error at startup: &#123;&#125;\", th.getMessage()); throw new IllegalStateException(th); &#125; &#125; // 初始化调度任务（cacheRefreshTask、heartbeatTask） initScheduledTasks();&#125; DiscoveryClient构造器中调用的最核心的两个方法是fetchRegistry()和initScheduledTasks()，fetchRegistry()方法中调用了getAndStoreFullRegistry()，最终在此方法中向Eureka Server发送了获取所有实例的请求 1234567891011private void getAndStoreFullRegistry() throws Throwable &#123; long currentUpdateGeneration = fetchRegistryGeneration.get(); Applications apps = null; EurekaHttpResponse&lt;Applications&gt; httpResponse = clientConfig.getRegistryRefreshSingleVipAddress() == null ? eurekaTransport.queryClient.getApplications(remoteRegionsRef.get()) : eurekaTransport.queryClient.getVip(clientConfig.getRegistryRefreshSingleVipAddress(), remoteRegionsRef.get()); if (httpResponse.getStatusCode() == Status.OK.getStatusCode()) &#123; apps = httpResponse.getEntity(); &#125;&#125; 操作完成之后，调用DiscoveryClient的initScheduledTasks()方法，在这个方法中，注册两个定时任务，一个是以指定的时间间隔获取注册表信息的任务，另一个是在给定的时间间隔内更新租约的heartbeat任务，并且在任务都初始化完成之后调用InstanceInfoReplicator#start方法初始化一个注册远程服务的定时任务。类InstanceInfoReplicator实际上是一个线程，实现自Runnable接口，在他的run方法里调用了DiscoveryClient的register()方法通过REST调用向eureka服务注册。 Eureka接口： 123456789101112POST /eureka/apps/&#123;appName&#125; 注册新的实例 DELETE /eureka/apps/&#123;appName&#125;/&#123;instanceId&#125; 注销应用实例 PUT /eureka/apps/&#123;appName&#125;/&#123;instanceId&#125; 应用实例发送心跳 GET /eureka/apps 查询所有的实例 GET /eureka/apps/&#123;appName&#125; 查询指定appId的实例 GET /eureka/apps/&#123;appName&#125;/&#123;instanceId&#125; 查询指定appId和instanceId的实例 GET /eureka/instances/&#123;instanceId&#125; 查询指定的instanceId的实例 PUT /eureka/apps/&#123;appName&#125;/&#123;instanceId&#125;/status?value=OUT_OF_SERVICE 暂停应用实例 PUT /eureka/apps/&#123;appName&#125;/&#123;instanceId&#125;/status?value=UP 恢复应用实例 PUT /eureka/apps/&#123;appName&#125;/&#123;instanceId&#125;/metadata?key=value 更新元数据信息 GET /eureka/vips/&#123;vipAddress&#125; 根据vip地址查询 GET /eureka/svips/&#123;svipAddress&#125; 根据svip地址查询 这些接口被定义在eureka-core.jar的com.netflix.eureka.resources包中 Eureka核心类 12345InstanceInfo : 注册的服务实例,里面包含服务实例的各项属性LeaseInfo : Eureka用这个类来标识应用实例的租约信息ServiceInstance : 发现的实例信息的抽象接口,约定了服务发现的实例应用有哪些通用信息InstanceStatus : 用于标识服务实例的状态,是一个枚举类,主要有状态UP,DOWN,STARTING,OUT_OF_SERVICE,UNKNOWNEurekaServerConfigBean : Eureka Server的核心配置类，里面包含了Eureka Server的各项核心属性信息 renew续约心跳 Eureka的续约需要每隔一段时间执行一次，目的是要告诉Eureka Server客户端还活着，以免Eureka Server将其当作是宕机的服务而剔除掉。 Client默认是每隔30秒发送一次renew请求，可以通过配置信息eureka.instance.lease-renewal-interval-in-seconds修改。 Server收到renew请求后，根据接收到的参数找到对应的实例，更新实例的续约时间，再将最新的续约时间同步到集群中的其他Server节点，最终完成续约。 Client端的续约定时任务是在实例化之后在initScheduledTasks()方法中被定义的： 123456789101112131415161718192021222324private void initScheduledTasks() &#123; if (clientConfig.shouldRegisterWithEureka()) &#123; int renewalIntervalInSecs = instanceInfo.getLeaseInfo().getRenewalIntervalInSecs(); int expBackOffBound = clientConfig.getHeartbeatExecutorExponentialBackOffBound(); logger.info(\"Starting heartbeat executor: \" + \"renew interval is: &#123;&#125;\", renewalIntervalInSecs); // 创建心跳实例 heartbeatTask = new TimedSupervisorTask( \"heartbeat\", scheduler, heartbeatExecutor, renewalIntervalInSecs, TimeUnit.SECONDS, expBackOffBound, new HeartbeatThread() ); scheduler.schedule( heartbeatTask, renewalIntervalInSecs, TimeUnit.SECONDS); &#125; else &#123; logger.info(\"Not registering with Eureka server per configuration\"); &#125;&#125; 从代码中可以看到心跳最终使用的是类HeartbeatThread，这个类实际上就是一个线程类，通过ScheduledExecutorService来执行： 12345678private class HeartbeatThread implements Runnable &#123; public void run() &#123; if (renew()) &#123; lastSuccessfulHeartbeatTimestamp = System.currentTimeMillis(); &#125; &#125;&#125; 1234567891011121314151617181920boolean renew() &#123; EurekaHttpResponse&lt;InstanceInfo&gt; httpResponse; try &#123; // 调用接口进行续约 httpResponse = eurekaTransport.registrationClient.sendHeartBeat(instanceInfo.getAppName(), instanceInfo.getId(), instanceInfo, null); // 若Server端返回服务不存在，则重新将服务注册到Server if (httpResponse.getStatusCode() == Status.NOT_FOUND.getStatusCode()) &#123; REREGISTER_COUNTER.increment(); long timestamp = instanceInfo.setIsDirtyWithTime(); boolean success = register(); if (success) &#123; instanceInfo.unsetIsDirty(timestamp); &#125; return success; &#125; return httpResponse.getStatusCode() == Status.OK.getStatusCode(); &#125; catch (Throwable e) &#123; return false; &#125;&#125; Eureka Server根据Jersey框架实现HTTP请求，续约请求最终会被com.netflix.eureka.resources.InstanceResource#renewLease接口接收到，然后通过InstanceRegistry递交给PeerAwareInstanceRegistryImpl，最终递交给AbstractInstanceRegistry#renew处理具体的操作，经过一系列rule操作之后，最终调用Lease#renew完成对lastUpdateTimestamp的更新。 1234567891011121314151617181920212223242526272829public boolean renew(String appName, String id, boolean isReplication) &#123; RENEW.increment(isReplication); Map&lt;String, Lease&lt;InstanceInfo&gt;&gt; gMap = registry.get(appName); Lease&lt;InstanceInfo&gt; leaseToRenew = null; if (gMap != null) &#123; leaseToRenew = gMap.get(id); &#125; if (leaseToRenew == null) &#123; RENEW_NOT_FOUND.increment(isReplication); return false; &#125; else &#123; InstanceInfo instanceInfo = leaseToRenew.getHolder(); if (instanceInfo != null) &#123; // touchASGCache(instanceInfo.getASGName()); // 匹配服务状态 InstanceStatus overriddenInstanceStatus = this.getOverriddenInstanceStatus(instanceInfo, leaseToRenew, isReplication); if (overriddenInstanceStatus == InstanceStatus.UNKNOWN) &#123; RENEW_NOT_FOUND.increment(isReplication); return false; &#125; if (!instanceInfo.getStatus().equals(overriddenInstanceStatus)) &#123; instanceInfo.setStatusWithoutDirty(overriddenInstanceStatus); &#125; &#125; renewsLastMin.increment(); leaseToRenew.renew(); return true; &#125;&#125; 续约操作成功完成后，会调用PeerAwareInstanceRegistryImpl#replicateToPeers方法通知其他Eureka节点 renew控制： eureka.instance.lease-renewal-interval-in-seconds=10 #10秒renew一次，默认30秒 eureka.instance.lease-expiration-duration-in-senconds=80 #如果80秒内未发送续约请求，则关闭该客户端，默认为90秒 lease-expiration-duration-in-senconds不宜过大，否则可能出现客户端已down，但还是会有流量转发给它；但是也不宜过小，不然客户端可能会因为出现网络抖动而被移除。大于lease-renewal-interval-in-seconds两三倍以上。","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://luxiaowan.github.io/categories/SpringBoot/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://luxiaowan.github.io/tags/SpringCloud/"},{"name":"Eureka","slug":"Eureka","permalink":"http://luxiaowan.github.io/tags/Eureka/"}]},{"title":"MySQL执行计划explain解析","slug":"MySQL执行计划explain解析","date":"2020-04-09T17:03:00.000Z","updated":"2020-04-09T17:22:58.427Z","comments":true,"path":"2020/04/10/MySQL执行计划explain解析/","link":"","permalink":"http://luxiaowan.github.io/2020/04/10/MySQL执行计划explain解析/","excerpt":"","text":"用法 explain table或explain EXTENDED table 参数解释 id：select查询的序列号，可以当做是执行顺序 id相同时，执行顺序由上至下 如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行 id如果相同，可以认为是一组，从上往下顺序执行；在所有组中，id值越大，优先级越高，越先执行 select_type：查询中每个select子句的类型 SIMPLE(简单SELECT,不使用UNION或子查询等) PRIMARY(查询中若包含任何复杂的子部分,最外层的select被标记为PRIMARY) UNION(UNION中的第二个或后面的SELECT语句) DEPENDENT UNION(UNION中的第二个或后面的SELECT语句，取决于外面的查询) UNION RESULT(UNION的结果) SUBQUERY(子查询中的第一个SELECT) DEPENDENT SUBQUERY(子查询中的第一个SELECT，取决于外面的查询) DERIVED(派生表的SELECT, FROM子句的子查询) UNCACHEABLE SUBQUERY(一个子查询的结果不能被缓存，必须重新评估外链接的第一行) table：显示这一行的数据是关于哪张表的，有时不是真实的表名字,看到的是derivedx(x是个数字,我的理解是第几步执行的结果) partitions：分区号 type：表示MySQL在表中找到所需行的方式，又称“访问类型”，常用的类型有： ALL, index, range, ref, eq_ref, const, system, NULL（从左到右，性能从差到好） ALL：Full Table Scan， MySQL将遍历全表以找到匹配的行 index：Full Index Scan，index与ALL区别为index类型只遍历索引树 range：只检索给定范围的行，使用一个索引来选择行 ref：表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 eq_ref：类似ref，区别就在使用的索引是唯一索引，对于每个索引键值，表中只有一条记录匹配，简单来说，就是多表连接中使用primary key或者 unique key作为关联条件 const、system：当MySQL对查询某部分进行优化，并转换为一个常量时，使用这些类型访问。如将主键置于where列表中，MySQL就能将该查询转换为一个常量,system是const类型的特例，当查询的表只有一行的情况下，使用system NULL：MySQL在优化过程中分解语句，执行时甚至不用访问表或索引，例如从一个索引列里选取最小值可以通过单独索引查找完成。 possible_keys：指出MySQL能使用哪个索引在表中找到记录，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用 该列完全独立于EXPLAIN输出所示的表的次序。这意味着在possible_keys中的某些键实际上不能按生成的表次序使用。如果该列是NULL，则没有相关的索引。在这种情况下，可以通过检查WHERE子句看是否它引用某些列或适合索引的列来提高你的查询性能。如果是这样，创造一个适当的索引并且再次用EXPLAIN检查查询 Key：显示MySQL实际决定使用的键（索引） 如果没有选择索引，键是NULL。要想强制MySQL使用或忽视possible_keys列中的索引，在查询中使用FORCE INDEX、USE INDEX或者IGNORE INDEX。 key_len：表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度（key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的） 不损失精确性的情况下，长度越短越好 ref：表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 rows：表示MySQL根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数 Extra：该列包含MySQL解决查询的详细信息,有以下几种情况： Using filesort MySQL有两种方式可以生成有序的结果，通过排序操作或者使用索引，当Extra中出现了Using filesort 说明MySQL使用了后者，但注意虽然叫filesort但并不是说明就是用了文件来进行排序，只要可能排序都是在内存里完成的。大部分情况下利用索引排序更快，所以一般这时也要考虑优化查询了。使用文件完成排序操作，这是可能是ordery by，group by语句的结果，这可能是一个CPU密集型的过程，可以通过选择合适的索引来改进性能，用索引来为查询结果排序。 Using temporary 用临时表保存中间结果，常用于GROUP BY 和 ORDER BY操作中，一般看到它说明查询需要优化了，就算避免不了临时表的使用也要尽量避免硬盘临时表的使用。 Not exists MYSQL优化了LEFT JOIN，一旦它找到了匹配LEFT JOIN标准的行， 就不再搜索了 Using index 说明查询是覆盖了索引的，不需要读取数据文件，从索引树（索引文件）中即可获得信息。如果同时出现using where，表明索引被用来执行索引键值的查找，没有using where，表明索引用来读取数据而非执行查找动作。这是MySQL服务层完成的，但无需再回表查询记录。 Using index condition 这是MySQL 5.6出来的新特性，叫做“索引条件推送”。简单说一点就是MySQL原来在索引上是不能执行如like这样的操作的，但是现在可以了，这样减少了不必要的IO操作，但是只能用在二级索引上。 Using where 使用了WHERE从句来限制哪些行将与下一张表匹配或者是返回给用户。注意：Extra列出现Using where表示MySQL服务器将存储引擎返回服务层以后再应用WHERE条件过滤 Using join buffer 使用了连接缓存：Block Nested Loop，连接算法是块嵌套循环连接;Batched Key Access，连接算法是批量索引连接 impossible where where子句的值总是false，不能用来获取任何元组 select tables optimized away 在没有GROUP BY子句的情况下，基于索引优化MIN/MAX操作，或者对于MyISAM存储引擎优化COUNT(*)操作，不必等到执行阶段再进行计算，查询执行计划生成的阶段即完成优化。 distinct 优化distinct操作，在找到第一匹配的元组后即停止找同样值的动作","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/categories/MySQL/"}],"tags":[]},{"title":"MySQL索引建立原则","slug":"MySQL索引建立原则","date":"2020-04-09T16:50:00.000Z","updated":"2020-04-09T17:02:53.639Z","comments":true,"path":"2020/04/10/MySQL索引建立原则/","link":"","permalink":"http://luxiaowan.github.io/2020/04/10/MySQL索引建立原则/","excerpt":"","text":"前言 索引的目的是提升查询数据的效率，所以我们建索引的标准应该一切从提升查询效率为基准。 小技巧 对于有唯一值的列尽量使用唯一索引 索引长度尽量小一点，长度小的索引可以节省索引空间，也会使查找的速度得到提升，因为索引页只有16k，索引列长度小的话，一页可以容纳更多的数据 太长的列可以选择部分内容做索引，遵循最左前缀原则 更新频繁的列不适合建索引 利用断桥原则（最左前缀原则），比如建立了一个联合索引(a,b,c)，那么其实我们可利用的索引就有(a), (a,b), (a,b,c) 不要过多创建索引，索引越多占用的空间越多，而且每次增、删、改操作都会重建索引，并且索引太多的话也会增加之后的优化复杂度 尽量扩展索引，比如现有索引(a)，现在我又要对(a,b)进行索引，不需要再建一个索引(a,b)，只需要在原索引(a)的基础上新增b列即可 一次查询是不能应用多个索引，即使你查询条件中有多个索引，最终也只会选择最优的一个 &lt;，&lt;=，=，&gt;，&gt;=，BETWEEN，IN 可用到索引，&lt;&gt;，not in ，!= 则不行 like “xxxx%” 是可以用到索引的，like “%xxxx” 和 like “%xxx%” 则不行（但会用到索引下推） NULL会使索引的效果大打折扣 索引列若出现函数或计算，则索引不会生效","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/categories/MySQL/"}],"tags":[{"name":"索引","slug":"索引","permalink":"http://luxiaowan.github.io/tags/索引/"}]},{"title":"MySQL回表、索引覆盖、索引下推","slug":"MySQL回表、索引覆盖、索引下推","date":"2020-04-09T13:01:00.000Z","updated":"2020-04-09T16:37:44.165Z","comments":true,"path":"2020/04/09/MySQL回表、索引覆盖、索引下推/","link":"","permalink":"http://luxiaowan.github.io/2020/04/09/MySQL回表、索引覆盖、索引下推/","excerpt":"","text":"准备 创建一张表，并创建一个自增主键索引和一个组合索引 12345678910CREATE TABLE index_opt_test ( id int(11) NOT NULL AUTO_INCREMENT, name varchar(11) DEFAULT NULL, title varchar(11) DEFAULT NULL, age int(11) DEFAULT NULL, sex varchar(11) DEFAULT NULL, content varchar(500) DEFAULT NULL, PRIMARY KEY (id), KEY idx_cb (name,title,age)) ENGINE=InnoDB; 回表 什么是回表 回表是发生在二级索引上的一种数据查询操作，简单点讲就是我们要查询的列不在二级索引的列中，那么就必须根据二级索引查到主键ID，然后再根据主键ID到聚簇索引树上去查询整行的数据，这一过程就叫作回表。 为什么会回表 写一个会回表查询的SQL： 1select id, name, age, sex from index_opt_test where name=&apos;cc&apos; and title=&apos;T7&apos; and age=35; 解析： ​ SQL需要查询的列包括id、name、age、sex，查询条件命中索引idx_cb，其中列id、name、age都在索引idx_cb中可以获取到，但是sex不能通过索引获取到，必须要获取到整行数据之后再从结果中捞出来sex列的数据，这种情况就必须要回表。 什么情况下不需要回表 当所有的列都能在二级索引树中查询到，就不需要再回表了，这种情况就是索引覆盖。 索引覆盖 什么是索引覆盖 当SQL语句中查询的列都在索引中时，我们就不需要回表去把整行数据都捞出来了，可以从非聚簇索引树中直接获取到我们需要的列的数据，这就叫索引覆盖。简单点来讲就是：所有不需要回表的查询操作都叫索引覆盖。 为什么会发生索引覆盖 关于为什么会发生索引覆盖这个问题，通过一条SQL来理解： 1select id, name, age from index_opt_test where name=&apos;cc&apos; and title=&apos;T7&apos;; 这条SQL要查询的列id、name、age全部都能从非聚簇索引idx_cb中直接查询出来，可能会有个疑问：我们的索引列是name、title和age，为什么id明明不在组合索引中却还能发生索引覆盖？提出这个问题的同学真的是欠我一顿小烧烤，非聚簇索引的叶子节点里存的是什么东西：主键的关键字啊，我们这里主键是id，他的关键字就是id的值啊，那我们通过非聚簇索引是不是可以直接将主键id查出来，是不是就不用再回表了，不用回表是不是就发生了索引覆盖啊，就是那么简单。 索引下推 什么是索引下推 索引下推又叫索引条件下推(Index Condition Pushdown，简称ICP)，ICP默认是开启的，使用ICP可以减少存储引擎访问基础表的次数和Server访问存储引擎的次数。 ICP没有启用：Server层会根据索引的断桥原则将命中的索引字段推送到引擎层获取数据，并把匹配到的数据全部返回到Server层，由Server层再根据剩余的where条件进行过滤，即使where条件中有组合索引的其他未命中的字段，也会保留在Server层做筛选，然后返回给Client 1select id, name, sex from index_opt_test where name=&apos;cc&apos; and title like &apos;%7&apos; and sex=&apos;male&apos;; 执行过程： Server层把name推到引擎层 引擎层根据name去idx_cb的索引树中匹配主键 回表去捞数据返回给Server层 Server层再根据title、sex筛选出最终的数据 最后返回给客户端 ICP启用：Server层会将where条件中在组合索引中的字段全部推送到引擎层，引擎层根据断桥原则匹配出索引数据，然后将其他索引字段带入再进行一次筛选，然后拿最终匹配的主键关键字回表查询出数据后返回给Server层，Server层再根据剩余的where条件做一次筛选，然后返回给Client 1select id, name, sex from index_opt_test where name=&apos;cc&apos; and title like &apos;%7&apos; and sex=&apos;male&apos;; 执行过程： Server把name和title都推到引擎层 引擎层根据name去idx_cb中查询出主键关键字和title、age 再由title筛选出匹配的主键关键字 回表去捞数据返回给Server层 Server层再根据sex筛选出最终的数据 再返回给客户端 索引下推适用条件 ICP 用于访问方法是 range/ref/eq_ref/ref_or_null，且需要访问表的完整行记录。 ICP适用于 InnoDB 和 MyISAM 的表，包括分区的表。 对于 InnoDB 表，ICP只适用于二级索引。ICP 的目标是减少访问表的完整行的读数量从而减少 I/O 操作。对于 InnoDB 的聚簇索引，完整的记录已经读进 InnoDB 的缓存，使用 ICP 不能减少 I/O 。 ICP 不支持建立在虚拟列上的二级索引（InnoDB 支持在虚拟列上建立二级索引）。 引用子查询、存储函数的条件没法下推，Triggered conditions 也没法下推。 所以ICP 适用的一个隐含前提是二级索引必须是组合索引、且在使用索引进行扫描时只能采用最左前缀匹配原则。组合索引后面的列出现在 where 条件里，因此可以先过滤索引元组、从而减少回表读的数量。 为什么会发生索引下推 索引下推在5.6版本加入的，默认开启，可以通过命令SHOW VARIABLES like '%optimizer_switch%'查看当前状态 关闭索引下推 1SET optimizer_switch = &apos;index_condition_pushdown=off&apos;; 执行之后查看一下状态： 然后我们执行一下SQL语句： 12explainselect id, name, sex from index_opt_test where name=&apos;cc&apos; and title like &apos;%7&apos; and sex=&apos;male&apos;; 从执行计划我们可以看出当我们关闭索引下推后，Extra中的是Using where 开启索引下推 1SET optimizer_switch = &apos;index_condition_pushdown=on&apos;; 执行之后查看一下状态： 然后我们执行以下SQL语句： 12explainselect id, name, sex from index_opt_test where name=&apos;cc&apos; and title like &apos;%7&apos; and sex=&apos;male&apos;; 从执行计划中看到使用了Using index condition和Using where，Using index condition说明ICP生效了，title被推到了引擎层，而Using where是因为where条件中的sex字段","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/categories/MySQL/"}],"tags":[{"name":"索引","slug":"索引","permalink":"http://luxiaowan.github.io/tags/索引/"}]},{"title":"MySQL聚簇索引和非聚簇索引","slug":"MySQL聚簇索引和非聚簇索引","date":"2020-04-09T10:00:00.000Z","updated":"2020-04-09T12:20:19.744Z","comments":true,"path":"2020/04/09/MySQL聚簇索引和非聚簇索引/","link":"","permalink":"http://luxiaowan.github.io/2020/04/09/MySQL聚簇索引和非聚簇索引/","excerpt":"","text":"聚簇索引 聚簇索引是指叶子节点存储的是一整行记录，比如InnoDB的主键索引，主键和表数据存储在一起。聚簇索引并不是一种单独的索引类型，而是一种数据存储方式，因为一行数据不能同时存储在两个地方，所以一张表中只能有一个聚簇索引，因为一张表的数据存储顺序只能是一种，故只有InnoDB主键索引是聚簇索引。 聚簇索引的存放顺序和数据的物理存储顺序是一致的，即只要是索引是挨着的，那么对应的数据在磁盘上的存储位置一定也是挨着的。 这里有一个问题：如果我们不用自增的字段作为主键，而使用字符串的话，会有什么不妥的地方？我们来分析看下： 自增主键：按照主键的值按顺序递增，也就是会一直往后添加数据，只需要分配新页就可以了，那么已经存储了数据的页就永远不会再分裂，物理地址则不需要变动 字符串：需要根据字符串的ASSIC码值进行计算所要存储的位置，这个过程中会引起已存储数据的物理地址发生变动，并且需要不断的进行页的分裂，带来的性能开销非常大 优势 可以一次性将相邻的数据加载到内存中，减少了磁盘IO次数 由于聚簇索引是将索引和数据存储在一起，那么我们找到索引位置的时候实际上就是找到了具体的数据，否则还要进行一次磁盘IO去将最终数据捞出来 劣势 插入速度严重依赖于插入顺序：如果使用的是非自增主键，则可能需要进行页的分裂，非常影响性能 主键更新代价大：更新一次主键，可能导致被更新的行发生移动，引起页的分裂，非常影响性能 二级索引查询需要再次根据主键索引回表查询整行数据，因为InnoDB的二级索引的叶子节点存储的是主键的值 非聚簇索引 非聚簇索引的叶子节点存储的是主键值或行数据存储的物理位置，MyIsam甭管是主键还是非主键索引都是非聚簇索引索引，InnoDB的非主键索引用的也是非聚簇索引，但是这两种存储引擎的非主键索引的叶子节点存储的内容是不同的。 InnoDB非聚簇索引：叶子节点存储的是主键关键字，当聚簇索引发生页分裂或移动时(主键关键字未变)，非聚簇索引不需要改变 MyIsam非聚簇索引：所有索引的叶子节点存储的都是行数据的物理磁盘存储地址，只要行数据发生位置移动时，会引起所有的索引发生改变 总结 聚簇索引叶子节点存储的是行数据，非聚簇索引的叶子节点存储的是主键关键字或数据物理存储地址 InnoDB的主键索引是聚簇索引，InnoDB的二级索引和MyIsam的所有索引都是非聚簇索引 InnoDB非聚簇索引叶子节点存储的是主键关键字，MyIsam非聚簇索引叶子节点存储的是数据物理存储地址 聚簇索引尽量使用自增列，可以减少页分裂和行存储位置移动，提升性能","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/categories/MySQL/"}],"tags":[{"name":"索引","slug":"索引","permalink":"http://luxiaowan.github.io/tags/索引/"}]},{"title":"MySQL索引类型","slug":"MySQL索引类型","date":"2020-04-09T07:49:49.544Z","updated":"2020-04-09T09:38:13.017Z","comments":true,"path":"2020/04/09/MySQL索引类型/","link":"","permalink":"http://luxiaowan.github.io/2020/04/09/MySQL索引类型/","excerpt":"","text":"索引类型 MySQL中我们常用的索引类型有五种： 普通索引 唯一索引 主键索引 组合索引 全文索引 创建表： 12345678CREATE TABLE `INDEX_TEST` ( `id` int(11) DEFAULT NULL, `name` varchar(11) DEFAULT NULL, `idno` varchar(11) DEFAULT NULL, `age` int(11) DEFAULT NULL, `sex` varchar(11) DEFAULT NULL, `content` varchar(500) DEFAULT NULL) ENGINE=InnoDB; 普通索引 最基本的一种索引，没有任何限制，可以为null，可以重复，普通索引的目的就是为了加快对数据的访问速度，为那些常被用来作查询条件的字段创建一个索引 SQL： 1ALTER TABLE INDEX_TEST ADD INDEX idx_normal(name); 唯一索引 与普通索引类似，但是不同点在于被索引的字段的值不能重复，但是可以为null，比如身份证、员工编号等必须唯一的信息，被设置了唯一索引的字段在整张表中都不能重复 SQL： 1ALTER TABLE INDEX_TEST ADD UNIQUE idx_unique(idno); 主键索引 与唯一索引类似，但是不同点是字段值不能为null，一般一张表只能有一个主键。实际上每张表都要有一个主键字段，通常情况下我们都是在创建表的时候手动添加，如果一张InnoDB引擎的表未明确主键字段，那么InnoDB引擎会自动为表创建一个隐式的自增主键，所以最优的情况下，我们最好手动创建一个自增主键。 SQL： 1ALTER TABLE INDEX_TEST ADD PRIMARY KEY idx_pk(id); 创建索引的时候，索引名称可以不写。 组合索引 组合索引是创建一个包含表中多个字段的索引，组合索引的使用遵循&quot;断桥原则&quot;，也叫&quot;最左前缀原则&quot;，我们在使用组合索引的时候，会从第一个往后匹配，使用顺序影响不大，因为MySQL的优化器会进行再次优化，如果中间有断桥(漏掉了中间一个或多个字段)，则断桥左侧的字段才会被用到索引。 比如索引字段(a, b, c)，有以下情况： where a and c and b：索引字段全部命中 where a and c：索引命中a where a and b：索引命中a、b where b and c：索引未命中 where a and b order by c：索引字段全部命中 SQL： 1ALTER TABLE INDEX_TEST ADD INDEX idx_cb(age, sex); 全文索引 主要用来查找文本中的关键字，而不是直接与索引中的值相比较。fulltext索引跟其它索引大不相同，它更像是一个搜索引擎，而不是简单的where语句的参数匹配。fulltext索引配合match against操作使用，而不是一般的where语句加like。它可以在create table，alter table ，create index使用，不过目前只有char、varchar，text 列上可以创建全文索引。值得一提的是，在数据量较大时候，现将数据放入一个没有全局索引的表中，然后再用CREATE index创建fulltext索引，要比先为一张表建立fulltext然后再将数据写入的速度快很多，因为建表的时候表中没有数据，而已经有了数据再创建的话，要将已存在的数据都先fulltext一下，所以慢。 SQL： 1ALTER TABLE INDEX_TEST ADD FULLTEXT idx_fulltext(content); 总结 最终我们的表创建语句变成了： 12345678910111213CREATE TABLE `index_test` ( `id` int(11) NOT NULL, `name` varchar(11) DEFAULT NULL, `age` int(11) DEFAULT NULL, `sex` varchar(11) DEFAULT NULL, `content` varchar(500) DEFAULT NULL, `idno` varchar(11) DEFAULT NULL, PRIMARY KEY (`id`),# 主键索引 UNIQUE KEY `idx_unique` (`idno`),# 唯一索引 KEY `normal_index` (`name`),# 普通索引 KEY `idx_cb` (`age`,`sex`),# 组合索引 FULLTEXT KEY `idx_fulltext` (`content`)# 全文索引) ENGINE=InnoDB;","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/categories/MySQL/"}],"tags":[{"name":"索引","slug":"索引","permalink":"http://luxiaowan.github.io/tags/索引/"}]},{"title":"MySQL自适应Hash索引","slug":"MySQL自适应Hash索引","date":"2020-04-09T06:40:00.000Z","updated":"2020-04-09T07:45:31.343Z","comments":true,"path":"2020/04/09/MySQL自适应Hash索引/","link":"","permalink":"http://luxiaowan.github.io/2020/04/09/MySQL自适应Hash索引/","excerpt":"","text":"Hash索引 Hash是一种查找数据非常快的数据结构，在正常情况下这种查找的时间复杂度为O(1)，即一般仅需要一次查找就能定位数据，正常情况是指不存在哈希冲突的情况；而B+树的查找次数，取决于B+树的高度，B+树的高度一般为34层，所以一般最少需要34次的查询。 InnoDB的Hash索引 InnoDB存储引擎会监控对表上各索引页的查询，如果监控到某个索引页被频繁查询，并诊断后发现如果为这一页的数据创建Hash索引会带来更大的性能提升，则会自动为这一页的数据创建Hash索引，并称之为自适应Hash索引。自适应Hash是通过缓冲池中B+树的页进行构建的，建立速度很快，不需要对整张表的数据都构建Hash索引，所以我们又可以把自适应Hash索引看成是索引的索引，。注意一点就是InnoDB只会对热点页构建自适应索引，且是由InnoDB自动创建和删除的，所以不能人为干预是否在一张InnoDB的表中创建Hash索引。 自适应Hash索引 官方有告诉我们每一种存储引擎所支持的索引结构，在https://dev.mysql.com/doc/refman/5.7/en/create-index.html中可以查看，我们截图保留下： 我们可以看到MyIsam和InnoDB仅支持BTree结构的索引，但是我们在创建的时候却有Hash结构可选择，这是因为啥？细心的话可以发现不论我们在创建索引的时候选择了Hash还是BTree，在保存的时候都会自动转换成BTree，就是这个原因。 但是我们又在官方的https://dev.mysql.com/doc/refman/5.7/en/innodb-adaptive-hash.html页面看到这么一句话： 大致意思是：自适应Hash索引特征能使InnoDB在具有适当的工作负载和足够缓冲池内存的系统上执行的更像内存中的数据库的操作，且不会牺牲事务特性或可靠性，MySQL能基于监视到的搜索规则，使用索引键的前缀构建Hash索引，前缀可以是任意长度，并且可能只有b+树中的某些值出现在Hash索引中，Hash索引其实就是对经常访问的索引页进行构建的。 这又说明其实InnoDB是支持Hash索引的，但并不是真正意义上的Hash，而是通过自己的监视情况自动对某些热点索引值构建的内存Hash。 开启和关闭 默认情况下自适应索引是开启状态，毕竟是可以提升性能的嘛，我们也可以通过命令开启和关闭，并可以查看自适应索引的 开启 默认就是开启的，可以通过命令show variables like 'innodb_adaptive_hash_index';查看自适应哈希索引的状态，并可以在命令行通过show engine innodb status\\G查看自适应Hash索引的使用信息(AHI的大小，使用情况，每秒使用AHI搜索的情况等等) 关闭 负载较重的情况下，就不太适合开启自适应Hash索引了，因为这样可以避免额外的索引维护带来的开销，可以在启动的时候通过参数--skip-innodb-adaptive-hash-index关闭","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/categories/MySQL/"}],"tags":[{"name":"索引","slug":"索引","permalink":"http://luxiaowan.github.io/tags/索引/"}]},{"title":"MySQL的Hash索引相关","slug":"MySQL的Hash索引相关","date":"2020-04-09T02:20:00.000Z","updated":"2020-04-09T16:49:28.507Z","comments":true,"path":"2020/04/09/MySQL的Hash索引相关/","link":"","permalink":"http://luxiaowan.github.io/2020/04/09/MySQL的Hash索引相关/","excerpt":"","text":"hash索引结构使用方式较为局限，仅适用于=、IN和&lt;=&gt;三种，但是由于通过hash可以直接查找到具体的值，而不用像B+树那样每次都从root节点开始遍历，所以在通常情况下，hash的查找效率要比B+树高。 hash的缺陷： 1. hash不能进行范围查找 值在计算hash后，并不能保证计算后的hash值和计算前的大小排列一样，所以hash不适用于范围查找 2. hash不能进行排序查询 值计算后的hash值无法保证与原值大小顺序一样，所以无法进行排序 3. 组合索引不能使用部分字段查询 组合索引的hash值是所有索引字段的值组合在一起进行计算的，若仅使用部分字段进行查询的话，计算出的hash值基本不会与索引的hash值相同 4. hash在出现大量值碰撞的时候，性能会降低 hash出现大量的值相等的时候，需要进行表扫描以进行精确匹配，效率较低","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/categories/MySQL/"}],"tags":[{"name":"索引","slug":"索引","permalink":"http://luxiaowan.github.io/tags/索引/"}]},{"title":"MySQL存储引擎下的索引","slug":"MySQL存储引擎下的索引","date":"2020-04-09T01:55:00.000Z","updated":"2020-04-09T06:39:18.806Z","comments":true,"path":"2020/04/09/MySQL存储引擎下的索引/","link":"","permalink":"http://luxiaowan.github.io/2020/04/09/MySQL存储引擎下的索引/","excerpt":"","text":"MySQL存储引擎 MySQL支持多种存储引擎，我们可以通过show engines命令查看当前数据库所支持的存储引擎列表，那么不同存储引擎下的索引实现方式是否会有所不同？那是当然的，但只不过都大同小异而已，我们目前常用的存储引擎大多是InnoDB，主要是因为InnoDB支持事务、行锁、外键等功能，第二常用的就是MyIsam，但是在MySQL5.5版本之后，InnoDB就变成了默认的存储引擎，也可以看出InnoDB的强大和普适性。我们主要记录InnoDB和MyIsam下的索引实现。 MyIsam下的索引 MyIsam使用B+树作为索引结构，叶子节点存储的是目标数据的内存地址，大致结构图如下： 我们发现，每一个叶子节点内存储的都是指向具体数据的内存地址，可以通过地址直接找到数据。那么我们是不是可以推算出MyIsam的索引在磁盘上是怎么存储的？ 第一：索引存储的是具体数据的地址，和数据没有关系，只要通过地址就可以找到具体数据了 第二：索引可以单独维护，数据也可以单独维护，就比如是两个微服务项目 由这两点，我们可以推断出MyIsam的索引和数据应该是分开存储的，即索引存储一个文件，数据存储一个文件，当然还应该有表结构定义要单独一个文件，那么一张表是可以创建非常多的索引，是所有的索引都放在一个文件里，还是每个索引都创建一个文件，文件怎么命名？我们去MySQL的data目录下看看 这个目录下有很多的文件夹，每一个文件夹都对应了我们一个用户库，这说明每个库其实都是相互隔离的，隔离的方式就是以目录的形式，我们进到db_test目录下 我们事先在db_test库中创建了一张MyIsam引擎的表myisam_db(id, name)，并且创建了name字段的索引和id的主键索引，看到这一张表对应了三个文件，且文件是以表的名字命名的 *.frm：记录描述表结构文件，字段长度等 *.MYI：索引信息文件，记录所有的索引，My Index *.MYD：数据信息文件，存储数据信息， My Data 总结 MyIsam的索引是B+树的数据结构，叶子节点中存储的是数据的地址，实现了索引和数据分离，查询的过程中通过值找到数据地址，然后再根据数据地址将地址内存储的数据捞出返回。 其实我们可以单独的根据文件的存储方式反推出索引的存储策略了，自己反推试试 InnoDB下的索引 InnoDB和MyIsam一样都是采用B+树作为索引的数据结构，不同的在于InnoDB的叶子节点存储的是具体数据，而不是内存地址 我们都知道MySQL的每张表都必须创建一个主键索引，如果建表时未指定主键，那么MySQL引擎会选择一个非空唯一索引来当主键，若非空唯一索引也没有，那就会自动给添加一个自增的列来作为虚主键，InnoDB和MyIsam在存储上最大的一个区别就是索引和数据在同一个文件中，我们看一下InnoDB的表在磁盘上是怎么存储的。我们先在db_test中创建一张使用InnoDB存储引擎的表innodb_db，然后到/data/db_test目录下查看文件 磁盘上针对innodb_test表仅生成了两个文件，*.frm是表结构和字段描述等基础信息文件，重点来说一下*.ibd文件。 表空间(Tablespace) ibd是单表表空间文件，每个表使用一个表空间文件，存放用户数据库表数据、索引，InnoDB的每个数据文件都归属于一个表空间，不同的表空间都有唯一的标识space id来标记，系统表空间文件为ibdata1, ibdata2…，公用同一个表空间，用户创建的表产生的ibd文件都使用唯一的space id，只包含一个文件。 页(Page) 表空间文件，其中最基本的单位是页(Page)，每一个Page的大小默认为16k，所以ibd的大小必定是16k的整数倍，当然也可以通过innodb_page_size选项将页大小减少到8KB或4KB，或增加到32KB或64KB，但是只能在实例初始化之前设置，不支持动态设置。每个页由header、body、trailer等组成，header标识了类型和checksum信息，可以根据header的类型将body解析成对应的类型，body记录详细的内容，trailer则通过记录checksum等信息来确认该页是否已经写入完成。 区(Extent) Extent用于管理Page，每64个Page组成一个Extent，大小默认为1M，一个Extent内的所有页都是连续的，当表空间页容量不足要分配新页的时候，不会一页一页的分配，会一次性分配一个Extent，也就是连续的64个Page 段(Segment) Segment用于管理Extent，一个表至少会有两个Segment，一个用于管理叶子节点的Extent，一个用于管理非叶子节点的Extent，每增加一个索引就会多出两个Segment，一张表的Segment数量=索引数量*2 Memory下的索引 memory存储引擎是MySQL中的一类特殊的存储引擎。其使用存储在内存中的内容来创建表，而且所有数据也放在内存中，很少用到，至今我没用到过，不过在information_schema库中有很多使用Memory引擎的表。 Memory存储引擎默认使用哈希(HASH)索引，其速度比使用B型树(BTREE)索引快。如果我们需要使用B型树索引，可以在创建索引时选择使用。 Hash索引基于哈希表实现，只有匹配所有列的查询才有效。对于每一行数据，存储引擎都会对所有索引列计算一个哈希码，哈希码是一个较小的值，不同键值的行计算出的哈希码也不一样。哈希索引将所有的哈希码存储在索引中，同时保存指向每个数据行的指针。如果多个列的哈希值相同，索引会以链表的方式存放多个记录指针到同一个哈希条目中去。","categories":[],"tags":[{"name":"索引","slug":"索引","permalink":"http://luxiaowan.github.io/tags/索引/"}]},{"title":"MySQL索引初识","slug":"MySQL索引初识","date":"2020-04-08T14:10:00.000Z","updated":"2020-04-09T04:12:05.118Z","comments":true,"path":"2020/04/08/MySQL索引初识/","link":"","permalink":"http://luxiaowan.github.io/2020/04/08/MySQL索引初识/","excerpt":"","text":"索引是什么 索引是一种能够改变操作速度的特殊数据结构，索引的表达形式有很多种，生活中有很多用到索引的地方，我们举几个例子来理解： 图书馆书目标签：给每一本书建立唯一的编号，将编号和书架位置做好一一映射，这样我们就可以通过&quot;类型-编号&quot;快速的找到我们想要的书；有的同学可能会说：我们在查找的时候都是通过书的名字去查询的，没有用书的编号去查询啊，但也能查出来，这又是为啥？艾玛~这不就是书名和编号又做了一对多映射了么(因为书名重复的时候会有多个编号，所以用一对多)，我们可以先通过书名找出书的编号，然后再根据编号找到书在书架的位置，然后一锤定音(了解过MySQL索引的这时候是不是立刻想到了回表这个词)。这里书编号、书名就是图书馆书目的索引 词典：我们这里说一下汉语词典，我们可以按拼音、笔画、偏旁部首等排序的目录快速查找到需要的字。这里拼音、笔画、偏旁部首等就是词典的索引 酒店房卡：我们住酒店的时候，如果自助办完手续后，自助机吐出给我们一张房卡，然后如果房卡上没有门牌号的话，我们需要挨个儿房间的去尝试，酒店要是有几百个房间且我们正巧是最后一个房间的话，那估计等找到房子已经该要退房了，哈哈~所以房卡上直接告诉了我们房号，我们可以根据房号直接找到我们要入住的房间。这里房间号就是酒店房间的索引。 通过上面的例子，相信一定能理解索引是什么、作用是什么了吧，我们来总结一下：索引是标识一个物体/数据的特殊属性，这个属性可能是唯一的，也可能不是唯一的，可以是一个属性作索引，也可以是多个属性一起作索引。 索引有什么用 索引的作用是什么，我们为什么要用索引？ 相信很多人都会有这个问题，其实在数据量很小的情况下，比如一眼就看得到所有数据，那就没必要用索引了，为什么没必要？因为本来数据量就少，如果创建了索引的话，我们每次还要去查索引，然后从索引再去查具体数据，是不是很麻烦？并且有新数据插入或者索引的数据变更的时候，还需要同步去修改索引的数值，不然就会造成索引出现脏数据和数据无对应的索引等情况；若不建立索引的话，遍历所有的数据去查询某一条或者某几条出来，耗时貌似也不大，并且还不用在修改插入数据的时候也不用耗时去维护索引。 但是在数据量较大的情况下，我们就必须要建立索引了，因为如果我们为了查找一条数据而要去遍历几十万条数据的话，耗时是相当大的，特别是如果恰巧我们要查的数据在倒数第一条，那就彻底崩溃了，所以我们需要为数据集依据查询条目建立索引，我们先根据条件查询出来索引，然后把索引指向的存储位置上的数据捞出来直接返回就行了，是不是很方便？当然插入更新数据引起的索引更新相对于无索引查询带来的消耗来说，微乎其微。 聊到这里，肯定有一个疑问：我们在查询索引时不一样是要遍历所有的索引吗？这和遍历数据集中的所有数据有什么区别？ 索引结构 为了解决索引的快速查询和命中，我们有哈希和树两种结构，在不同的MySQL存储引擎中支持的索引结构不同，比如MyIsam支持Hash，InnoDB支持树，并且是B+树，当然也知道Hash，但是属于是变形的Hash，和MyIsam中的不同，且Hash索引的创建和维护完全由MySQL自己决定，只有在数据量达到一定数量的时候才会创建Hash，但是使用Hash结构也是有缺陷的： Hash结构索引 优势 Hash索引实际上就是采用一定的Hash算法，把键值转换成哈希值，检索的时候不需要像树那样从根开始找起，能够根据哈希值一次性定位到位置，所以在哈希冲突不严重的情况下，检索效率远高于树 缺陷 只能匹配是否相等，不能实现范围查找，只能用于=、IN和&lt;&gt; 无法使用Order By进行排序 组合索引的时候，无法实现最左匹配，因为组合索引会将所有的字段整合一起做Hash计算，所以如果仅部分索引字段进行Hash计算的话，可能匹配到其他数据 当数据量越来越大的时候，哈希冲突会越来越严重，性能会下降的很严重 结构图 树结构 树有多种，MySQL中使用的是B+树，这是由B树演变而来的，B树又是从AVL树演变而来，我们分别来看一下各种树的结构和检索方式，然后推导出为什么MySQL使用B+树。 AVL树 结构 特征 在AVL树中，任一节点对应的两棵子树的最大高度差为1，因此它也被称为高度平衡树。查找、插入和删除在平均和最坏情况下的时间复杂度都是O(log n)。增加和删除元素的操作则可能需要借由一次或多次树旋转，以实现树的重新平衡 作为索引 每个节点均能存储数据 每一个节点都需要进行一次磁盘IO 由于每一个节点最多只能有两个子节点，所以在数据量大的时候，树的高度会很大 B树 (B-树) 结构 特征 B树，概括来说是一个一般化的二叉查找树（binary search tree）一个节点可以拥有2个以上的子节点。与自平衡二叉查找树不同，B树适用于读写相对大的数据块的存储系统，例如磁盘。B树减少定位记录时所经历的中间过程，从而加快存取速度。B树这种数据结构可以用来描述外部存储。这种数据结构常被应用在数据库和文件系统的实现上。 作为索引 解决AVL树每个节点只能存储一个数据的问题 减少AVL树查询过程中的IO次数 B+树 结构 特征 B+树在非叶子节点存储关键字和指向下一节点的指针，仅在叶子节点存储数据，叶子节点之间依据关键字从小到大有序排列成一个有序链表； B+树是B树的一个升级版，相对于B树来说B+树更充分的利用了节点的空间，让查询速度更加稳定，其速度完全接近于二分法查找； 所有的叶子节点都在同一高度上，对于所有内部节点，子指针的数目总是与元素的数目相同。 作为索引 非叶子节点不保存关键字记录的指针，只进行数据索引，这样使非叶子节点能保存大量的关键字 所有的数据或数据地址只有在叶子节点才能获取到，所以每次查询所有的路径相同，查询稳定 非叶子节点的关键字都是从小到大有序排列 所有的叶子节点形成有序链表，方便于范围查询 总结 以上的内容带我们认识了什么是索引、索引有什么用、索引有哪些存储结构，基本上对索引有了一个最基本的认识，通过对几个存储结构的分析，我们可以看出为什么MySQL选择B+树：每一次的演变都是为了提供更快的查询，如何加快查询效率，就是减少IO的操作次数。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/categories/MySQL/"}],"tags":[{"name":"索引","slug":"索引","permalink":"http://luxiaowan.github.io/tags/索引/"}]},{"title":"getClassLoader能否为null","slug":"getClassLoader能否为null","date":"2020-04-08T01:30:00.000Z","updated":"2020-04-08T13:58:13.327Z","comments":true,"path":"2020/04/08/getClassLoader能否为null/","link":"","permalink":"http://luxiaowan.github.io/2020/04/08/getClassLoader能否为null/","excerpt":"","text":"前言 我们Javaer都知道类想要被加载是需要一个个ClassLoader来帮助的，并且类加载的方案叫双亲委派模式，说是双亲，其实就是单亲，可能我们最初的翻译人想让我们的加载器的家庭更完整吧，所以翻译成双亲。默认的类加载器包括BootstrapClassLoader、ExtClassLoader、AppClassLoader，他们都定义在在rt.jar中的sun.misc.Launcher类中，他们的&quot;继承&quot;关系是AppClassLoader—&gt;ExtClassLoader—&gt;BootstrapClassLoader，ExtClassLoader的parent获取不到BootstrapClassLoader，只能获取到一个null。 bootstrap是C++编写的类加载器，主要加载%JRE_HOME/lib/目录下的jar包 ExtClassLoader主要加载%JRE_HOME/lib/ext目录下的jar包 AppClassLoader主要加载java环境变量CLASSPATH所指定的路径下的jar包和class文件，通过System.getProperty(“java.class.path”)获取考验获取CLASSPATH路径 用户自定义ClassLoader，加载用户自己制定的类 getClassLoader会不会为空 说了这么多，其实就是想说getClassLoader当然可能会为空，是不是此时会有个疑惑：加载器都为空了，那这个类是怎么加载的，被谁加载的？答：被BootstrapClassLoader加载的。 我们知道BootstrapClassLoader是由C++编写的，我们是用Java代码获取不到的，BootstrapClassLoader也不是ExtClassLoader的父类，而是它的父亲，这里要搞清关系，父亲和父类是两码事，父类是有继承关系，父亲是上一层的关系，所以我们在获取String、Integer、int、double、BufferedInputStream等等一系列在rt.jar包中被BootstrapClassLoader加载的类的加载器时，返回的都是null。 ExtClassLoader是怎么成为AppClassLoader的父亲的 类加载器并非是继承关系，而是父子关系，就像上面说的BootstrapClassLoader是ExtClassLoader的父亲，不是父类。关键点就在于ClassLoader的实例变量parent，这个parent指定了当前类加载器的父亲，但是翻遍了AppClassLoader的代码也没发现是在哪里把ExtClassLoader设置进去的，怎么parent就是ExtClassLoader了呢。 刚才我们说这些类加载器被定义在了Launcher类中，那么我们就去看下这个类的构造器 123456789101112131415161718192021public Launcher() &#123; Launcher.ExtClassLoader var1; try &#123; // 获取ExtClassLoader var1 = Launcher.ExtClassLoader.getExtClassLoader(); &#125; catch (IOException var10) &#123; throw new InternalError(\"Could not create extension class loader\", var10); &#125; try &#123; // 获取AppClassLoader实例并赋值给loader，并把ExtClassLoader的实例传入到方法中， this.loader = Launcher.AppClassLoader.getAppClassLoader(var1); &#125; catch (IOException var9) &#123; throw new InternalError(\"Could not create application class loader\", var9); &#125; // ...&#125;// 返回AppClassLoader实例public ClassLoader getClassLoader() &#123; return this.loader;&#125; 从Launcher的构造器我们看到关键点在AppClassLoader.getAppClassLoader(var1)这句，那我们就看这个方法是怎么写的 1234567891011121314151617181920static class AppClassLoader extends URLClassLoader &#123; public static ClassLoader getAppClassLoader(final ClassLoader var0) throws IOException &#123; final String var1 = System.getProperty(\"java.class.path\"); final File[] var2 = var1 == null ? new File[0] : Launcher.getClassPath(var1); return (ClassLoader)AccessController.doPrivileged(new PrivilegedAction&lt;Launcher.AppClassLoader&gt;() &#123; public Launcher.AppClassLoader run() &#123; URL[] var1x = var1 == null ? new URL[0] : Launcher.pathToURLs(var2); // 关键看这一句，调用了AppClassLoader的构造器，并把ExtClassLoader实例传了进去，那就跳到构造器去看 return new Launcher.AppClassLoader(var1x, var0); &#125; &#125;); &#125; // 构造器 AppClassLoader(URL[] var1, ClassLoader var2) &#123; // 调用了父类的构造器，他的父类是哪个？从定义上看应该是URLClassLoader没跑了 super(var1, var2, Launcher.factory); this.ucp.initLookupCache(this); &#125;&#125; 其实再往里的代码就不用在这闲扯了，里面就是不断的将var2往上传递，直到ClassLoader这个类的构造器中，在ClassLoader中完成的设置。 那为什么ExtClassLoader没通过这种形式将BootstrapClassLoader设置给parent呢？ 是不是傻，上面刚说了BootstrapClassLoader是C++写的，Java代码不能直观的获取。","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[]},{"title":"Java获取项目中的文件","slug":"Java获取项目中的文件","date":"2020-04-07T15:17:00.000Z","updated":"2020-04-07T15:43:48.016Z","comments":true,"path":"2020/04/07/Java获取项目中的文件/","link":"","permalink":"http://luxiaowan.github.io/2020/04/07/Java获取项目中的文件/","excerpt":"","text":"Class.getResource()与ClassLoader.getResource()的区别 Class.getResource()是指AAA.class.getResource() Class.getResource(String name)：在当前class类的同一路径下查找资源 ClassLoader.getResource(String name)：在根目录下查找该资源文件，即&quot;/&quot;或classpath目录 粗解 在当前项目目录下查找指定的文件，此方式只能加载当前项目classpath下的文件，用.表示当前目录，不能以/开头，会报错 1234// 方式1URL url = ClassLoader.getSystemResource(\".\");// 方式2url = LoaderResourceTest.class.getClassLoader().getResource(\".\"); url会输出当前class文件所在的根目录，比如当前我输出的结果就是file:/Users/chuan/Documents/projects_code/java-project/demo/demo-core/target/classes/ 以当前类所在的目录为基础，在当前目录查找并加载指定名称的资源文件，比如当前类LoaderResourceTest在项目的cc.kevinlu.demo.core.loader包中，若是我们以LoaderResourceTest.class.getResource(&quot;LoaderT.class&quot;)的方式加载一个文件，则首先会在cc.kevinlu.demo.core.loader包中查找LoaderT.class文件，若查找不到则返回null，如果我们想要加载其他包中的文件，则可以使用&quot;/&quot;开头并加上文件在项目中的完整包路径，比如LoaderResourceTest.class.getResource(&quot;/cc/kevinlu/demo/core/gof/facade/FacadeTest.class&quot;) 12345// 获取当前类所在的包目录url = LoaderResourceTest.class.getResource(\".\");// 获取其他包内的文件url = LoaderResourceTest.class.getResource(\"/cc/kevinlu/demo/core/gof/facade/FacadeTest.class\"); 这两条语句的输出： 123file:/Users/chuan/Documents/projects_code/java-project/demo/demo-core/target/classes/cc/kevinlu/demo/core/loader/file:/Users/chuan/Documents/projects_code/java-project/demo/demo-core/target/classes/cc/kevinlu/demo/core/gof/facade/FacadeTest.class 由此可以很明显的看出Class的getResource是获取当前类所在的包目录，如果不同的包中都有相同名称的资源文件，但是仅针对当前包可用，那么就可以使用这种方式去加载读取，不会出现读到其他目录的文件的情况，但是这仅受限于我们写的类，不适用于jar包中的。 如果我们想读取项目中引用的jar包内的文件，那么怎么办？其实和读取项目中自己写的文件是相同的办法，因为在系统启动的时候会将jar包中的文件加载到classpath目录下，所以读取方式一样的。 以上讲解的只是读取单个资源，那么要是想把所有匹配到的资源都获取到，则可以使用Class.getClassLoader().getResources()和ClassLoader.getSystemResources()两种方式 12345678910Enumeration&lt;URL&gt; urls = LoaderResourceTest.class.getClassLoader().getResources(\"META-INF/spring.factories\");while (urls.hasMoreElements()) &#123; System.out.println(urls.nextElement());&#125;System.out.println(\"--ClassLoader.getSystemResources--\");urls = ClassLoader.getSystemResources(\"META-INF/spring.factories\");while (urls.hasMoreElements()) &#123; System.out.println(urls.nextElement());&#125; 读取资源文件内容方式 123456InputStream is = LoaderResourceTest.class.getClassLoader().getResourceAsStream(\"META-INF/spring.factories\");BufferedReader br = new BufferedReader(new InputStreamReader(is));String line = null;while ((line = br.readLine()) != null) &#123; System.out.println(line);&#125; 整个测试文件代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package cc.kevinlu.demo.core.loader;import java.io.BufferedReader;import java.io.InputStream;import java.io.InputStreamReader;import java.net.URL;import java.util.Enumeration;import org.springframework.boot.admin.SpringApplicationAdminMXBean;public class LoaderResourceTest &#123; public static void main(String[] args) throws Exception &#123; // 在当前项目目录下查找指定的文件，此方式只能加载当前项目classpath下的文件，用.表示当前目录，不能以/开头，会报错 URL url = ClassLoader.getSystemResource(\".\"); System.out.println(url); // 此方式和ClassLoader.getSystemResource一样 url = LoaderResourceTest.class.getClassLoader().getResource(\".\"); System.out.println(url); // 以当前类所在的目录为基础，在当前目录查找并加载指定名称的资源文件，比如当前类LoaderResourceTest在 // 项目的cc.kevinlu.demo.core.loader包中，若是我们以LoaderResourceTest.class.getResource(\"LoaderT.class\")的 // 方式加载一个文件，则首先会在cc.kevinlu.demo.core.loader包中查找LoaderT.class文件，若查找不到则返回null，如果我们想要加载其他包中的文件， // 则可以使用/开头并加上文件在项目中的完整包路径，比如LoaderResourceTest.class.getResource(\"/cc/kevinlu/demo/core/gof/facade/FacadeTest.class\") url = LoaderResourceTest.class.getResource(\".\"); System.out.println(url); url = LoaderResourceTest.class.getResource(\"/cc/kevinlu/demo/core/gof/facade/FacadeTest.class\"); System.out.println(url); // 获取jar包内的文件有以下方式 // 1. 使用Class.getClassLoader().getResource()，这种方式只能获取第一个匹配到的文件 System.out.println(\"************\"); url = LoaderResourceTest.class.getClassLoader().getResource(\"META-INF/spring.factories\"); System.out.println(url); System.out.println(\"************\"); // 2. 使用Class.getResource()，这种方式也只能获取第一个匹配到的文件 url = LoaderResourceTest.class.getResource(\"META-INF/spring.factories\"); System.out.println(url); // 3. 使用ClassLoader.getSystemResource()方法，这种也仅仅只是获取第一个匹配的文件 url = ClassLoader.getSystemResource(\"META-INF/spring.factories\"); System.out.println(url); // 那我们如果想要获取到所有匹配到的文件要怎么办呢？那么就使用Class.getClassLoader().getResources()，这个方法会返回一个Enumeration&lt;URL&gt;实例， // 我们迭代返回的这个实例就可以读取所有匹配到的所有文件了，如果没有匹配到任何文件也可以大胆的去迭代，不用判空，因为在getResources方法中进行了非空封装 System.out.println(\"-----\"); Enumeration&lt;URL&gt; urls = LoaderResourceTest.class.getClassLoader().getResources(\"META-INF/spring.factories\"); while (urls.hasMoreElements()) &#123; System.out.println(urls.nextElement()); &#125; System.out.println(\"-----\"); urls = ClassLoader.getSystemResources(\"META-INF/spring.factories\"); while (urls.hasMoreElements()) &#123; System.out.println(urls.nextElement()); &#125; System.out.println(\"-----\"); // 如果想要读取文件的具体内容，那么我们就要用到文件流了，常用的方法就是使用BufferedReader去读了 // getResourceAsStream()方法，它相当于你用getResource()取得File文件后，再new InputStream(file)一样的结果 InputStream is = LoaderResourceTest.class.getClassLoader().getResourceAsStream(\"META-INF/spring.factories\"); BufferedReader br = new BufferedReader(new InputStreamReader(is)); String line = null; int i = 1; while (i == 1 &amp;&amp; (line = br.readLine()) != null) &#123; System.out.println(line); i++; &#125; System.out.println(\"*******\"); url = SpringApplicationAdminMXBean.class.getResource(\".\"); System.out.println(url); &#125;&#125;","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[]},{"title":"运行Jar包时指定启动端口","slug":"运行jar包时指定启动端口","date":"2020-04-02T13:30:00.000Z","updated":"2020-04-02T13:32:00.255Z","comments":true,"path":"2020/04/02/运行jar包时指定启动端口/","link":"","permalink":"http://luxiaowan.github.io/2020/04/02/运行jar包时指定启动端口/","excerpt":"","text":"java -jar xxx.jar --server.port=9090 –server.port一定要在最后","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[]},{"title":"浅谈RPC","slug":"浅谈RPC","date":"2020-04-01T17:10:00.000Z","updated":"2020-04-07T15:44:11.888Z","comments":true,"path":"2020/04/02/浅谈RPC/","link":"","permalink":"http://luxiaowan.github.io/2020/04/02/浅谈RPC/","excerpt":"","text":"前言 RPC是Remote Procedure Call的简称，译为远程过程调用，何为远程？就是从这个地方到另一个地方，也就是调用双方不在同一个进程或者不在同一台服务器上，由于双方不在一个内存空间中，所以需要借助网络来实现通信和数据传递。 工作过程 A服务器上的进程P1调用A服务器上的进程P2，或者A服务器上的进程P1调用B服务器上的P2，P1发起调用行为之后，进程被挂起，P2接收到请求之后开始执行，然后返回结果给P1，P1继续执行。P1可以使用参数将信息传递给P2，然后通过P2传回的消息得到结果。 Client function中像调用本地方法一样调用远程服务 Client stub接收到调用请求后，将方法、参数序列化 Client通过Socket将消息发送到远程服务 Server接收到消息后转发给Server stub，Server stub将消息进行解码（反序列化） Server stub根据解码后的内容调用本地Server服务 Server执行完本地方法后，将执行结果返回给Server stub Server stub将结果进行编码（序列化），然后传给Socket 然后通过Socket将消息发送给Client Client接收到返回的消息后转发给Client stub，随之进行解码（反序列化） Client接收到返回数据做最终处理 以上就是一次RPC请求的全过程，整个RPC调用过程有同步和异步两种方式，同步也就是Client在接收到Server返回的消息之前一直将请求保持在运行中状态，例如IO和BIO；异步则是Client将请求发出去之后就将请求挂起，然后Server执行完后再发消息过来通知服务唤醒对应的请求继续处理，例如NIO和AIO。 特点 在RPC理论被提出时就被赋予了简单、高效、通用三个特点。 简单：RPC概念语义清晰，使建立分布式应用更加简单，服务之间调用方式简单 高效：过程调用使用起来很高效，会隐藏底层的通信细节，不需要我们之间去处理Socket通信 通用：RPC是一个请求响应模型，调用双方可以使用不同的编程语言去实现，选择合适的序列化方式即可 简单点说，就是RPC能够使我们像调用本地方法一样去调用远程方法，并且调用者不需要知道远程服务到底部署在何处，达到像傻子一样去编程。 序列化 在说到RPC调用过程的时候，有反复的提到序列化和反序列化，RPC支持的数据序列化方式有Java序列化、JSON、Hessian、Kryo等，还有我们很早之前WebService常用的XML。 JSON： 可读性较高 无法表示数据内的引用关系 统一JSON字段命名规范较困难 Hessian二进制 几乎无可读性而言 支持的语言不够多，比如直接忽略了对js的支持 二进制兼容性较高 二进制传输速度比XML、JSON快 RPC框架 Dubbo 阿里研发的，后由当当网扩展出了Dubbox，阿里的已经不维护了，现在丢给Apache了 Feign Netflix的一套，由Spring Cloud拓展出来了一套轻量级的，很好的融合于Spring Cloud，微服务中还是常用的，适用于Java gRPC 谷歌的一套RPC框架，基于HTTP/2 协议传输，在多语言服务之间交互时常被采用 Java RMI 较古老的一款RPC框架，仅适用于Java程序，无法跨语言，用起来也不方便 Thrift Facebook开源的一套RPC框架，它主要是一种接口描述语言和二进制通信协议，跨语言协作，现在丢给Apache了 SOAP 由XML-RPC演变而来，不常用，老项目里可能会用到","categories":[{"name":"分布式","slug":"分布式","permalink":"http://luxiaowan.github.io/categories/分布式/"}],"tags":[]},{"title":"聊一聊SPI","slug":"聊一聊SPI","date":"2020-04-01T17:10:00.000Z","updated":"2020-04-05T14:06:22.399Z","comments":true,"path":"2020/04/02/聊一聊SPI/","link":"","permalink":"http://luxiaowan.github.io/2020/04/02/聊一聊SPI/","excerpt":"","text":"前言 SPI全称是Service Provider Interface，是一种服务发现机制（哎，服务发现机制？和Zookeeper什么关系？）。SPI 的本质是将接口实现类的全限定名配置在文件META-INFO/services目录下以接口全限定名命名的文件中，并由服务加载器(ServiceLoader)读取配置文件，加载实现类。这样可以在运行时，动态为接口替换实现类。正因此特性，我们可以很容易的通过 SPI 机制为我们的程序提供拓展功能。 使用方式 场景 如果在使用过程中需要动态替换接口类的实现逻辑，那么比较适合使用SPI。比如java.sql.Driver。 方式 在服务提供者提供了接口的一种具体实现后，在jar包的META-INF/services目录下创建一个以接口全限定名为命名的文件，内容为实现类的全限定名 通过maven引入接口实现类所在的jar包，或者将jar包放到应用的classpath中 通过java.util.ServiceLoder动态装载接口实现模块，它通过扫描META-INF/services目录下的配置文件找到实现类的全限定名，把类加载到JVM SPI的实现类必须有一个无参的public构造器 疑问 为什么要把定义放在META-INF/services目录下？ 这个目录位置是在ServiceLoader中定义的，并且还是final不能修改的，所以还是乖乖遵守吧，因为ServiceLoader也是final的，所以也不能从这个类进行扩展。 1234public final class ServiceLoader&lt;S&gt; implements Iterable&lt;S&gt;&#123; private static final String PREFIX = \"META-INF/services/\";&#125; 为什么需要一个无参的public构造器？ ServiceLoader内部实例化扫描到的类是通过LazyIterator迭代器在迭代的时候使用Class的newInstance()方法，所以这里必须要有一个无参的public构造器，否则实例化的时候会报错。 如果引入的多个jar包中均有目标接口的实现类及SPI定义，咋整？ 全部都会被加载，只要是在项目中被引入了，就会全部被加载。 运行过程是什么样子的？ 案例 服务方 接口 1234567package cc.kevinlu.spidemo.spi;public interface SPIUserService &#123; void name();&#125; 实现类 123456789101112131415// 领导实现类public class LeaderServiceImpl implements SPIUserService &#123; @Override public void name() &#123; System.out.println(\"领导发话了\"); &#125;&#125;// 员工实现类public class EmployeeServiceImpl implements SPIUserService &#123; @Override public void name() &#123; System.out.println(\"员工暴怒了\"); &#125;&#125; spi文件 在实现类所在的项目中的resources目录下创建文件夹META-INF/services文件夹，然后创建文件cc.kevinlu.spidemo.spi.SPIUserService 12cc.kevinlu.spidemoimpl.spi.LeaderServiceImplcc.kevinlu.spidemoimpl.spi.EmployeeServiceImpl 项目结构 调用方 pom.xml引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;cc.kevinlu&lt;/groupId&gt; &lt;artifactId&gt;spi-demo-impl&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 调用 123456789101112ServiceLoader&lt;SPIUserService&gt; services = ServiceLoader.load(SPIUserService.class);// 隐式迭代器方式for (SPIUserService service : services) &#123; service.name();&#125;System.out.println(\"---------------\");// 显式迭代器方式Iterator&lt;SPIUserService&gt; iterator = services.iterator();while (iterator.hasNext()) &#123; SPIUserService service = iterator.next(); service.name();&#125; MySQL 在mysql-connector-java包中的META-INFO/services目录下可以找到以接口java.sql.Driver为名的文件，文件内容是com.mysql.jdbc.Driver或者com.mysql.cj.jdbc.Driver。再来我们就需要找到是在哪里对这个类进行加载的，我们从com.mysql.jdbc.Driver可以进入到java.sql.DriverManager，哦哟~原来是在DriverManager类中的静态代码块实现的对目标类的加载 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * Load the initial JDBC drivers by checking the System property * jdbc.properties and then use the &#123;@code ServiceLoader&#125; mechanism */static &#123; loadInitialDrivers(); println(\"JDBC DriverManager initialized\");&#125;private static void loadInitialDrivers() &#123; String drivers; // If the driver is packaged as a Service Provider, load it. // Get all the drivers through the classloader // exposed as a java.sql.Driver.class service. // ServiceLoader.load() replaces the sun.misc.Providers() AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; ServiceLoader&lt;Driver&gt; loadedDrivers = ServiceLoader.load(Driver.class); Iterator&lt;Driver&gt; driversIterator = loadedDrivers.iterator(); /* Load these drivers, so that they can be instantiated. * It may be the case that the driver class may not be there * i.e. there may be a packaged driver with the service class * as implementation of java.sql.Driver but the actual class * may be missing. In that case a java.util.ServiceConfigurationError * will be thrown at runtime by the VM trying to locate * and load the service. * * Adding a try catch block to catch those runtime errors * if driver not available in classpath but it's * packaged as service and that service is there in classpath. */ try&#123; while(driversIterator.hasNext()) &#123; driversIterator.next(); &#125; &#125; catch(Throwable t) &#123; // Do nothing &#125; return null; &#125; &#125;); println(\"DriverManager.initialize: jdbc.drivers = \" + drivers);&#125; 疑 前几天面试的时候，有个同学问我：我们订单类型分很多种，有美食、外卖、酒店、门票、火车票、机票等等，我们这边是做一个中台(中间平台)，为客户端提供统一下单、出单、订单等服务，但是订单需要流转到相对应的业务部门，怎么做？ 我说SDK、微服务等形式，比如下单的时候由中台判断订单要流转的业务线，然后依据策略调用不同业务线提供的API服务。 然后又问我每个业务线提供的接口定义不同，调用起来会很麻烦，怎么办？ 我说这个很简单啊，由中台定义接口，业务方引入中台的接口依赖，然后实现中台提供的接口，去编写自己的业务逻辑，并把该api暴露给中台去调用，这样中台对外的接口定义都一致，其实就是对外统一接口请求规范。 接着这位同学问了句：你知道SPI么？ 艾玛~SPI会把业务方的实现代码依赖进来，如果某个业务方的处理逻辑改变了，或者他们的包版本变更了，岂不是所有的依赖方都要去更新依赖？这种方式其实不是不可取，只是会麻烦一些，并且我们把所有的业务方的实现都依赖进来了，那么我们使用ServiceLoader加载之后在使用的时候判断起来也麻烦。 是吗？","categories":[{"name":"分布式","slug":"分布式","permalink":"http://luxiaowan.github.io/categories/分布式/"}],"tags":[]},{"title":"Spring Cloud全家桶初探","slug":"SpringCloud全家桶初探","date":"2020-04-01T14:56:00.000Z","updated":"2020-04-05T14:07:16.784Z","comments":true,"path":"2020/04/01/SpringCloud全家桶初探/","link":"","permalink":"http://luxiaowan.github.io/2020/04/01/SpringCloud全家桶初探/","excerpt":"","text":"什么是Spring Cloud 官网上面有一段话：Spring Cloud为开发人员提供了快速构建分布式系统中的一些常见模式的工具(例如配置管理、服务发现、断路器、智能路由、微代理、控制总线、一次性令牌、全局锁、领导选举、分布式会话、集群状态)。 Spring Boot是Spring的一套快速配置脚手架，可以基于Spring Boot快速开发一个微服务应用，而Spring Cloud是一个基于Spring Boot实现云应用的开发工具； Spring Boot专注于快速、方便的创建单个微服务，Spring Cloud专注于微服务全局的服务治理框架； Spring Boot使用了约定大于配置的理念，大部分集成方案都预设好了，不需要过多的配置，或者说能不配置就不配置，而Spring Cloud是基于Spring Boot来实现的，也就是说Spring Boot可以独立于Spring Cloud，而Spring Cloud强依赖于Spring Boot。 什么是微服务 微服务架构 分散：不同的功能模块部署在不同的服务器/容器中，减轻功能模块高并发带来的压力 集群：不同服务器/容器中部署相同的功能模块，通过负载均衡服务配置实现功能模块的高可用 微服务：微服务架构简单来说就是将web应用拆分成一系列小的服务应用，这些应用可以独立的编译、部署，应用之间通过暴露各自的API实现通信，共同组成一个完整的web应用 微服务的特点 单一职责：每一个微服务模块都对应不同的服务功能，负责单一业务的业务实现 微/细：服务拆分的粒度很小，但依据分久必合合久必分原则，微服务之间也是可以进行再拆分或合并的 面向服务：每个服务应用对外暴露自己的API，调用者不需要关注具体的业务实现 自我治理： 服务独立，研发团队独立 技术独立：只要提供相应的API即可，实现技术和实现语言不必一致 前后端分离 配置独立 解耦：独立部署，通过RPC或REST方式通信，耦合影响较小 服务容错、限流 微服务的劣势 微服务使整个应用分散成多个服务应用，定位问题非常困难（trace解决定位难的问题） 稳定性下降，服务数量过多会导致整个应用出现问题的概率变大，其中一个服务挂掉就可能导致整个应用不可用，访问量越大出问题的可能性越大 服务数量过多，部署、管理的工作量变大 开发的过程中很难实现相互依赖的服务之间同步进行（mock解决此问题） 测试难度增大，由原先的单体应用测试变成服务间调用的测试，测试过程更加复杂 服务运行过程中可能会经常发生服务宕机，所以对于微服务必须建立完善的服务监控体系，尽可能的第一时间发现故障服务并进行故障通知、转移和恢复（Zookeeper、Eureka、Consul、Etcd等） 微服务拆分依据 微服务拆分不是一蹴而就的，而是需要在开发过程中不断的去分析和理清每一个服务的边界。对于老工程中尚未分清拆分方向的，可先留于其中，最终可考虑将这些功能作为一个微服务。 基于业务逻辑 基于可扩展 基于可靠性 基于性能 微服务拆分规范 粒度：先少后多，先粗后细 调用：保持单向调用，尽量禁止循环调用，比如订单—&gt;产品，产品—x&gt;订单 接口幂等：应保证接口的幂等性，避免出现脏数据 纵向拆分尽量少于三层，也即维持在控制层—&gt;业务服务层—&gt;基础服务层 先拆分服务，后拆分数据库 基础元件 Config 分布式服务，由于服务数量较多，每一个服务都会有1+套配置文件，如果每个项目单独配置一个yml/properties文件，管理起来会很混乱，并且无法实现动态变更配置属性的值，所以我们需要一个分布式配置中心组件，Spring Cloud Config就因此应运而生，它支持配置信息放在配置服务的内存中，也支持放在远程的git/svn仓库中，Config分两个角色，一个Server和一个Client。 Server：创建一个简单的Config Server，使用git作为配置中心，我们再git仓库中创建目录config，这个目录名称需要和spring.cloud.config.server.git.search-paths配置的一致，然后在目录中创建client1-dev.properties pom.xml 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt;&lt;/dependency&gt; application.properties 12345server.port=7701spring.cloud.config.server.git.uri=http://gitlab.xx.com/spring-cloud/config-center.gitspring.cloud.config.server.git.username=xxxxxspring.cloud.config.server.git.password=xxxxxspring.cloud.config.server.git.search-paths=config spring.cloud.config.server.git.uri：git仓库地址 spring.cloud.config.server.git.search-paths：git仓库地址下的相对地址，可以配置多个，用,分割 spring.cloud.config.server.git.username：git仓库登录用户名 spring.cloud.config.server.git.password：git仓库登录密码 启动类 123456789@EnableConfigServer@SpringBootApplicationpublic class SpringCloudConfigApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringCloudConfigApplication.class, args); &#125;&#125; 我们再启动类加上@EnableConfigServer注解，表明该项目是Config Server项目，启动后访问http://localhost:7701/client1/dev，如果返回如下，则表示我们已经配置正常，可以使用： { “name”: “client1”, “profiles”: [ “dev” ], “label”: null, “version”: “e3741fe5e48b303c80f34c1b8a44c0ef2999e22b”, “state”: null, “propertySources”: [ { “name”: “http://gitlab.xxx.com/spring-cloud/config-demo.git/config/client1-dev.yml”, “source”: { “name”: “cc” } } ] } ☆说明： 仓库中的配置文件会被转换成 Web 接口，访问可以参照以下的规则： /{application}/{profile}[/{label}] /{application}-{profile}.yml /{label}/{application}-{profile}.yml /{application}-{profile}.properties /{label}/{application}-{profile}.properties 上面的 URL 会映射 {application}-{profile}.yml 对应的配置文件，其中 {label} 对应 Git 上不同的分支，默认为 master。以 config-client-dev.yml 为例子，它的 application 是 config-client，profile 是 dev。 Client pom.xml 123456789101112131415&lt;!-- web项目 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- config --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 动态刷新配置信息 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 我们引入了actuator，目的是可以在修改了git仓库中的配置信息之后，可以手动刷新，让项目读取最新的配置信息，而不需要重启我们的应用 bootstrap.properties config的配置必须配置在bootstrap.properties文件中，这样才能被正确的加载，按照springboot的配置文件加载顺序来看，会先加载bootstrap，然后再加载application，并且我们的config的配置要优先于项目中的其他内容 1234spring.cloud.config.profile=devspring.cloud.config.label=devspring.cloud.config.uri=http://127.0.0.1:7701/spring.cloud.config.name=client1 label：对应git仓库的分支名称，默认是master，我们这里dev表示我们使用的是dev分支，若分支不存在，则报错 profile：环境，对应文件后缀 uri：Config Server地址 name：配置中心的项目文件夹名称，若配置一个不存在的文件名，则会报错 application.properties 123spring.application.name=client1server.port=8080management.endpoints.web.exposure.include=health, info, refresh 在application.properties中指定端口和项目名称，因为我们引入的actuator，但是它默认暴露的接口是health和info两个接口，其他的都涉及到安全所以需要手动将接口暴露出来，因为我们要可以动态刷新，所以将refresh接口也暴露出来，以POST方式请求该接口：curl -X POST http://127.0.0.1:8080/actuator/refresh TestController.java 123456789101112131415@RefreshScope@RestController@RequestMapping(\"/index\")public class TestController &#123; @Value(\"$&#123;name&#125;\") private String name; @GetMapping(\"/one\") public String one() &#123; System.out.println(name); return name; &#125;&#125; 注解@RefreshScope是指明该类下的属性注入可以动态刷新 测试 访问http://127.0.0.1:8080/index/one 修改client-dev.properties文件中name=one_1 刷新：curl -X POST http://127.0.0.1:8080/actuator/refresh 访问http://127.0.0.1:8080/index/one Eureka 服务注册发现中心，基于CAP理论的AP实现，一个基于REST的服务，用于服务的发现和定位，以实现云端中间层服务发现和故障转移。Eureka的AP原则保证了注册中心的高可用，它是一个去中心化的架构，也就是集群中的所有节点都无主从之分，每一个节点都是平等的，可以相互注册，每一个节点都需要添加一个或多个serviceUrl指向其他节点，每个节点都可以视为其他节点的副本，这一特点实现了Eureka的高可用。 当有一台Server宕机后，Eureka Client会将请求自动转发到其他的Server节点上，当故障的Server节点恢复之后，Eureka会将其再次加入到服务器集群中。 当一个新的Server启动并加入进群后，会首先从其他的临近节点获取所有的可用服务列表信息完成初始化。 Server：创建一个Eureka Server中心只需要在启动类上加上注解@EnableEurekaServer即可，启动服务之后通过访问http://127.0.0.1:8080/进入eureka控制中心，我们需要在pom.xml中引入一个jar包 12345678910111213141516171819 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;version&gt;2.2.6.RELEASE&lt;/version&gt; &lt;/dependency&gt; ↓(内嵌) &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-netflix-eureka-server&lt;/artifactId&gt; &lt;version&gt;2.2.2.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; ↓(内嵌) &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-netflix-eureka-client&lt;/artifactId&gt;&lt;version&gt;2.2.2.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; 1234567 @SpringBootApplication @EnableEurekaServer public class EurekaServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaServerApplication.class, args); &#125;&#125; 如果我们仅仅是部署一个Eureka Server，那么我们还需要修改一下eureka.client前缀的配置 123eureka.client.register-with-eureka=false eureka.client.fetch-registry=falseeureka.client.service-url.defaultZone=http://127.0.0.1:$&#123;server.port&#125;/eureka/ 这些配置信息对应的类是在spring-cloud-netflix-eureka-client.jar包中的org.springframework.cloud.netflix.eureka.EurekaClientConfigBean类中，看下这个类中上面三个参数的默认值： 123456789101112131415161718192021222324 @ConfigurationProperties(EurekaClientConfigBean.PREFIX) public class EurekaClientConfigBean implements EurekaClientConfig, Ordered &#123; // 默认的eureka客户端配置前缀，就用在上面↑ public static final String PREFIX = \"eureka.client\"; // Eureka默认的访问地址，如果我们不修改这个配置的话，就可以通过8761端口的这个地址去访问 public static final String DEFAULT_URL = \"http://localhost:8761\" + DEFAULT_PREFIX + \"/\"; // 如果没设置zone就使用下面这个默认的 public static final String DEFAULT_ZONE = \"defaultZone\"; // 指定是否将当前服务注册到Eureka Server以被其他服务发现和使用 // 如果不想使当前应用被其他服务发现，则修改此属性为false private boolean registerWithEureka = true; // 指定当前服务是否从Eureka Server拉取监听的服务列表 private boolean fetchRegistry = true; // Eureka Server访问地址，一个Map，可以存储很多值 private Map&lt;String, String&gt; serviceUrl = new HashMap&lt;&gt;(); &#123; // 将默认的server地址放入服务地址Map中 this.serviceUrl.put(DEFAULT_ZONE, DEFAULT_URL); &#125;&#125; Client：创建一个Eureka Client很简单，在启动类上加上注解@EnableEurekaClient即可，如果Eureka Server服务的端口并不是使用的8761，那么则需要修改一下配置信息： 1eureka.client.service-url.defaultZone=http://127.0.0.1:8080/eureka/ 仅仅只需要修改折一个配置即可，因为其他的默认都为true，不用再配置了，这就是Spring Boot所说的约定大于配置 1234567 @SpringBootApplication @EnableEurekaClient public class EurekaServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaServerApplication.class, args); &#125;&#125; 自我保护模式 Eureka Server在运行期间会不断的统计心跳失败比例，如果再15分钟内有超过85%的client未正常发送心跳过来(单机模式很容易满足，生产环境如果出现这个情况则一般是网络通信出现了问题)，那么Eureka Server就会认为自己和客户端出现了网络故障，就会进入自我保护模式 自我保护模式的原则是宁可放过不可错杀，意思就是即使服务宕机了，我们在注册中心看到的服务状态依然是UP 自我保护模式使的Eureka实现了CAP中的AP，默认是开启的，可以通过参数eureka.server.enable-self-preservation=false进行关闭，但是在实际生产环境中不建议关闭，因为如果关闭的话，可能就直接导致服务不可用了 在正常情况下，如果超过默认的90秒未接收到某client的心跳，则将该client做下线处理，并从服务列表中移除掉；但是在保护模式下，不会将无心跳的client从服务列表中移除 自我保护模式下，当前Server节点依然可以接收通信正常的服务的注册和发现，但是不会将新注册的服务同步给集群中的其他Server节点，只有待网络恢复且退出自我保护模式之后，才会将自我保护模式期间新注册的服务同步给其他节点 Eureka的服务健康检查是通过actuator的/info和/health来实现的 Consul Consul用于实现分布式系统的服务发现与配置，其它分布式服务注册与发现的方案，满足CAP的CP，Consul 的方案更“一站式”，内置了服务注册与发现框 架、分布一致性协议实现、健康检查、Key/Value 存储、多数据中心方案，不再需要依赖其它工具（ZooKeeper等），Consul可以与Docker完美融合使用Go语言开发，基于 Mozilla Public License 2.0 的协议开源。 Consul工作原理 Producer启动的时候，会向Consul发送自己的服务信息，比如IP、port等 Consul每隔10秒(默认)会向Producer发送一个心跳监控请求，检测Producer是否健康 Consumer每隔10秒(默认)会向Consul拉取一次服务列表缓存在本地，Consumer的请求都基于本地的服务缓存列表进行请求 安装Consul服务 Docker运行（单机版） 12345678910111213# 1. 查询consul可用镜像docker search consul# 2. 拉取consul镜像docker pull consul:latest# 3. 启动镜像docker run \\--name consul \\-p 8599:8500 \\-v /data/consul/conf/:/consul/conf/ \\-v /data/consul/data/:/consul/data/ \\-d consul 正常安装 下载地址：https://www.consul.io/downloads.html 帮助文档：https://learn.hashicorp.com/consul/getting-started/install 访问consul：http://127.0.0.1:8599 Consul与客户端集成 pom.xml 123456789101112131415&lt;!-- web包，必须引入，否则服务无法注册到注册中心 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- consul discovery --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-consul-discovery&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 心跳 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; application.properties 12345server.port=7702spring.application.name=consul-demospring.cloud.consul.host=127.0.0.1spring.cloud.consul.port=8500#spring.cloud.consul.discovery.health-check-path=/health spring.cloud.consul.host：consul服务IP spring.cloud.consul.port：consul服务端口 spring.cloud.consul.discovery.health-check-path：consul心跳地址，默认为actuator/health，我们可以自定义 启动类 12345678910// 利用注解@EnableDiscoveryClient开启服务自动注册和发现@EnableDiscoveryClient@SpringBootApplicationpublic class SpringCloudConsulApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringCloudConsulApplication.class, args); &#125;&#125; 运行机制 Consul的一致性算法采用了Raft，比Zookeeper的Paxos算法简单很多 强一致性给Consul带来了可用性的下降 当一个服务注册到leader节点时，因为Raft算法要求所有节点数投票同意过半才能将服务真正的注册进来，所以新服务注册的时间被拉长了； 当leader节点挂掉之后，需要根据Raft算法选举出新的leader节点，选举的过程中，Consul服务不可用 Ribbon 负载均衡(Load Balance)是用于解决一个服务节点无法处理所有请求的算法，Ribbon是一款基于TCP和HTTP的客户端负载均衡工具，基于Netflix Ribbon，由Spring Cloud进行了再封装，将我们的REST模板的请求均转换为客户端负载均衡的服务调用，Ribbon仅仅只是一个工具，包含在每一个Spring Cloud应用中，无需单独的部署其他服务。 由于Ribbon属于是客户端负载均衡，所以需要在客户端维护一份服务端列表信息，而这些列表信息来自于服务注册中心，如Eureka、Consul等，客户端的服务端列表信息也需要不断的去更新，保证服务列表中服务的可用性，这个过程并不是在客户端去检测各个服务端的心跳，而是通过与服务注册中心进行数据交换来维护服务端的健康。 Spring Cloud默认会自动加载和配置Ribbon的一切，我们只需要去解读一下org.springframework.cloud.netflix.ribbon.eureka.RibbonEurekaAutoConfiguration和org.springframework.cloud.consul.discovery.RibbonConsulAutoConfiguration就可以知道一切了，这里不做源码解释 如何在Spring Cloud中使用Ribbon 服务端启动多个实例，并注册到同一个服务注册中心，或者同一个集群 客户端的RestTemplate实例创建的时候通过注解@LoadBalanced修饰 Ribbon带来的好处 当集群中某一个服务宕掉后，整个服务集群依然可以正常提供服务 可以选择合适的负载算法保证服务的良性使用，避免在流量激增的时候拖垮CPU 负载均衡算法 权重 为每台机器设置在集群中的比重，请求过来后按照比重分配进行轮询 随机 对集群中的机器随机访问，通过随机数定位要访问的机器 哈希 请求按照一定规则映射到要访问的机器上 轮询 轮询是指将请求轮流分配给每台服务器，当服务器群中各服务器的处理能力相同时，且每笔业务处理量差异不大时，最适合使用这种算法 Ribbon工作原理 获取被@LoadBalanced修饰的RestTemplate 我们知道SpringBoot中的自配装配都是通过MATE-INF/spring.factories文件中org.springframework.boot.autoconfigure.EnableAutoConfiguration指定的类来实现的，在注解@LoadBalanced所在的包中我们找到了org.springframework.cloud.client.loadbalancer.LoadBalancerAutoConfiguration，其实整个EnableAutoConfiguration的列表中只有这一个和LoadBalance有关，所以很容易找，我们看下LoadBalanceAutoConfiguration的代码是怎么写的 123@LoadBalanced@Autowired(required = false)private List&lt;RestTemplate&gt; restTemplates = Collections.emptyList(); 我们知道@Autowired可以将对象赋值给一个对象，也可以赋值给一个对象的集合，这里我们不做解释，可以去了解一下@Autowired和@Qualifier。 为RestTemplate添加一个拦截器，也就是org.springframework.cloud.client.loadbalancer.LoadBalancerInterceptor，拦截每一次的RestTemplate请求 我们看下LoadBalancerInterceptor的创建过程： 1234567891011121314151617181920212223242526272829303132333435363738@Beanpublic SmartInitializingSingleton loadBalancedRestTemplateInitializerDeprecated( final ObjectProvider&lt;List&lt;RestTemplateCustomizer&gt;&gt; restTemplateCustomizers) &#123; return () -&gt; restTemplateCustomizers.ifAvailable(customizers -&gt; &#123; for (RestTemplate restTemplate : LoadBalancerAutoConfiguration.this.restTemplates) &#123; for (RestTemplateCustomizer customizer : customizers) &#123; // 触发LoadBalancerInterceptorConfig.restTemplateCustomizer的执行 customizer.customize(restTemplate); &#125; &#125; &#125;);&#125;@Configuration@ConditionalOnMissingClass(\"org.springframework.retry.support.RetryTemplate\")static class LoadBalancerInterceptorConfig &#123; // 创建拦截器 @Bean public LoadBalancerInterceptor ribbonInterceptor( LoadBalancerClient loadBalancerClient, LoadBalancerRequestFactory requestFactory) &#123; return new LoadBalancerInterceptor(loadBalancerClient, requestFactory); &#125; // 为RestTemplate实例添加拦截器 @Bean @ConditionalOnMissingBean public RestTemplateCustomizer restTemplateCustomizer( final LoadBalancerInterceptor loadBalancerInterceptor) &#123; return restTemplate -&gt; &#123; List&lt;ClientHttpRequestInterceptor&gt; list = new ArrayList&lt;&gt;( restTemplate.getInterceptors()); list.add(loadBalancerInterceptor); // 拦截器设置点 restTemplate.setInterceptors(list); &#125;; &#125;&#125; 在拦截器中获取服务所有的列表，根据负载规则选择一台机器提供服务 通过上面的分析，我们可以看到只要为RestTemplate添加了拦截器之后，根据拦截器的工作性质，往后通过RestTemplate实例的每一次请求都会先走到这个拦截器的intercept()方法中，在intercept()方法中调用了LoadBalancerClient.execute()进行了Server选举。 12345678910111213@Overridepublic &lt;T&gt; T execute(String serviceId, LoadBalancerRequest&lt;T&gt; request) throws IOException &#123; // 获取服务列表，检测服务是否存活，每一次请求均需要检测一次，所有的路由均实现于ILoadBalancer ILoadBalancer loadBalancer = getLoadBalancer(serviceId); // 根据配置的路由规则选择最终要提供的服务的地址，所有的规则都实现于IRule Server server = getServer(loadBalancer); if (server == null) &#123; throw new IllegalStateException(\"No instances available for \" + serviceId); &#125; RibbonServer ribbonServer = new RibbonServer(serviceId, server, isSecure(server, serviceId), serverIntrospector(serviceId).getMetadata(server)); return execute(serviceId, ribbonServer, request);&#125; 发送请求获取结果，然后将结果返回 123456789101112131415161718192021222324252627282930@Override public &lt;T&gt; T execute(String serviceId, ServiceInstance serviceInstance, LoadBalancerRequest&lt;T&gt; request) throws IOException &#123; Server server = null; if(serviceInstance instanceof RibbonServer) &#123; server = ((RibbonServer)serviceInstance).getServer(); &#125; if (server == null) &#123; throw new IllegalStateException(\"No instances available for \" + serviceId); &#125; RibbonLoadBalancerContext context = this.clientFactory .getLoadBalancerContext(serviceId); RibbonStatsRecorder statsRecorder = new RibbonStatsRecorder(context, server); try &#123; // 最终请求语句 T returnVal = request.apply(serviceInstance); statsRecorder.recordStats(returnVal); return returnVal; &#125; catch (IOException ex) &#123; statsRecorder.recordStats(ex); throw ex; &#125; catch (Exception ex) &#123; statsRecorder.recordStats(ex); ReflectionUtils.rethrowRuntimeException(ex); &#125; return null; &#125; 借他人的一张图来说明一下请求流程： 配置负载策略 Ribbon默认的负载策略为ZoneAwareLoadBalancer，我们可以通过配置修改策略方案 使用Ribbon中已存在的负载策略 1u-service.ribbon.NFLoadBalancerRuleClassName=com.netflix.loadbalancer.RandomRule 所有的策略都实现于IRule接口 这里单独说一下WeightedResponseTimeRule这个策略，我们在使用Nginx的时候，可以手动预先设置好每一台服务器的权重，但是在Ribbon中，权重是依据服务器响应时间动态设置的，在应用运行期间，这个权重有可能会改变，在WeightedResponseTimeRule类中有一个内部类DynamicServerWeightTask，它是一个定时器，调度时间默认为30秒一次，如果想要修改这个时间，则可以使用自定义负载规则。 使用自定义负载策略 创建自定义配置器 1234567891011121314151617@Configurationpublic class MyRule &#123; @Bean public IRule ribbonRule() &#123; // 使用权重规则 WeightedResponseTimeRule rule = new WeightedResponseTimeRule(); IClientConfig config = new DefaultClientConfigImpl(); config.loadDefaultValues(); // 修改权重定时器执行时间 config.set(WeightedResponseTimeRule.WEIGHT_TASK_TIMER_INTERVAL_CONFIG_KEY, 5 * 1000); // 调用方法修改配置 rule.initWithNiwsConfig(config); return rule; &#125;&#125; 声明要使用该规则的服务 在启动类上加入@RibbonClient(name = &quot;user-service&quot;, configuration = MyRule.class)，name属性指定服务名称，configuration指定要使用的规则 Hystrix Spring Cloud Hystrix是从Netflix Hystrix延伸出的一个轻量级的组件，主要功能是服务容错和线程隔离。 在微服务架构中，服务之间通过远程调用的方式进行通信，会出现一个请求走过多个服务的情况，在请求链路中，一旦某个服务出现故障，那么所有依赖这个服务的其他服务均会发生故障，严重的情况下会导致整个系统全部瘫痪，这与我们做微服务的初衷是相悖的。Hystrix的断路器模式帮我们提升了服务的故障容错能力，当某个服务或某个接口出现故障时，通过断路器的监控，给调用方返回一个指定的错误响应，避免调用方因为长期等待一直占用线程而造成的故障蔓延。 Hystrix具备服务降级、服务熔断、线程隔离、请求缓存、请求合并和服务监控等功能，我们常用的大多就是降级和熔断了。 服务熔断降级 服务熔断是指在下游服务变得不可用或响应时间过长而导致调用方放弃继续调用转而直接返回的一种处理方式，目的是为了保证上游服务的可用性和稳定性。然后我们创建一个具有熔断器的项目，下游服务的代码不做解释，我们看下上游服务如何开启熔断。 pom.xml 我们要使用Hystrix的第一步当然是要引入jar包，我们在pom.xml文件中引入依赖jar： 1234567891011121314&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- euraka --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- Hystrix --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;&lt;/dependency&gt; 为什么要引入eureka？因为服务端是注册到eureka里面的啊，所以要引入eureka去做服务发现。那么可以不用eureka么？当然可以。 API 12345678910@HystrixCommand(fallbackMethod = \"faultMethod\")@GetMapping(\"/cuser\")public String getUser() &#123; String obj = restTemplate.getForObject(\"http://cuser-service/user/1\", String.class); return obj;&#125;public String faultMethod() &#123; return \"服务熔断\";&#125; API的定义和我们平时写的不太一样，多了一个注解@HystrixCommand，这个注解的作用就是在我们API内发生异常的时候进行熔断，参数fallbackMethod指定的就是在发生异常的时候跳转的熔断方法，这个方法参数和返回类型需要和被熔断的方法如出一辙，否则就会报错。 注解@HystrixCommand参数解析 fallbackMethod：指定服务降级处理方法； ignoreExceptions：忽略某些异常，不发生服务降级； commandKey：命令名称，用于区分不同的命令； groupKey：分组名称，Hystrix会根据不同的分组来统计命令的告警及仪表盘信息； threadPoolKey：线程池名称，用于划分线程池。 启动类 123456789101112131415@EnableHystrix@SpringBootApplicationpublic class SpringCloudCuserConsumerHystrixApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringCloudCuserConsumerHystrixApplication.class, args); &#125; @LoadBalanced @Bean public RestTemplate restTemplate() &#123; return new RestTemplate(); &#125;&#125; 注解@EnableHystrix的作用是为应用开启Hystrix的能力，等于一个开关。老版本中使用的是注解@EnableCircuitBreaker，新版本中可以看到EnableHystrix中已经集成在一起了，所以我们使用这一个注解就行了。 运行过程就不用说了，也就是eureka、server、client都启动了，然后访问http://127.0.0.1:8090/cuser，接着把server停掉，然后再访问，会发现接口发生了熔断。 高级参数配置 注解@HystrixCommand中可以通过参数commandProperties、threadPoolProperties设置熔断降级相关的参数，参数名称在类com.netflix.hystrix.contrib.javanica.conf.HystrixPropertiesManager中有指定，并且类中也说明了这两个属性所适用的参数名称，太多了，写两个比较常用的吧 threadPoolProperties coreSize：线程池核心线程数 keepAliveTimeMinutes：线程最大存活时间，单位为分钟 maxQueueSize：最大等待线程队列容量 queueSizeRejectionThreshold：等待队列拒绝添加的阈值 commandProperties execution.isolation.thread.timeoutInMilliseconds：设置调用者等待命令执行的超时限制，超过此时间，HystrixCommand被标记为TIMEOUT，并执行回退逻辑。 服务限流 服务接口限流我们可以通过设置注解@HystrixCommand的threadPoolProperties的参数来实现，限制请求的线程池数量来达到限流的作用 1234567891011121314151617181920212223static int index = 0;@HystrixCommand(fallbackMethod = \"faultMethod\", threadPoolProperties = &#123; @HystrixProperty(name = \"coreSize\", value = \"1\"), @HystrixProperty(name = \"queueSizeRejectionThreshold\", value = \"2\"), @HystrixProperty(name = \"maxQueueSize\", value = \"1\") &#125;, commandProperties = &#123; @HystrixProperty(name = \"execution.isolation.thread.timeoutInMilliseconds\", value = \"2000\") &#125;)@GetMapping(\"/cuser\")public String getUser() &#123; try &#123; index++; if (index % 3 == 0) &#123; Thread.sleep(3000); &#125; &#125; catch (InterruptedException e) &#123; &#125; String obj = restTemplate.getForObject(\"http://cuser-service/user/1\", String.class); return obj;&#125;public String faultMethod() &#123; return \"服务熔断\";&#125; 我们用index来模拟一个请求挂起的操作，我们线程池核心数设置为1，最大等待队列长度为1，我们用JMeter来模拟45个并发，会发现最终只有3个请求顺利通过，其他的全部都被熔断了。 然后我们修改一下线程池大小 123456&gt; @HystrixCommand(fallbackMethod = \"faultMethod\", threadPoolProperties = &#123;&gt; @HystrixProperty(name = \"coreSize\", value = \"10\"),&gt; @HystrixProperty(name = \"queueSizeRejectionThreshold\", value = \"10\"),&gt; @HystrixProperty(name = \"maxQueueSize\", value = \"100\") &#125;, commandProperties = &#123;&gt; @HystrixProperty(name = \"execution.isolation.thread.timeoutInMilliseconds\", value = \"2000\") &#125;)&gt; 把核心数和队列数都增大，然后还是45个并发，会发现全部请求都正常了，把并发改为200后，会发现有那么一两个请求会被熔断 再改一次 123456&gt; @HystrixCommand(fallbackMethod = \"faultMethod\", threadPoolProperties = &#123;&gt; @HystrixProperty(name = \"coreSize\", value = \"2\"),&gt; @HystrixProperty(name = \"queueSizeRejectionThreshold\", value = \"2\"),&gt; @HystrixProperty(name = \"maxQueueSize\", value = \"10\") &#125;, commandProperties = &#123;&gt; @HystrixProperty(name = \"execution.isolation.thread.timeoutInMilliseconds\", value = \"2000\") &#125;)&gt; 150个并发，大概平均每次会有32个请求正常访问，其他的全部被熔断，我们的请求熔断过期时间是2秒，一开始会有2个请求进入核心线程去处理，后续会有10个请求进入等待队列，依次去请求 问题：如果我们想把接口限制并发20，怎么办？ 123456&gt; @HystrixCommand(fallbackMethod = \"faultMethod\", threadPoolProperties = &#123;&gt; @HystrixProperty(name = \"coreSize\", value = \"10\"),&gt; @HystrixProperty(name = \"queueSizeRejectionThreshold\", value = \"2\"),&gt; @HystrixProperty(name = \"maxQueueSize\", value = \"10\") &#125;, commandProperties = &#123;&gt; @HystrixProperty(name = \"execution.isolation.thread.timeoutInMilliseconds\", value = \"150000\") &#125;)&gt; 我们只能大概的控制一下，并不能如此严谨的设置 Feign Spring Cloud Feign是从Netflix Feign扩展出来的一套通过声明式服务调用客户端的组件，使用Feign可用帮助我们更简单的构建一个Web服务，我们只需要通过注解来编写接口，就可以完成对服务接口的绑定。Feign对Hystrix有依赖关系，它只是一个简单的REST框架，最终还是需要通过Ribbon去做负载均衡，通过上面的内容可以看出Feign+Eureka+Ribbon是一家人，Feign通过整合Eureka和Ribbon来实现支持负载均衡的客户端服务。 使用Feign，我们需要将下游服务的接口定义引入到当前应用中，毕竟Java是一个面向对象的语言，即便是RPC调用，我们也需要使用相同的参数类型，并且尽量保证我们参与传递的参数都能够被序列化，所以既然我们要引入下游服务的接口定义，那么我们尽量在下游接口定义中定义FeignClient，这样做的好处是，服务可以被多个客户端使用，不需要每个客户端都定义一次 Feign 接口。 客户端需要在启动类上使用注解@EnableFeignClients开启Feign，Feign最终仍然是使用HTTP方式去发起请求。 上游服务A，下游服务B B：interface(feign client)—&gt;controller A：dependency#A—&gt;enable discovery client &amp;&amp; enable feign clients 客户端调用 定义要调用的目标API接口 12345@FeignClient(name = \"cuser-service\")public interface CUserService &#123; @GetMapping(\"/user/&#123;id&#125;\") String getUser(@PathVariable(name = \"id\") Integer id);&#125; 这里只需要定义接口即可，接口的参数和请求路径都需要和对应的API一致，接口由注解@FeignClient标注。 调用 1234567@Resourceprivate UserService userService;@GetMapping(\"/cuser/&#123;id&#125;\")public String getUser(@PathVariable(name = \"id\") Integer id) &#123; return userService.getUser(id);&#125; Feign支持多种注解方式：Feign、JAX-RS、SpringMVC。 ​","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://luxiaowan.github.io/categories/SpringBoot/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://luxiaowan.github.io/tags/SpringCloud/"}]},{"title":"MyBatis中的设计模式","slug":"MyBatis中的设计模式","date":"2020-03-29T15:55:00.000Z","updated":"2020-04-01T17:06:16.344Z","comments":true,"path":"2020/03/29/MyBatis中的设计模式/","link":"","permalink":"http://luxiaowan.github.io/2020/03/29/MyBatis中的设计模式/","excerpt":"","text":"Builder模式 SqlSessionFactoryBuilder、XMLConfigBuilder、XMLMapperBuilder、XMLStatementBuilder、CacheBuilder 工厂模式 SqlSessionFactory、ObjectFactory、MapperProxyFactory 单例模式 ErrorContext和LogFactory 代理模式 Mybatis实现的核心，比如MapperProxy、ConnectionLogger，用的jdk的动态代理；还有executor.loader包使用了cglib或者javassist达到延迟加载的效果 组合模式 SqlNode和各个子类ChooseSqlNode等 模板方法模式 BaseExecutor和SimpleExecutor，还有BaseTypeHandler和所有的子类例如IntegerTypeHandler 适配器模式 Log的Mybatis接口和它对jdbc、log4j等各种日志框架的适配实现 装饰者模式 Cache包中的cache.decorators子包中等各个装饰者的实现 迭代器模式 迭代器模式PropertyTokenizer","categories":[{"name":"MyBatis","slug":"MyBatis","permalink":"http://luxiaowan.github.io/categories/MyBatis/"}],"tags":[]},{"title":"Spring解决循环依赖","slug":"Spring解决循环依赖","date":"2020-03-29T15:35:00.000Z","updated":"2020-04-07T15:44:04.469Z","comments":true,"path":"2020/03/29/Spring解决循环依赖/","link":"","permalink":"http://luxiaowan.github.io/2020/03/29/Spring解决循环依赖/","excerpt":"","text":"什么是循环依赖 循环依赖的关键点在循环，为了直观一点，我们看个图 从图中我们看到，要想实例化类A，必须先要实例化类B，但是在实例化B之前也必须先实例化C，可想要实例化C，就要先实例化A，喏，死循环了，三个类相互引用又相互等待。 循环依赖如何产生的 循环依赖的产生可能有多种情况： A的构造器中依赖了B的实例，B的构造器中依赖了A的实例 A的构造器中依赖了B的实例，B的属性或者setter依赖了A的实例 A的属性或setter依赖了B的实例，B的属性或者setter依赖了A的实例 Spring IOC解决办法 Spring并不是能解决所有的循环依赖情况，比如上方的情况1是无法解决的，并且如果Bean的scope是protype的，也无法解决。 scope=prototype 此种情况无法解决循环依赖，因为如果获取bean实例的时候，如果类正在创建中，则会抛出异常，详情查看org.springframework.beans.factory.support.AbstractBeanFactory#doGetBean方法 12345// Fail if we're already creating this bean instance:// We're assumably within a circular reference.if (isPrototypeCurrentlyInCreation(beanName)) &#123; throw new BeanCurrentlyInCreationException(beanName);&#125; 从注释我们就可以看出：如果创建bean实例失败则多半因为存在循环依赖。 12345678910/** * Return whether the specified prototype bean is currently in creation * (within the current thread). * @param beanName the name of the bean */protected boolean isPrototypeCurrentlyInCreation(String beanName) &#123; Object curVal = this.prototypesCurrentlyInCreation.get(); return (curVal != null &amp;&amp; (curVal.equals(beanName) || (curVal instanceof Set &amp;&amp; ((Set&lt;?&gt;) curVal).contains(beanName))));&#125; prototype类型的Bean在创建之前会进行标记和创建之后进行解标记 123456789101112if (mbd.isPrototype()) &#123; // It's a prototype -&gt; create a new instance. Object prototypeInstance = null; try &#123; beforePrototypeCreation(beanName); prototypeInstance = createBean(beanName, mbd, args); &#125; finally &#123; afterPrototypeCreation(beanName); &#125; bean = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd);&#125; 1234567protected void beforePrototypeCreation(String beanName) &#123; Object curVal = this.prototypesCurrentlyInCreation.get(); if (curVal == null) &#123; this.prototypesCurrentlyInCreation.set(beanName); &#125; else ...&#125; 1234567protected void afterPrototypeCreation(String beanName) &#123; Object curVal = this.prototypesCurrentlyInCreation.get(); if (curVal instanceof String) &#123; this.prototypesCurrentlyInCreation.remove(); &#125; else ...&#125; scope=singleton Spring通过三级缓存的模式解决非构造器注入引起的循环依赖，详情查看org.springframework.beans.factory.support.AbstractBeanFactory#doGetBean方法 1Object sharedInstance = getSingleton(beanName); 我们看下getSingleton方法： 123456789101112131415161718192021protected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; // 第一级缓存获取 Object singletonObject = this.singletonObjects.get(beanName); // 第一级未找到缓存且bean处于创建中(例如A定义的构造函数依赖了B对象，得先去创建B对象，或者在populatebean过程中依赖了B对象，得先去创建B对象，此时A处于创建中) if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123; synchronized (this.singletonObjects) &#123; singletonObject = this.earlySingletonObjects.get(beanName); // 第二级未找到缓存并允许循环依赖即从工厂类获取对象 if (singletonObject == null &amp;&amp; allowEarlyReference) &#123; ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) &#123; singletonObject = singletonFactory.getObject(); // 将三级缓存移入二级缓存 this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); &#125; &#125; &#125; &#125; return singletonObject;&#125; 在singleton方法中出现了三个属性：singletonObjects,earlySingletonObjects,singletonFactories，这也就是上面说的三级缓存，这个方法的逻辑是先获取一级缓存，若一级缓存中不存在，则获取第二级缓存，二级缓存若也不存在则获取三级缓存，若三级缓存都不存在，则在getBean方法中当成prototype处理。那么这三级缓存中主要存的是什么东西呢？ 一级缓存：已经初始化完成的bean对象Cache 二级缓存：提前曝光的bean对象Cache 三级缓存：ObjectFactory工厂bean缓存, 存储实例化后的bean factory Spring创建一个Bean的实例大致经过三个步骤： createBeanInstance：调用Bean的构造器进行实例化，调用构造器并未填充属性 populateBean：填充属性，也就是setter所有的property initializeBean：初始化Bean，会调用指定的init方法，或者afterPropertiesSet方法，这个时候类属性已经注入完成了 三级缓存何时设置的 三级缓存的设置点是什么时候？我们来看下类的创建过程org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory#doCreateBean 12345678910111213141516171819202122232425262728293031323334353637protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final @Nullable Object[] args) throws BeanCreationException &#123; // 1. Instantiate the bean. // 2. 创建三级缓存 boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(\"Eagerly caching bean '\" + beanName + \"' to allow for resolving potential circular references\"); &#125; // 获取Bean的ObjectFactory并放入三级缓存 addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean)); &#125; // Initialize the bean instance. Object exposedObject = bean; try &#123; // 填充属性 populateBean(beanName, mbd, instanceWrapper); // 初始化Bean exposedObject = initializeBean(beanName, exposedObject, mbd); &#125; catch (Throwable ex) &#123; if (ex instanceof BeanCreationException &amp;&amp; beanName.equals(((BeanCreationException) ex).getBeanName())) &#123; throw (BeanCreationException) ex; &#125; else &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Initialization of bean failed\", ex); &#125; &#125; return exposedObject;&#125; 可以看到第三级缓存是在createBeanInstance方法之后就被设置，这个时候Bean的对象已经被创建出来了，只不过是还不够完美，只是一个壳，但是在容器中已经可以根据对象引用被认出来了； 第二级缓存是在getSingleton方法之后，若第三级缓存中已经存在，则将对象从三级缓存中转移到二级缓存中； 完全初始化之后将自己放入到一级缓存singletonObjects中","categories":[{"name":"Spring","slug":"Spring","permalink":"http://luxiaowan.github.io/categories/Spring/"}],"tags":[]},{"title":"TCP三次握手和四次挥手","slug":"TCP三次握手和四次挥手","date":"2020-03-29T12:20:00.000Z","updated":"2020-03-29T15:32:38.853Z","comments":true,"path":"2020/03/29/TCP三次握手和四次挥手/","link":"","permalink":"http://luxiaowan.github.io/2020/03/29/TCP三次握手和四次挥手/","excerpt":"","text":"简介 传输控制协议(Transmission Control Protocol，简称TCP)是一种面向连接的、可靠的、基于字节流的传输层通信协议，应用层向TCP层发送用于网间传输的用8位字节表示的数据流，TCP将数据流做分隔处理后透传给IP层，然后由IP层将数据传输给目标端TCP层。 TCP为保证不发生丢包，会对所有的包进行编号，接收端依据编号按序接收，全部数据包接收完成后按序号进行合并，这里可能会发生粘包、拆包的问题(暂不介绍)。接收端在每次接收到数据包的时候都会返回给发送端一个确认信息(ACK)，若发送端在一定时间内未接收到ACK信息，则主观认为数据发送失败或数据丢失，会重新发送丢失序号的包。 三次握手 TCP在发送数据之前，需要先建立连接，客户端主动，TCP使用三次握手的方式建立连接，三次握手的意思就是客户端和服务端进行三次通信，三次握手的目的是为了确认通信双方的发送和接收能力是否正常。主要过程： 第一次握手：客户端：服务端，我想要向你发送数据 客户端向服务端发送一个SYN包，并指明客户端的初始化序列号ISN，随后客户端进入SYN_SEND状态。 同部位SYN=1，初始序列号seq=x，此报文不能携带任何的数据，却会消耗掉一个序列号 第二次握手：服务端：客户端，我准备好了，你可以把数据发过来了 服务端接收到客户端发送的消息，需要返回一个SYN/ACK包进行响应，否则客户端会以为消息跑丢了造成重发，SYN/ACK以服务端的为准，SYN=1, ACK=x+1 服务端响应消息也属于是一次通信，所以服务端需要将自己的序列号放入到报文中携带给客户端，初始化序列号seq=y 总的响应报文：SYN=1 ACK=x+1 seq=y 服务端进入SYN_RECV状态 第三次握手：客户端：服务端，我知道了，敌人还有三秒到达战场 客户端接收到服务端的响应之后，要告诉服务端老子收到你的响应了，不要以为消息丢了，等着接收数据吧您嘞 这次握手等于是服务端响应的翻版，报文包括ACK=y+1 seq=x+1 客户端和服务端均进入ESTABLISHED（TCP连接成功）状态 为什么使用三次握手，两次是否可以？ *** 之所以进行三次握手，是保证客户端和服务端均可以进行一次发送、接收和发送接收反馈，目的是让客户端和服务端各自确认自己的发送和接收能力都正常** 四次挥手 TCP发送完数据之后，需要将连接断开，当然断开连接的也需要客户端主动发起，因为数据是否发送完毕只有发送者知道。断开连接需要通过四次挥手(也可以叫四次握手)来保证合理性。 第一次挥手：客户端：我数据发送完了，可以断开连接了，我先进入准备断开状态 客户端发送完数据之后，向服务端发送断开请求报文，FIN=1 seq=u 客户端进入FIN-WAIT-1状态，等待服务端的响应 表示客户端没有数据要发给服务端了 第二次挥手：服务端：我收到了，我同意你的请求，但我要看下我是否还有数据尚在发送，你等会 服务端接收到客户端的断开请求之后，根据序列号进行反馈，ACK=1 seq=v ack=u+1 服务端反馈之后进入CLOSE-WAIT状态，客户端接收到反馈之后进入FIN-WAIT-2状态，等待服务端的确认断开请求 第三次挥手：服务端：我的数据也都发完了，可以断开了 服务端再处理完自己的事情之后，向客户端发送确认断开报文，FIN=1 ACK=1 seq=w ack=u+1 这里的ack=u+1，发现和第二次挥手时候的ack一模一样，这也就是和客户端的断开请求对应起来，否则把别的给断开了岂不是很尴尬 服务端进入LAST-ACK状态，等待客户端的回应 第四次挥手：客户端：我知道了，断开 客户端接收到确认断开报文后，向服务端反馈断开报文，ACK=1 seq=u+1 ack=w+1 服务端接收到反馈后关闭连接 客户端等待2MSL后关闭连接 看一个官方描述图片： 为什么要等待2MSL 目的是防止ACK消息丢失，服务端重发消息后可以再次接收并反馈； 如果在第三次挥手反馈后客户端立刻关闭，如果反馈报文丢失，那么服务端可能就会一直处于重发第三次挥手的报文中，服务端将无法正常进入关闭状态 如果第四次挥手的报文丢失，服务端会再次发送确认关闭消息，客户端重新等待2MSL","categories":[{"name":"杂谈","slug":"杂谈","permalink":"http://luxiaowan.github.io/categories/杂谈/"}],"tags":[]},{"title":"Redis哨兵简介","slug":"Redis哨兵简介","date":"2020-03-28T17:05:00.000Z","updated":"2020-03-28T19:20:52.904Z","comments":true,"path":"2020/03/29/Redis哨兵简介/","link":"","permalink":"http://luxiaowan.github.io/2020/03/29/Redis哨兵简介/","excerpt":"","text":"前言 Redis Sentinel是Redis官方建议的高可用(HA)解决方案，在我们搭建Redis集群时，Redis本身并未集成主备切换功能，sentinel本身是独立运行的，能够监控多个Redis集群，发现master宕机后能够自动切换，选举一个slave成为新的master，当原master恢复之后，sentinel会自动将其作为slave加入到集群中，整个过程不需要人工参与，完全自动化。 主要介绍 sentinel主要功能 定期监控Redis服务是否运行正常 定期监控其他sentinel服务是否正常 能够自动切换master节点 sentinel节点不存储数据 sentinel集群 这个不难理解，如果我们用一个非高可用的sentinel去实现Redis的高可用，明显是不科学的，当这一台sentinel宕机之后，Redis显然无法继续保持它的高可用，所以我们在部署sentinel的时候也会采用集群的方式 优势： ​ 即使有sentinel服务宕机，只要还有一台sentinel运行正常，就可以使Redis继续保持高可用 sentinel版本问题 sentinel在Redis2.6版本中引入的，当时是sentinel 1，貌似有蛮多问题，毕竟初版 在Redis2.8版本中升级到sentinel 2，之后就非常稳定了 不过现在Redis已经发展了很久，版本也越来越高，sentinel已经非常值得信赖了 sentinel中的定时任务 每隔10秒向各个Redis服务器(master和slave节点)发送INFO命令，根据回应获取master和slave信息，通过master的回复可以获取到新增的slave节点 每隔02秒向Redis的master服务器发送命令(hello消息)，用于发现和监视其他sentinel，sentinel之间的监控不在额外创建订阅 每隔01秒向Redis和sentinel所有服务发送PING消息(sentinel本身的ip、端口、id等内容)，通过回复PONG判断服务是否在线 下线判断 主观下线：当前sentinel断定master下线 客观下线：满足sentinel配置文件中quorum数量的sentinel均断定master下线 配置文件解读 1234567891011121314151617181920212223242526272829303132333435# sentinel运行的端口，默认为26379port 26377 dir \"/private/tmp\"logfile \"/var/log/redis/sentinel_26377.log\"# 以守护进程执行daemonize yes# 守护进程运行的pid保存文件pidfile \"/var/run/redis-sentinel.pid\"# 格式：sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt;# 该行的意思是：&lt;master-name&gt;：自定义# &lt;ip&gt;：master主机的IP# &lt;redis-port&gt;：master的端口# &lt;quorum&gt;：表示在sentinel集群中，使master由主观下线变为客观下线的sentinel数量。sentinel monitor cc_master 127.0.0.1 6379 2 # 格式：sentinel down-after-milliseconds &lt;master-name&gt; &lt;milliseconds&gt;# sentinel会向master发送心跳PING来确认master是否存活，如果master在&lt;milliseconds&gt;时间内回应的不是PONG，那么这个sentinel会主观地认为这个master下线了。&lt;milliseconds&gt;的单位是毫秒，默认30秒。sentinel down-after-milliseconds cc_master 15000# 格式：sentinel failover-timeout &lt;master-name&gt; &lt;milliseconds&gt;# failover过期时间，当failover开始后，在此时间内仍然没有触发任何failover操作，当前sentinel将会认为此次failoer失败。默认180秒，即3分钟。sentinel failover-timeout cc_master 60000# sentinel parallel-syncs &lt;master-name&gt; &lt;numreplicas&gt;# 在发生failover主备切换时，这个选项指定了最多可以有多少个slave同时对新的master进行同步，这个数字越小，完成failover所需的时间就越长，但是如果这个数字越大，就意味着越多的slave因为replication而不可用。可以通过将这个值设为1来保证每次只有一个slave处于不能处理命令请求的状态。sentinel parallel-syncs cc_master 1# 格式：sentinel auth-pass &lt;master-name&gt; &lt;password&gt;# sentinel连接的master节点的登录密码sentinel auth-pass cc_master redis 故障转移 sentinel使用Raft投票选举出一个leader去执行故障转移 每一个将master标记为主观下线的sentinel节点发起投票 其他sentinel节点收到投票后，若尚未参与投票(也就是尚未投票给其他sentinel)，则同意，否则拒绝 最终收到过半同意的sentinel节点作为leader 若有两个sentinel收到了过半投票，那么就再重新选举 选举新的master节点 选择replica-priority配置数字最高的slave节点为master，默认为100 若replica-priority相同，则选择偏移量最大的slave节点，偏移量是指slave从master同步的进度，偏移量越大说明数据越完整，可以通过Redis的info命令查看(slave_repl_offset)当前slave的偏移量 若偏移量相同，则选择最先启动的slave作为master 更改master后，通知其他slave节点同步为新的master节点的slave节点 原master节点恢复之后自动加入到集群中，成为新master的slave节点 实战 在本机上启动3个Redis实例，采用1主2从的模式，以下只记录redis.conf和sentinel.conf中关键内容 redis.conf redis-master.conf配置 12# 默认端口port 6379 redis-slave1.conf配置 123456# 端口port 63791# 格式：replicaof &lt;masterip&gt; &lt;masterport&gt;# 从节点归属的master节点replicaof 127.0.0.1 6379 redis-slave2.conf配置 123456# 端口port 63792# 格式：replicaof &lt;masterip&gt; &lt;masterport&gt;# 从节点归属的master节点replicaof 127.0.0.1 6379 sentinel.conf sentinel0.conf 1234567# 端口port 26379sentinel myid 842c9102c48eb0cedeb06fe55e7d2258595ac267# 监控mastersentinel monitor cc_master 127.0.0.1 6379 2 sentinel1.conf 1234567# 端口port 26378sentinel myid 842c9102c48eb0cedeb06fe55e7d2258595ac266# 监控mastersentinel monitor cc_master 127.0.0.1 6379 2 sentinel2.conf 1234567# 端口port 26377sentinel myid 842c9102c48eb0cedeb06fe55e7d2258595ac265# 监控mastersentinel monitor cc_master 127.0.0.1 6379 2 启动 启动sentinel 12345redis-sentinel ~/Documents/develop_tools/tools/redis-5.0.5/sentinel0.confredis-sentinel ~/Documents/develop_tools/tools/redis-5.0.5/sentinel1.confredis-sentinel ~/Documents/develop_tools/tools/redis-5.0.5/sentinel2.conf 启动Redis 12345redis-server ~/Documents/develop_tools/tools/redis-5.0.5/redis-master.confredis-server ~/Documents/develop_tools/tools/redis-5.0.5/redis-slave1.confredis-server ~/Documents/develop_tools/tools/redis-5.0.5/redis-slave2.conf Redis通过info查看信息 127.0.0.1:6379&gt;info all 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161# Server服务器信息redis_version:5.0.5 # Redis 服务器版本redis_git_sha1:00000000 # Git SHA1redis_git_dirty:0 # Git dirty flagredis_build_id:6c6e38af7cea0726 # Redis构建IDredis_mode:standalone # Redis运行模式os:Darwin 19.3.0 x86_64 # 运行环境操作系统版本arch_bits:64 # 架构（32 或 64 位）multiplexing_api:kqueue # Redis 所使用的事件处理机制atomicvar_api:atomic-builtingcc_version:4.2.1 # 编译的GCC版本process_id:61985 # 服务器进程的 PIDrun_id:433b78ec513c8b782f3a46ba6b4ade1f12439aca # Redis 服务器的随机标识符(用于Sentinel和集群)tcp_port:6379 # Redis端口uptime_in_seconds:108 # Redis运行时长，秒uptime_in_days:0 # Redis运行市场，天hz:10configured_hz:10lru_clock:8363059 # 以分钟为单位进行自增的时钟，用于 LRU 管理executable:/Users/chuan/redis-server # 运行命令config_file: # 启动使用的配置文件# Clientsconnected_clients:7 # 已连接客户端的数量client_recent_max_input_buffer:2 # 当前连接的客户端当中，最长的输出列表client_recent_max_output_buffer:0 # 当前连接的客户端当中，最大输入缓存blocked_clients:0 # 正在等待阻塞命令（BLPOP、BRPOP、BRPOPLPUSH）的客户端的数量# Memory (太多了，不做解释了)used_memory:2235920 # 由 Redis 分配器分配的内存总量，以字节（byte）为单位used_memory_human:2.13M # 以可读的格式返回 Redis 分配的内存总量used_memory_rss:3153920 # 从操作系统的角度，返回 Redis 已分配的内存总量（俗称常驻集大小）。这个值和top 、 ps 等命令的输出一致。used_memory_rss_human:3.01M # 以可读的格式返回rssused_memory_peak:2317840 # Redis 的内存消耗峰值（以字节为单位）used_memory_peak_human:2.21Mused_memory_peak_perc:96.47%used_memory_overhead:2221526used_memory_startup:987776used_memory_dataset:14394used_memory_dataset_perc:1.15%allocator_allocated:2271808allocator_active:3116032allocator_resident:3116032total_system_memory:17179869184total_system_memory_human:16.00Gused_memory_lua:37888used_memory_lua_human:37.00Kused_memory_scripts:0used_memory_scripts_human:0Bnumber_of_cached_scripts:0maxmemory:0maxmemory_human:0Bmaxmemory_policy:noevictionallocator_frag_ratio:1.37allocator_frag_bytes:844224allocator_rss_ratio:1.00allocator_rss_bytes:0rss_overhead_ratio:1.01rss_overhead_bytes:37888mem_fragmentation_ratio:1.39mem_fragmentation_bytes:882112mem_not_counted_for_evict:0mem_replication_backlog:1048576mem_clients_slaves:33844mem_clients_normal:151226mem_aof_buffer:0mem_allocator:libcactive_defrag_running:0lazyfree_pending_objects:0# Persistenceloading:0 # 服务器是否正在载入持久化文件rdb_changes_since_last_save:1 # 距离最后一次成功创建持久化文件之后，改变了多少个键值rdb_bgsave_in_progress:0 # 服务器是否正在创建RDB文件rdb_last_save_time:1585421263 # 最近一次成功创建RDB文件的UNIX时间rdb_last_bgsave_status:ok # 最后一次创建RDB文件的结果是成功还是失败rdb_last_bgsave_time_sec:0 # 最后一次创建RDB文件耗费的秒数rdb_current_bgsave_time_sec:-1 # 记录当前创建RDB操作已经耗费了多长时间（单位为秒）rdb_last_cow_size:0aof_enabled:0 # AOF是否处于打开状态aof_rewrite_in_progress:0 # 服务器是否正在创建AOF文件aof_rewrite_scheduled:0 # 是否需要执行预约的AOF重写操作aof_last_rewrite_time_sec:-1 # 最后一次重启AOF的秒数aof_current_rewrite_time_sec:-1 # 记录当前正在重写AOF的秒数aof_last_bgrewrite_status:ok # 最后一次重写AOF文件的结果aof_last_write_status:ok # 最后一次写入结果aof_last_cow_size:0# Stats (可以不做了解)total_connections_received:9total_commands_processed:720instantaneous_ops_per_sec:6total_net_input_bytes:34197total_net_output_bytes:229238instantaneous_input_kbps:0.34instantaneous_output_kbps:1.19rejected_connections:0sync_full:2 # 主从完全同步成功次数sync_partial_ok:0 # 主从部分同步成功次数sync_partial_err:0 # 主从部分同步失败次数expired_keys:0 # 运行以来过期的key的数量expired_stale_perc:0.00 # 过期的比率expired_time_cap_reached_count:0 # 过期计数evicted_keys:0keyspace_hits:1keyspace_misses:1pubsub_channels:1pubsub_patterns:0latest_fork_usec:278migrate_cached_sockets:0slave_expires_tracked_keys:0active_defrag_hits:0active_defrag_misses:0active_defrag_key_hits:0active_defrag_key_misses:0# Replication（master节点）role:master # 角色 master和slaveconnected_slaves:2 # slave节点数slave0:ip=127.0.0.1,port=63791,state=online,offset=20163,lag=1 # 从节点1slave1:ip=127.0.0.1,port=63792,state=online,offset=20163,lag=0 # 从节点2master_replid:895f219aa1e7ed5ecda50dcb1f77eea9f1ef9c3d # 主实例启动随机字符串master_replid2:0000000000000000000000000000000000000000 # 主实例启动随机字符串2master_repl_offset:20163 # 主从同步偏移量,此值如果和上面的offset相同说明主从一致没延迟，与master_replid可被用来标识主实例复制流中的位置。second_repl_offset:-1 # 主从同步偏移量2,此值如果和上面的offset相同说明主从一致没延迟repl_backlog_active:1 # 复制积压缓冲区是否开启repl_backlog_size:1048576 # 复制积压缓冲大小repl_backlog_first_byte_offset:1 # 复制缓冲区里偏移量的大小repl_backlog_histlen:20163 # 此值等于 master_repl_offset - repl_backlog_first_byte_offset,该值不会超过repl_backlog_size的大小# Replication（slave节点）role:slave # 角色 master和slavemaster_host:127.0.0.1 # master节点IPmaster_port:6379 # master节点端口master_link_status:up # master通信master_last_io_seconds_ago:1 # 主库多少秒未发送数据到从库master_sync_in_progress:0 # 从服务器是否在与主服务器进行同步slave_repl_offset:42262 # slave复制偏移量slave_priority:100 # slave优先级slave_read_only:1 # 从库是否设置只读connected_slaves:0 # 连接的slave实例个数master_replid:895f219aa1e7ed5ecda50dcb1f77eea9f1ef9c3dmaster_replid2:0000000000000000000000000000000000000000master_repl_offset:42262 # master偏移量，与slave_repl_offset相同则表示同步完整second_repl_offset:-1repl_backlog_active:1 # 复制积压缓冲区是否开启repl_backlog_size:1048576 # 复制积压缓冲大小repl_backlog_first_byte_offset:1 # 复制缓冲区里偏移量的大小repl_backlog_histlen:42262 # 此值等于 master_repl_offset - repl_backlog_first_byte_offset,该值不会超过repl_backlog_size的大小# CPUused_cpu_sys:0.104404 # 将所有redis主进程在核心态所占用的CPU时求和累计起来used_cpu_user:0.079472 # 将所有redis主进程在用户态所占用的CPU时求和累计起来used_cpu_sys_children:0.002037 # 将后台进程在核心态所占用的CPU时求和累计起来used_cpu_user_children:0.000648 # 将后台进程在用户态所占用的CPU时求和累计起来# Clustercluster_enabled:0 # 实例是否启用集群模式# Keyspacedb0:keys=1,expires=0,avg_ttl=0 # db0的key的数量,以及带有生存期的key的数,平均存活时间","categories":[{"name":"Redis","slug":"Redis","permalink":"http://luxiaowan.github.io/categories/Redis/"}],"tags":[{"name":"Sentinel","slug":"Sentinel","permalink":"http://luxiaowan.github.io/tags/Sentinel/"}]},{"title":"线程池运行的线程和队列中等待的线程是同一个吗","slug":"线程池运行的线程和队列中等待的线程是同一个吗","date":"2020-03-27T16:52:00.000Z","updated":"2020-03-27T18:42:31.562Z","comments":true,"path":"2020/03/28/线程池运行的线程和队列中等待的线程是同一个吗/","link":"","permalink":"http://luxiaowan.github.io/2020/03/28/线程池运行的线程和队列中等待的线程是同一个吗/","excerpt":"","text":"线程池 在高并发场景下，线程池是会被频繁使用到的，简单介绍下线程池： 线程池基础参数：核心线程数、最大线程数、线程最大存活时间、时间单位、阻塞队列、线程池工厂、拒绝策略 创建方式： ThreadPoolExecutor类：ThreadPoolExecutor tpe = new ThreadPoolExecutor(1, 1, 0, TimeUtil.SECONDS, new ArrayBlockingQueue(), new DefaultThreadFactory()) Executors类： newFixedThreadPool(1); newSingleThreadExecutor(); newCachedThreadPool(); newScheduledThreadPool(1); 等待队列： ArrayBlockingQueue：由数组结构组成的有界阻塞队列 LinkedBlockingQueue：由链表结构组成的有界阻塞队列 DelayQueue：使用优先级队列实现的无界阻塞队列 PriorityBlockingQueue：支持优先级排序的无界阻塞队列 SynchronousQueue：不存储元素的阻塞队列 拒绝策略： DiscardPolicy：丢弃被拒绝任务 DiscardOldestPolicy：丢弃队列头部的任务 AbortPolicy：抛出RejectedExecutionException CallerRunsPolicy：在调用execute方法的线程中运行被拒绝的任务 工作原理： 正文 线程池创建过程 创建语句： 123456789ThreadPoolExecutor pools = new ThreadPoolExecutor(5, 10, 10, TimeUtil.SECONDS, new LinkedBlockingQueue(), new ThreadPoolExecutor.AbortPolicy());-- 核心线程数：5 最大线程数：10 非核心线程最大存活时间：10秒 阻塞队列：LinkedBlockingQueue 线程池工厂：DefaultThreadFactory 拒绝策略：AbortPolicy 在线程池真正运行之前，核心线程尚未创建，因为默认是在实际使用的时候才会去创建，但是如果我们想要在线程池创建的时候就初始化核心线程，可以调用ThreadPoolExecutor的实例方法prestartAllCoreThreads()，如果我们想要让核心线程在空闲时可以过期，那么我们可以调用ThreadPoolExecutor的实例方法allowCoreThreadTimeOut(boolean value)来设置 1234567891011121314151617181920212223242526/** * 设置核心线程是否允许过期 */public void allowCoreThreadTimeOut(boolean value) &#123; // 若value为true，但是线程最大存活时间不大于0，那么则抛异常 if (value &amp;&amp; keepAliveTime &lt;= 0) throw new IllegalArgumentException(\"Core threads must have nonzero keep alive times\"); // 如果设置的新值和当前值不同，则执行计划 if (value != allowCoreThreadTimeOut) &#123; allowCoreThreadTimeOut = value; // 若value为true，则终止线程池内的所有空闲Worker if (value) interruptIdleWorkers(); &#125;&#125;/** * 初始化核心线程 */public int prestartAllCoreThreads() &#123; int n = 0; // 循环创建工作线程Worker while (addWorker(null, true)) ++n; return n;&#125; 创建线程Worker 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172private boolean addWorker(Runnable firstTask, boolean core) &#123; retry:// goto语法 for (;;) &#123;// 无限循环，循环体内控制退出 int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; !(rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; !workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c);// 当前线程数 // 校验当前正在执行的线程数是否超过了2^29 - 1，或者根据创建的是否为核心线程来与核心线程数和最大线程数做校验，如果已经超过了相关的值，则返回false拒绝创建 if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; // 改变当前运行的线程数，这里使用的CAS来保证线程安全，设置成功则跳出最外层循环 if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl // 当前线程池状态和方法最初的对比，若不等，则重新执行for循环体 if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; // 创建Worker线程实例 w = new Worker(firstTask); // Worker实例的属性，在Worker构造器中通过getThreadFactory().newThread(this);来创建 final Thread t = w.thread; if (t != null) &#123; // 加锁，保证线程安全 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); // 校验rs是否为RUNNING，或者停止且队列中无任务 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); // 将新创建的Worker实例放入HashSet集合中 workers.add(w); int s = workers.size(); // 更新最大线程运行数 if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; // 工作线程创建成功后，调用线程的start()方法开启线程 if (workerAdded) &#123; t.start();// tag-cc307 workerStarted = true; &#125; &#125; &#125; finally &#123; // 创建失败的话，则处理失败计划 if (!workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125; 由addWorker方法我们可以看出，线程池的核心执行器是Worker内部类 123456789101112131415// Worker类定义private final class Worker extends AbstractQueuedSynchronizer implements Runnable-- final修饰：不可被扩展 继承自AQS：保证线程运行的隔离性，线程池的线程安全核心 实现自Runnable，所以Worker也是一个线程类-- 构造器 Worker(Runnable firstTask) &#123; setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; // 使用线程池工厂创建新线程，并将创建的线程赋值给实例属性thread，也就是在我们调用了thread的start()方法之后，会运行Worker类中的run()方法 this.thread = getThreadFactory().newThread(this); &#125; 看到这里，就应该去看Worker类中的run方法了，我们看到在run方法中调用了runWorker方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495// 核心关键方法，final修饰，不允许被overload和overridefinal void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); // 全文搜索tag-cc307 Runnable task = w.firstTask; w.firstTask = null; // 加锁前先释放锁，查看Worker中的tryRelease方法 w.unlock(); // allow interrupts boolean completedAbruptly = true; try &#123; // 无限循环，这里的无限循环的实现方法主要在getTask()方法中，getTask()是从阻塞队列中获取等待的任务，这里我们可以看到阻塞队列中存储的是一个个Runnable实例 while (task != null || (task = getTask()) != null) &#123; // 线程加锁 w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; // 执行前计划 beforeExecute(wt, task); Throwable thrown = null; try &#123; // 直接调用任务的run方法，这里其实就是将队列中Runnable实例当成普通的非线程对象，我们都知道直接调用线程的run方法会以普通方法的形式去执行，这里之所以这样写，是因为我们当前已经处于一个线程中了，没必要再去启用一个线程去执行任务，否则线程池就没有存在的必要了 task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; afterExecute(task, thrown); &#125; &#125; finally &#123; task = null; // 记录线程Worker的成功任务数 w.completedTasks++; w.unlock(); &#125; &#125; completedAbruptly = false; &#125; finally &#123; processWorkerExit(w, completedAbruptly); &#125;&#125;/*** 获取队列中的任务*/private Runnable getTask() &#123; boolean timedOut = false; // Did the last poll() time out? for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); return null; &#125; int wc = workerCountOf(c); // 核心：判断是否允许核心线程过期 或 当前工作线程数是否超过了核心线程数，timed决定了是否回收核心线程 boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c)) return null; continue; &#125; try &#123; // 若需要销毁工作线程，则使用poll方法使阻塞队列消失 // 否则通过take方法继续阻塞，直到队列中有新数据 Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; timedOut = true; &#125; catch (InterruptedException retry) &#123; timedOut = false; &#125; &#125;&#125; 总结 从以上的内容中我们可以看出来：线程池运行的线程和队列中等待的线程不是同一个，线程池中实际运行的线程是Worker实例","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[]},{"title":"Redis相关东西","slug":"Redis相关东西","date":"2020-03-26T14:15:00.000Z","updated":"2020-03-28T16:03:57.189Z","comments":true,"path":"2020/03/26/Redis相关东西/","link":"","permalink":"http://luxiaowan.github.io/2020/03/26/Redis相关东西/","excerpt":"","text":"目录 Redis是什么 五种数据类型 Redis为何这么快 Redis和Memcached的区别 淘汰策略 持久化 主从复制 哨兵 Redis是什么 ​ Redis是由C语言编写的一个开源的高性能键值对的内存数据库，是一款NoSQL(not-only sql | no sql)数据库，可以用作缓存、数据库、消息中间件。 ​ Redis作为一款内存数据库，其优势为： 1. 性能优秀，数据都存于内存中，读写速度快，理论读取速度能达到10W/秒； 2. 单线程操作，线程安全，采用IO多路复用实现垃圾回收和持久化； 3. 支持丰富的数据类型：字符串(string)、列表(list)、散列(hash)、集合(set)、有序集合(zset)； 4. 支持数据持久化，并且提供RDB和AOF两种持久化方式； 5. 可以用于分布式锁，也可以利用Redis的发布-订阅特性实现消息队列； 6. 主从复制，哨兵，高可用。 五种数据类型 字符串(string)类型：Redis的字符串类型是最基本的类型，可以理解为一个key对应一个value，value可以是字符也可以是数字，该类型可以存储图片或者序列化后的对象等二进制数据，支持的value大小最大512M，当value小于44字节(3.2版本+是44，3.0版本-是39)时，字符串编码为embstr，大于的时候字符串编码为raw，常用的命令有set、get等。 散列(hash)类型：hash是一个键值对的集合，特别适合用于存储对象，可以直接获取到对象的某个属性的值，常用的命令有hset、hget、hgetall等。 列表(list)类型： 集合(set)类型： 有序集合(zset)类型： Redis为何那么快 ​ Redis之所以是单线程的还那么快，完全是因为Redis是纯内存操作，没有CPU上下文切换带来的消耗，也没有磁盘寻址等带来的IO开销，官方理论QPS为10W+。 ​ 既然Redis的性能瓶颈是内存和网络带宽，那么就没必要设计成多线程模式，否则会多出CPU切换，且只要涉及到多线程必然会因为资源竞争而衍生出资源锁的使用，频繁的加锁、释放锁还是非常浪费时间的，所以既然多线程会带来那么多问题，还是使用单线程得了，并且Redis本身就是K-V存储，查询时间复杂度限制在O(1)的情况，所以Redis才那么快。 Redis和Memcached的区别 存储方式：Memcache将数据存储在内存中，若服务器出现故障，则数据全部丢失，无法持久化。Redis提供了RDB和AOF两种方式进行数据持久化，就算是服务器宕机，在恢复之后依然可以保证数据的完整性； 数据类型：Memcache仅支持字符串存储，而Redis支持字符串、列表、散列、集合、有序集合等类型，使用起来更方便和多样化； value大小：Redis的字符串类型可以存储512M的内容，而Memcache最高仅能存储1M的内容，不过虽然Redis支持很大的value，但是一般不会那么用； 底层协议不同：Redis拥有自己的VM，Memcache使用系统函数 淘汰策略 Redis目前有6种淘汰策略，据说新版本中有8种 volatile-lru：从设置了过期时间的所有key中将最近最少使用(least recently used)的key淘汰掉 volatile-ttl：从设置了过期时间的所有key中将剩余存活时间最少(time to live)的key淘汰掉 volatile-random：从设置了过期时间的所有key中随机淘汰掉部分key allkeys-lru：从所有的key中将最近最少使用的key淘汰掉 allkeys-random：从所有的key中随机淘汰掉部分key noeviction：不执行数据淘汰，当内存不足时直接拒绝新的插入请求，并返回错误信息 volatile-lfu：从设置了过期时间的所有key中将访问频率最少(least frequently used)的key淘汰掉 allkeys-lfu：从所有的key中将访问频率最少的key淘汰掉 持久化 Redis支持两种持久化方式，持久化的目的是将内存中的数据写入到磁盘中，防止服务出现故障后的数据丢失的情况。 RDB方式：RDB是Redis的默认持久化方式，属于是定时保存，每隔一段时间将fork出一个子进程去将内存中的数据写入到一个临时dump.rdb(名字在配置文件中设置)文件中，待子进程执行完成之后，将这个临时的dump文件替换掉原来的dump文件，这样做的目的是可以实现copy-on-write，子进程运行过程中使用的内存资源与Redis主进程无关 *通过bgsave和save命令可以手动触发执行RDB 配置信息：redis.conf文件中 解读： save 900 1：900秒内若至少有1个key发生变化，则触发备份 save 300 10：300秒内若至少有10个key发生变化，则触发备份 save 60 10000：60秒内若至少有1万个key发生变化，则触发备份 劣势： RDB持久化方式适合于整库备份，dump文件用于故障恢复，但是由于RDB方式并不是实时的整库备份，所以我们拿到的dump文件总是会和内存中的数据不一致，如果你想要避免服务器发生故障的时候丢失数据，那么仅仅使用RDB是万万不行的，需要配合AOF使用。 为了使用子进程在磁盘上持久存储，RDB经常需要fork()。如果数据集很大，Fork()可能很耗时，并且可能导致Redis停止为客户端提供服务几毫秒甚至一秒钟(如果数据集很大，而且CPU性能不是很好)。AOF还需要fork()，但是您可以调整重写日志的频率，而不需要牺牲持久性。 优势： RDB是一个非常紧凑的单文件时间点表示您的Redis数据。RDB文件非常适合备份。例如，您可能希望在最近的24小时内每小时存档一次RDB文件，并在30天内每天保存一次RDB快照。这允许您在发生灾难时轻松地恢复不同版本的数据集。 RDB对于灾难恢复非常有用，它是一个紧凑的文件，可以传输到远程数据中心上。 RDB最大限度地提高了Redis性能，因为为了保持Redis父进程所需做的惟一工作就是创建一个将完成所有其余工作的子进程。父实例永远不会执行磁盘I/O或类似的操作。 与AOF相比，RDB允许对大数据集进行更快的重启。 AOF方式：AOF方式在Redis中是默认未开启的，在开启AOF后，会将内容写入到appendonly.aof文件中，文件的内容是服务器接收到的所有对数据进行修改的命令集合，按照时间顺序追加到文件尾部，并且在故障恢复的时候，会优先读取appendonly.aof文件中的内容，因为aof的默认策略是每秒钟写入一次，所以当采用aof进行持久化的时候，最多也仅仅丢失一秒的数据。 配置信息：redis.conf文件中 劣势： 随着服务运行时间越来越久，内存中的数据变更次数越来越多，会造成aof文件越来越大，当然我们可以在配置文件redis.conf中设置aof文件重写策略，默认当aof文件大小达到64mb且增长比例超过了之前是100%的时候进行重写，重写的规则是将内存中的数据的当前值全部以对应的set命令写入到新的aof文件中，比如当前aof文件100mb，重写之后80mb，那么只有当文件再次达到160mb(160&gt;=80*2&amp;&amp;160&gt;64)的时候才会再次进行重写， AOF文件损坏修复： ​ 如果在AOF文件写入的过程中突然宕机，可能会导致aof文件损坏，我们可以使用redis-check-aof --fix命令来修复 ####### 故障恢复 若同时开启了RDB和AOF，那么在故障恢复的时候先使用AOF文件进行恢复，这样可以保证丢失最少的数据，但是如果我们想尽快的恢复Redis服务，可以允许丢失一部分数据，那么可以禁用AOF，只使用RDB，使用RDB之所以比AOF快，是因为AOF是一条条命令的去执行的，直到最终状态，RDB是一次性把所有数据的最终状态刷到内存的 AOF日志文件的命令通过可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用flushall命令清空了所有数据，只要这个时候后台rewrite还没有发生，那么就可以立即拷贝AOF文件，将最后一条flushall命令给删了，然后再将该AOF文件放回去，就可以通过恢复机制，自动恢复所有数据 主从复制 ​ redis单节点存在单点故障问题，为了解决单点问题，一般都需要对redis配置从节点，然后使用哨兵来监听主节点的存活状态，如果主节点挂掉，从节点能继续提供缓存功能，从节点仅提供读操作，主节点提供写操作。对于读多写少的状况，可给主节点配置多个从节点，从而提高响应效率。 ​ 复制过程 从节点执行slaveof [masterIP] [masterPort]，保存主节点信息 ； 从节点中的定时任务发现主节点信息，建立和主节点的socket连接； 从节点发送Ping信号，主节点返回Pong，两边能互相通信； 连接建立后，主节点将所有数据发送给从节点（数据同步）； 主节点把当前的数据同步给从节点后，便完成了复制的建立过程； 接下来，主节点就会持续的把写命令发送给从节点，保证主从数据一致性。 哨兵 主从复制存在的问题 一旦主节点宕机，从节点晋升为主节点，同时需要修改应用方的主节点地址，还需要命令所有从节点去复制新的主节点，整个过程需要人工干预； 主节点的写能力受到单机的限制； 主节点的存储能力受到单机的限制； 原生复制的弊端在早期的版本中也会比较突出，比如：redis复制中断后，从节点会发起psync。此时如果同步不成功，则会进行全量同步，主库执行全量备份的同时，可能会造成毫秒或秒级的卡顿。 改善方式 哨兵模式是一种特殊的模式，Redis提供了哨兵命令，哨兵是一个独立的进程，原理是通过哨兵发送命令，然后等待Redis服务器的响应，进而实现对Redis实例的监控。 运行方式 通过命令的发送，Redis实例返回监控的运行状态，所有的Redis服务器 当master机器宕机后，会随机选择一个slave节点作为master，然后通过发布订阅模式通知其他slave节点，修改配置信息，更改跟随的主机 单哨兵模式相对来说不太可靠，毕竟会出现一言堂的情况，所以我们在使用哨兵的时候一般会采用多少兵模式，每一个哨兵都监控所有的Redis服务器，哨兵之间互相监控，当一个节点宕机后，只有指定数量的哨兵全部将其标记为下线，才会将节点移除 故障切换过程 主节点服务器宕机 哨兵1检测到主节点宕机，然后将其标记为客观下线，这个时候主节点还是主节点，并未进行failover过程 其他哨兵检测到主节点宕机，全部哨兵都会将主节点标记为客观下线 标记为客观下线的哨兵数量达到指定数量(sentinel.conf中配置，尽量设置为N/2+1)的时候，由一个哨兵(Raft选举)进行投票，根据投票结果决定是否进行主节点切换(选择优先级最高的，优先级可以通过slave-priority来设置，若优先级相同，则以复制偏移量最大的为主，若偏移量也全部相同，则选择服务ID最小的那个) 主节点切换完成之后，通过发布订阅模式让各个哨兵和从服务器更换主服务器配置，这个过程称为主观下线","categories":[{"name":"Redis","slug":"Redis","permalink":"http://luxiaowan.github.io/categories/Redis/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://luxiaowan.github.io/tags/面试/"}]},{"title":"服务治理思考","slug":"服务治理思考","date":"2020-03-24T05:21:00.000Z","updated":"2020-03-24T06:29:57.075Z","comments":true,"path":"2020/03/24/服务治理思考/","link":"","permalink":"http://luxiaowan.github.io/2020/03/24/服务治理思考/","excerpt":"","text":"前言 服务治理概念当下主要针对的是分布式架构下的多服务、微服务等。分布式系统由若干个大大小小的服务组成，服务之间通过HTTP或TCP的方式进行相互通信，促使整个系统活络起来。当分布式系统中的服务随着业务的发展不断变多时，我们就需要对服务进行相关的管理，不能一味的任由其野蛮增长。 思考 1. 节点管理，即服务提供者在注册中心及客户端创建的服务节点。 节点注册于注册中心，缓存于客户端，目的为若注册中心与服务端出现网络连通故障，但客户端与服务端网络连通正常，此时注册中心已将节点移除，但客户端在下一次同步注册中心节点之前仍可通过自身缓存的服务节点发送请求。 注册中心管理：服务提供者定时向注册中心发送心跳通知来证明其是存活状态，每次收到心跳通知均与上一次收到通知的时间进行比较，如果时差超出注册中心允许的最大值，则认为该服务提供者发生故障，将其从注册中心移除，随即通知监听客户端。 客户端管理：若注册中心与服务端网络连通出现问题，但客户端与服务端网络连接正常，直至下一次与注册中心同步之前仍可继续使用该节点。若客户端与服务端网络连通故障，但注册中心与服务端网络连通正常，则客户端会将该节点从缓存中移除直至下一次与注册中心进行同步，周而复始。 2. 负载均衡，顾名思义：平衡所有服务端处理请求的负载，防止某个服务端因接受过多请求导致服务故障。 随机算法：字面意思，简洁明了，就是采用随机数的方式选择本次请求所要转发的服务端，此法非常公平，不会因为服务端配置的优劣而对其另眼相看，绝对的公平！ 加权法：又叫轮询算法。本法则事在人为，完全按照主人的喜好行事，又称拍马屁，就好比食堂打饭，所有人围绕一个圈，如果打饭阿姨看到每个人的长相都一样，那么他对所有人都没有私心，从第一个开始每人给一勺，如此循环下去，谁都不会多谁也不会少，大家都均等，这就是大家的对注册中心来说权重都一样；如果打饭阿姨喜欢帅哥，看到长得帅的（比如我）每次都会多给一勺，其他人仍是一勺，此种情况对于注册中心而言，我的权重大于其他服务提供者，所以每一批请求中都会多分发给权重大的服务端。（此例不太恰当，换为吃饭：胖子和瘦子，胖子多吃，瘦子少吃，好像更好）。 最少活跃算法：这个拿吃饭来说吧，吃得多的碗落不下了，然后就少盛点，吃的少趁机多吃点均衡一下。上面也说了，打饭阿姨因为我长得帅，每次给别人打着饭呢都会不定时的拐到我这边给我加上一碗，递过来一碗饭，我桌子上的碗的数量就+1，等我吃完一碗饭将空碗回收后，桌子上的碗的数量就-1，但是打饭阿姨给的次数太过频繁，导致我面前很多碗，其他人面前的碗则很少，有人就向领导投诉，领导痛斥一顿后，阿姨则给面前碗最少的人开始打饭，这时此人碗的数量+1，然后阿姨重新统计，下一碗给统计后面前碗最少的人，这样大家都不至于被冷落，一旦落后，立刻照顾到。 一致性Hash算法：对每次请求的参数均计算hash，hash值相同的转发到同一个节点。上体育课1234报数排队，报到相同数字的站在一队，若某一队解散，由4队变成3队，则解散的这一队的人重新123报数，归并相关各队。（为什么不用吃饭举例了？因为再吃就撑死了！） 3. 服务路由 灰度访问：类似于单双号限行和不限行。一条马路刚修好，实行为期一个礼拜的单双号限行，一个礼拜之内无故障，则取消限行，大家都可以走。 就近原则：每次请求到达，客户端先关门在自己的局域网内查找可用的服务提供者，若有则直接调用，若未查到则出门浪。 配置分为静态配置和动态配置，这里不做解释了，字面意思！ 4. 服务容错：有容奶大！要有一颗包容的心！没错，是不是没发现奶非乃！😳 failover：拆开来就是fail over，也就是请求服务端a，然后a故障了，那就直接将请求转发给服务端b，结果b也故障了，那就再转发给c，直到成功！当然也可以设置最大转发次数，比如设置最大转发次数是两次，那么（划重点）在服务端2也故障时就不会转发给c了，直接返回给客户端告知失败！此方式为幂等的，也就是每一个服务提供方返回的数据均相等。 failback：遇到请求故障，那么就告知客户端请求失败，不再重试，然后根据返回的指令进行下一步操作。 failcache：遇到故障，就把请求缓存起来，间隔一段时间再发起请求，防止频繁请求影响服务端恢复。 failfast：遇到故障就返回，管他誓言有多真！绝不重试。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://luxiaowan.github.io/categories/分布式/"}],"tags":[{"name":"服务治理","slug":"服务治理","permalink":"http://luxiaowan.github.io/tags/服务治理/"}]},{"title":"OLTP和OLAP区别","slug":"OLTP和OLAP区别","date":"2020-03-24T03:15:00.000Z","updated":"2020-03-24T04:44:39.771Z","comments":true,"path":"2020/03/24/OLTP和OLAP区别/","link":"","permalink":"http://luxiaowan.github.io/2020/03/24/OLTP和OLAP区别/","excerpt":"","text":"我们在《MongoDB和Elasticsearch简单对比》一文中提到了OLAP和OLTP，然后我去问了几个技术人员，基本上知道这两个名词的人少之又少，当然这也情有可原，毕竟IT行业里名词太多了，我们来说一下这两者的区别 词义 OLTP：on-line Transaction Processing，联机(在线)事务处理 OLAP：on-line Analytical Processing，联机(在线)分析处理 从名词上我们就可以看出，OLTP主要是执行日常基本的事务处理，OLAP主要是执行日常的数据分析 特点 OLAP 实时性不高。比如ES中常见的使用日期检索日志 数据量大。ES利用其倒排索引的特点强化全文检索能力，即使有大量的日志打到ES中，我们仍然可以很快的查询出对应数据，效率贼高 动态检索纬度。我们在做数据分析时，数据的检索纬度是非常重要的一个条件，因为我们一般都是需要依据某一纬度做数据分析，这样才能将分析出来的数据提供给决策使用，不同的决策者需要的纬度不同，所以OLAP需要支持动态的检索纬度 OLTP 实时性高。既然是联机事务处理，那么对实时性要求肯定是一个高指标要求，会尽量杜绝出现数据变更不实时的情况 数据量不是很大。数据量过大会影响CRUD的性能 对确定性的数据进行操作。 高并发且满足ACID。 其他 OLTP一般是指我们常说的关系型数据库，或者说是支持频繁CRUD的数据存储媒介。 OLAP一般用于大数据处理和数据仓库，目前OLAP系统内的数据大多是针对OLTP内存储的数据做出进一步分析和应用，然后提供信息支持最终决策，对其大多是查多改少","categories":[{"name":"杂谈","slug":"杂谈","permalink":"http://luxiaowan.github.io/categories/杂谈/"}],"tags":[]},{"title":"MongoDB和Elasticsearch对比","slug":"MongoDB和Elasticsearch简单对比","date":"2020-03-24T02:09:00.000Z","updated":"2020-03-24T03:14:25.705Z","comments":true,"path":"2020/03/24/MongoDB和Elasticsearch简单对比/","link":"","permalink":"http://luxiaowan.github.io/2020/03/24/MongoDB和Elasticsearch简单对比/","excerpt":"","text":"前言 MongoDB和Elasticsearch都属于是NoSQL类型的数据存储媒介，两者有很大的一个相似度，但使用方式和场景还是有所区别的。 使用场景 MongoDB创立的初衷是为了干掉关系型数据库，和RDBMS算是竞争关系。 Elasticsearch起初就是以检索查询为主要应用场景出道，和RDBMS有点互相协助的意思。 相同点 数据存储格式为json 聚合和全文检索 CRUD 分片和复制 简单的join操作 适用于大数据量的处理 不支持事务 不同点 开发语言不同：ES的Java语言(restful)，Mongo是C++语言(driver)，从开发角度来看，ES对Java更方便 分片方式：ES是hash，Mongo是range和hash 分布式：ES的主副分片自动组合和配置，Mongo需要手动配置集群“路由+服务配置+sharding” 索引：ES自建倒排索引，检索力度强，Mongo手动创建索引（B树），不支持倒排索引，这点和RDBMS类似 检索字段：ES全文检索，可用的检索插件较多，Mongo对索引字段个数有限制，全文检索效率低乃至不采用 时效性：ES非实时，有丢数据的风险，Mongo实时，理论上无丢数据的风险 终 ES偏向于检索、查询和数据分析，适用于OLAP（on-line Analytical Processing）系统，Mongo偏向于大数据下的CRUD，适用于OLTP（on-line Transaction Processing）系统","categories":[{"name":"NoSQL","slug":"NoSQL","permalink":"http://luxiaowan.github.io/categories/NoSQL/"}],"tags":[]},{"title":"MySQL简单优化技巧","slug":"MySQL简单优化技巧","date":"2020-03-22T18:08:00.000Z","updated":"2020-03-23T14:48:44.504Z","comments":true,"path":"2020/03/23/MySQL简单优化技巧/","link":"","permalink":"http://luxiaowan.github.io/2020/03/23/MySQL简单优化技巧/","excerpt":"","text":"前言 一提到MySQL优化，大多数同学都比较依赖于DBA，但是对于程序员来说，掌握SQL的编写技巧其实很重要。 技巧 比较运算符能用“=”就不要用“&lt;&gt;”，因为“=”能够增大列索引的使用概率 如果只查询一条数据，那么就使用“limit 1”，告知查询游标找到第一个之后就返回，以免进行全表扫描 给列选择合适的类型，比如可以使用TINYINT代替INT，节省磁盘和内存的消耗 拆解复杂SQL，减少join的出现 若查询字段全部为某联合索引字段，则避免使用“SELECT * ”，*会造成回表 WHERE、ORDER BY、JOIN的列尽量使用索引字段 使用EXPLAIN查看执行计划 可以使用ENUM的时候不要用VARCHAR 字段尽量设置为NOT NULL，尤其是索引字段 长度比较大的字段尽量拆分为副表，如果这个字段不会被经常使用 经常发生变动的数据库尽量把查询缓存关闭，否则在每次变动的时候都要删除缓存，查询的时候也要查询和更新缓存，浪费时间 索引字段的长度尽量不要太长，毕竟一个索引数据页只有16k，如果一个索引内容过长，那么可能就会造成一个数据页只能存储一个索引字段，浪费空间","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/categories/MySQL/"}],"tags":[]},{"title":"JVM垃圾收集器浅谈","slug":"JVM垃圾收集器浅谈","date":"2020-03-22T03:15:00.000Z","updated":"2020-03-24T06:20:21.492Z","comments":true,"path":"2020/03/22/JVM垃圾收集器浅谈/","link":"","permalink":"http://luxiaowan.github.io/2020/03/22/JVM垃圾收集器浅谈/","excerpt":"","text":"前言 JVM是做Java的同学都必须要了解的东西，为什么这么说，因为我们只有知道了Java程序运行环境的配置和工作逻辑，才能对运行环境进行相关的优化和配置修改，让JVM在不同的服务器环境使用不同的配置，从而达到JVM环境最优化。 说到JVM就不得不说一下GC（garbage collection），垃圾收集的意思是找到垃圾并清理掉，但是常规的垃圾收集器却是找到正在被使用的对象，然后把其他的对象全部当作是垃圾对象清理掉。 写过C语言的同学都知道，在C语言中，我们需要手动的去管理内存，在使用内存之前我们需要先申请（malloc）一定大小的内存，使用完成之后需要手动的把使用的内存释放掉（free），如果忘记释放内存则很快会导致内存溢出， GC算法 引用计数法 为每个对象添加一个引用计数器，在对象被引用时，计数器+1，引用结束后，计数器-1，最终清除掉引用计数器为0的对象，并级联删除该对象引用的所有的对象，只保留引用计数不为0的对象。 这种算法看起来是不是很屌，是的，非常简单，只需要在对象被引用的时候串行修改引用计数器的值即可，但也容易出现一种问题：循环引用！循环引用就是几个废对象之间循环引用，尽管他们的引用计数器都不为0，但是在整个程序中却没有被使用，但是他们永远不会被回收，这样的对象多了之后很容易造成内存泄漏。 标记-清除 标记-清理-整理 可达性分析法","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[{"name":"GC","slug":"GC","permalink":"http://luxiaowan.github.io/tags/GC/"}]},{"title":"Rmi远程服务调用简述","slug":"rmi远程服务调用简述","date":"2020-03-19T16:15:00.000Z","updated":"2020-04-08T13:49:58.978Z","comments":true,"path":"2020/03/20/rmi远程服务调用简述/","link":"","permalink":"http://luxiaowan.github.io/2020/03/20/rmi远程服务调用简述/","excerpt":"","text":"RMI是什么 RMI(Remote Method Invocation)意为远程方式调用，顾名思义，是Java版的RPC实现技术，是建立分布式Java应用程序的方便途径。RMI是基于接口的，一般是接口定义和实现分开在不同的工程中。 相似：Hessian，Burlap，Httpinvoker，webservice 为什么使用RMI 它允许运行在一台虚拟机上的方法调用运行在另一台虚拟机上的对象方法，这样可以让每个任务运行在更适合的虚拟机上。 RMI怎么用 定义一个java.rmi.Remote的子接口，也就是定义一个接口AnimalService，继承自Remote，接口中的所有方法必须显式的抛出java.rmi.RemoteException异常，否则服务会注册失败，谨记! 创建AnimalService的实现类DogServiceImpl，这个类需要继承java.rmi.server.UnicastRemoteObject 将服务AnimalService注册到rmi中心 客户端引用接口定义jar包 客户端获取AnimalService的远程服务 进行相关方法调用 缺点 因为RMI是Java版的RPC通讯技术，所以他只适用于Java程序上，如果想跨语言通讯，那就只能另谋它法了 代码 接口（工程：rmi-api） 123456789import java.rmi.Remote;import java.rmi.RemoteException;public interface AnimalService extends Remote &#123; // 接口中所有的方法必须声明throws RemoteException void laugh() throws RemoteException;&#125; 实现（工程：rmi-service） 12345678910111213141516import java.rmi.RemoteException;import java.rmi.server.UnicastRemoteObject;import cc.kevinlu.spidemo.spi.AnimalService;public class DogServiceImpl extends UnicastRemoteObject implements AnimalService &#123; protected DogServiceImpl() throws RemoteException &#123; super(); &#125; @Override public void laugh() throws RemoteException &#123; System.out.println(\"汪汪!\"); &#125;&#125; Server 123456789101112131415161718192021import java.rmi.Naming;import java.rmi.registry.LocateRegistry;import cc.kevinlu.spidemo.spi.AnimalService;public class Server &#123; public static void main(String[] args) throws Exception &#123; AnimalService dogService = new DogServiceImpl(); AnimalService lionService = new LionServiceImpl(); // 设置服务提供的端口 LocateRegistry.createRegistry(8891); // 设置rmi的host为127.0.0.1，否则可能会出现connect refused错误 System.setProperty(\"java.rmi.server.host\", \"127.0.0.1\"); // 发布服务 Naming.bind(\"rmi://127.0.0.1:8891/dogs\", dogService); System.out.println(\"dog service publish success!\"); &#125;&#125; 客户端（rmi-client：引用rmi-api） 正常情况 12AnimalService dogService = (AnimalService) Naming.lookup(\"rmi://127.0.0.1:8891/dogs\");dogService.laugh(); 反射的方式去回调方法 123Object obj = Naming.lookup(\"rmi://127.0.0.1:8891/dogs\");Method method = obj.getClass().getMethod(\"laugh\");method.invoke(obj, null);","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[]},{"title":"设置生成CGLib和JDK代理生成的class文件","slug":"设置生成CGLib和JDK代理生成的class文件","date":"2020-03-17T13:20:00.000Z","updated":"2020-03-17T13:54:16.360Z","comments":true,"path":"2020/03/17/设置生成CGLib和JDK代理生成的class文件/","link":"","permalink":"http://luxiaowan.github.io/2020/03/17/设置生成CGLib和JDK代理生成的class文件/","excerpt":"","text":"jdk和cglib代理方式属于是老生常谈了，这里就不说了，我们说一下特别的。 我们都知道cglib是针对于类，jdk是针对于接口， cglib在目标类被代理后会自动生成目标类的子类，也就是xxxclass$$EnhancerByCGLIB$$c03f68c4.class jdk代理后会自动生成目标接口的实现，也就是$Proxy0.class 我们平时代码在编译过程中是不会生成代理类的class文件，只有在运行中才会生成 我们可以通过在启动类中设置代理类生成路径 1234// 设置CGLib代理类的生成位置System.setProperty(DebuggingClassWriter.DEBUG_LOCATION_PROPERTY, \"./cg\");// 设置JDK代理类的输出System.getProperties().put(\"sun.misc.ProxyGenerator.saveGeneratedFiles\", \"true\");","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[]},{"title":"Linux更换yum源","slug":"Centos更换yum源","date":"2020-03-12T05:05:00.000Z","updated":"2020-03-12T13:35:37.430Z","comments":true,"path":"2020/03/12/Centos更换yum源/","link":"","permalink":"http://luxiaowan.github.io/2020/03/12/Centos更换yum源/","excerpt":"","text":"我们安装的Linux虚拟机或购买的云服务器上默认使用的yum源在国内有时候会传输很慢，现在大多我们在使用Linux之前都会修改一下yum源，以下以centos为例，将yum源修改为阿里云的。可以到https://developer.aliyun.com/mirror/中查看帮助 备份机器中的源文件 备份的目的是为了操作失败后可以随时回滚 1mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak 下载新的 CentOS-Base.repo 到 /etc/yum.repos.d/ 我们的yum源配置文件在/etc/yum.repos.d/目录中，所以我们将阿里云的repo文件下载到该目录下（可以根据系统版本到http://mirrors.aliyun.com/repo/查看对应的文件） 123456# 要下载与系统版本一致的repo文件wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo或curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 运行yum makecache生成缓存 问题 若出现Couldn’t resolve host 'mirrors.cloud.aliyuncs.com’的信息，则表示网络不通，可以使用下面命令修改repo文件： 1sed -i -e '/mirrors.cloud.aliyuncs.com/d' -e '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo","categories":[{"name":"Linux","slug":"Linux","permalink":"http://luxiaowan.github.io/categories/Linux/"}],"tags":[]},{"title":"kubernates调整nodePort端口范围","slug":"kubernates调整nodePort端口范围","date":"2020-03-11T08:02:00.000Z","updated":"2020-04-10T02:33:45.390Z","comments":true,"path":"2020/03/11/kubernates调整nodePort端口范围/","link":"","permalink":"http://luxiaowan.github.io/2020/03/11/kubernates调整nodePort端口范围/","excerpt":"","text":"默认情况下，k8s 集群 nodePort 分配的端口范围为：30000-32767，如果我们指定的端口不在这个范围就会报错： Error: release kong failed: Service “xxx” is invalid: spec.ports[0].nodePort: Invalid value: 12306: provided port is not in the valid range. The range of valid ports is 30000-32767 所以我们修改 /etc/kubernetes/manifests/kube-apiserver.yaml 文件，在 command 下添加 --service-node-port-range=1-65535 参数，添加 nodePort 范围参数后会自动生效，无需进行其他操作： vim /etc/kubernetes/manifests/kube-apiserver.yaml","categories":[{"name":"Kubernates","slug":"Kubernates","permalink":"http://luxiaowan.github.io/categories/Kubernates/"}],"tags":[]},{"title":"Centos7.7安装kubernates集群","slug":"Centos7.7安装kubernates集群","date":"2020-03-11T05:11:00.000Z","updated":"2020-04-10T02:33:57.107Z","comments":true,"path":"2020/03/11/Centos7.7安装kubernates集群/","link":"","permalink":"http://luxiaowan.github.io/2020/03/11/Centos7.7安装kubernates集群/","excerpt":"","text":"基础 本文主要讲解使用kubeadm搭建高可用的集群，这种方式是最简单最快的。 安装步骤 我们安装k8s的机器资源条件如下： centos7.7 内存不低于2G，CPU不少于2核，否则在安装的时候会报错 集群中的所有机器都要保证网络连通性 相关端口开放 swap关闭 更新系统 在开始安装服务之前，我们先更新一下yum源，然后安装相关的软件 12345# 更新yum源yum update# 安装git(可选)yum install git 禁用swap分区 12345# 关闭swap分区，该命令只是临时关闭，机器重启后还会自动打开swapoff -a# 永久性关闭swap分区，禁止机器重启后自动打开sed -i '/ swap / s/^/#/' /etc/fstab 更换yum源为国内镜像 centos的yum源默认为国外的，如果你的服务器是在国内，那么可能访问不了，所以我们需要把yum的源更换为国内的 123456# 这里有一个注意点，就是下面的Centos-7.repo，这里因为我们使用的centos7，如果你的系统是centos8，那么就改成Centos-8.repo，也就是改成相对应的版本，否则yum安装不了软件cd /etc/yum.repos.d &amp;&amp; \\sudo mv CentOS-Base.repo CentOS-Base.repo.bak &amp;&amp; \\sudo wget -O CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo &amp;&amp; \\yum clean all &amp;&amp; \\yum makecache 安装docker环境 k8s内部可以支持多种容器，我们最常使用的就是docker，所以我们这里也以docker为基础 1234567891011121314151617181920212223242526272829303132333435363738394041# 安装docker依赖包yum install yum-utils device-mapper-persistent-data lvm2# 添加docker库yum-config-manager --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo# 更新源yum update# 安装dockeryum install containerd.io-1.2.10 \\ docker-ce-19.03.4 \\ docker-ce-cli-19.03.4# 配置docker daemonmkdir /etc/dockercat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123; \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": &#123; \"max-size\": \"100m\" &#125;, \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ]&#125;EOF# 创建docker.service.dmkdir -p /etc/systemd/system/docker.service.d# 启用docker.servicesystemctl enable docker.service# 重载&amp;重启dockersystemctl daemon-reloadsystemctl restart docker 安装完之后使用docker -v查看版本 更换docker为国内源 123456789# 配置tee /etc/docker/daemon.json &lt;&lt;-'EOF'&#123;\"registry-mirrors\": [\"https://registry.docker-cn.com\"]&#125;EOF# 重启dockerservice docker restart 安装kubeadm、kubelet、和kubectl kubeadm 负责引导集群，kubelet 在集群的所有节点运行，负责启动 pods 和 containers，kubectl 则负责与集群交互，我们需要在所有节点安装这些组件 配置k8s国内源 我们把k8s的源修改为阿里云的 1234567891011# 配置国内源cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF 关闭SELinux 12setenforce 0sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config 配置网络参数 123456789cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system# 确保 br_netfilter 模块已经加载modprobe br_netfilter 安装并启动kubeadm、kubelet、和kubectl 12345# 安装yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes# 启动systemctl enable --now kubelet 使用kubeadm初始化集群 因为我的服务器在国内，且由于 kubeadm 初始化集群的依赖镜像在国内访问不了，所以初始化集群之前先使用国内源拉取依赖镜像 拉取依赖镜像 1234567891011# 获取依赖镜像列表kubeadm config images list# 使用阿里源下载 K8s 依赖镜像kubeadm config images list |sed -e 's/^/docker pull /g' -e 's#k8s.gcr.io#registry.cn-hangzhou.aliyuncs.com/google_containers#g' |sh -x# 通过docker tag修改镜像与国外镜像名称相同，这样的目的是在初始化的时候，docker会先在本地查找，若本地已有镜像则不会再去远程拉取，等于来了一招瞒天过海docker images |grep registry.cn-hangzhou.aliyuncs.com/google_containers |awk '&#123;print \"docker tag \",$1\":\"$2,$1\":\"$2&#125;' |sed -e 's#registry.cn-hangzhou.aliyuncs.com/google_containers#k8s.gcr.io#2' |sh -x# 删除原镜像，这个可选docker images |grep registry.cn-hangzhou.aliyuncs.com/google_containers |awk '&#123;print \"docker rmi \", $1\":\"$2&#125;' |sh -x master节点初始化 我们使用kubeadm init指令初始化master节点，具体的参数可参考官方文档：https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/ 12# local_ip是本机局域网ip，kubectl_version是我们镜像的版本kubeadm init --apiserver-advertise-address=&lt;local_ip&gt; --kubernetes-version=&lt;kubectl_version&gt; --pod-network-cidr=10.244.0.0/16 --v=5 执行成功之后，日志会打印出下面语句，并且会告知我们节点加入的方式 1234# root或非root用户均可执行mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 或 1export KUBECONFIG=/etc/kubernetes/admin.conf 开启使用master节点资源 默认master节点是不参与pod调度的，这样对于master节点的资源来说有点太过浪费，所以我们通过下面的命令使master节点也参与pod调度 1kubectl taint nodes --all node-role.kubernetes.io/master- 添加网络组件 我们通过kubectl get nodes查看集群内的节点，当前应该只有master一个节点，但是节点的状态为NotReady，查看coredns的pod（kubectl get pod --all-namespaces），会发现coredns处于pending状态，原因就是我们还未安装网络组件。 12# 网络组件我们选择WeaveNet，安装完之后稍等一会就可以了kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\" 向集群中添加node节点 每一台node都需要操作从1~6的步骤，然后我们使用kubeadm join来添加到集群中，这里的token是在master节点创建的，我们可以使用kubeadm token list命令查看可用的token，如果没有可用的token，那么我们使用kubeadm token create创建新的token，一个token的有效期为24小时 12kubeadm join 192.168.0.54:6443 --token 9dmyq2.c50cvh32r62o6jlx \\ --discovery-token-ca-cert-hash sha256:4640dd5d3788968d86ce3cb792c1e368586ee6731de5a07ad8ad331926a2f233 验证 加入之后我们在master节点通过kubectl get nodes来查看所有的节点，验证是否加入成功。","categories":[{"name":"Kubernates","slug":"Kubernates","permalink":"http://luxiaowan.github.io/categories/Kubernates/"}],"tags":[]},{"title":"Centos中安装rz和sz替代ftp","slug":"Centos中安装rz和sz替代ftp","date":"2020-03-10T04:44:00.000Z","updated":"2020-03-10T04:47:26.799Z","comments":true,"path":"2020/03/10/Centos中安装rz和sz替代ftp/","link":"","permalink":"http://luxiaowan.github.io/2020/03/10/Centos中安装rz和sz替代ftp/","excerpt":"","text":"lrzsz 官网入口：http://freecode.com/projects/lrzsz/ 12# 安装lrzszyum install -y lrzsz 安装完成之后就可以直接使用了 12345# 上传文件rz# 下载文件sz 操作很简单","categories":[{"name":"Linux","slug":"Linux","permalink":"http://luxiaowan.github.io/categories/Linux/"}],"tags":[]},{"title":"k8s中YAML文件解释","slug":"k8s中YAML文件解释","date":"2020-03-09T16:30:00.000Z","updated":"2020-04-10T02:34:07.090Z","comments":true,"path":"2020/03/10/k8s中YAML文件解释/","link":"","permalink":"http://luxiaowan.github.io/2020/03/10/k8s中YAML文件解释/","excerpt":"","text":"YAML语法规则 大小写敏感 使用缩进表示层级关系 缩进时不允许使用Tal键，只允许使用空格 缩进的空格数目不重要，只要相同层级的元素左侧对齐即可 ”#” 表示注释，从这个字符一直到行尾，都会被解析器忽略 在Kubernetes中，只需要知道两种结构类型即可： Lists Maps ####YAML属性解释 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# yaml格式的pod定义文件完整内容：apiVersion: v1 #必选，版本号，例如v1kind: Pod #必选，Podmetadata: #必选，元数据 name: string #必选，Pod名称 namespace: string #必选，Pod所属的命名空间 labels: #自定义标签 - name: string #自定义标签名字 annotations: #自定义注释列表 - name: stringspec: #必选，Pod中容器的详细定义 containers: #必选，Pod中容器列表 - name: string #必选，容器名称 image: string #必选，容器的镜像名称 imagePullPolicy: [Always | Never | IfNotPresent] #获取镜像的策略 Alawys表示下载镜像 IfnotPresent表示优先使用本地镜像，否则下载镜像，Nerver表示仅使用本地镜像 command: [string] #容器的启动命令列表，如不指定，使用打包时使用的启动命令 args: [string] #容器的启动命令参数列表 workingDir: string #容器的工作目录 volumeMounts: #挂载到容器内部的存储卷配置 - name: string #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名 mountPath: string #存储卷在容器内mount的绝对路径，应少于512字符 readOnly: boolean #是否为只读模式 ports: #需要暴露的端口库号列表 - name: string #端口号名称 containerPort: int #容器需要监听的端口号 hostPort: int #容器所在主机需要监听的端口号，默认与Container相同 protocol: string #端口协议，支持TCP和UDP，默认TCP env: #容器运行前需设置的环境变量列表 - name: string #环境变量名称 value: string #环境变量的值 resources: #资源限制和请求的设置 limits: #资源限制的设置 cpu: string #Cpu的限制，单位为core数，将用于docker run --cpu-shares参数 memory: string #内存限制，单位可以为Mib/Gib，将用于docker run --memory参数 requests: #资源请求的设置 cpu: string #Cpu请求，容器启动的初始可用数量 memory: string #内存清楚，容器启动的初始可用数量 livenessProbe: #对Pod内个容器健康检查的设置，当探测无响应几次后将自动重启该容器，检查方法有exec、httpGet和tcpSocket，对一个容器只需设置其中一种方法即可 exec: #对Pod容器内检查方式设置为exec方式 command: [string] #exec方式需要制定的命令或脚本 httpGet: #对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port path: string port: number host: string scheme: string HttpHeaders: - name: string value: string tcpSocket: #对Pod内个容器健康检查方式设置为tcpSocket方式 port: number initialDelaySeconds: 0 #容器启动完成后首次探测的时间，单位为秒 timeoutSeconds: 0 #对容器健康检查探测等待响应的超时时间，单位秒，默认1秒 periodSeconds: 0 #对容器监控检查的定期探测时间设置，单位秒，默认10秒一次 successThreshold: 0 failureThreshold: 0 securityContext: privileged:false restartPolicy: [Always | Never | OnFailure]#Pod的重启策略，Always表示一旦不管以何种方式终止运行，kubelet都将重启，OnFailure表示只有Pod以非0退出码退出才重启，Nerver表示不再重启该Pod nodeSelector: obeject #设置NodeSelector表示将该Pod调度到包含这个label的node上，以key：value的格式指定 imagePullSecrets: #Pull镜像时使用的secret名称，以key：secretkey格式指定 - name: string hostNetwork:false #是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络 volumes: #在该pod上定义共享存储卷列表 - name: string #共享存储卷名称 （volumes类型有很多种） emptyDir: &#123;&#125; #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值 hostPath: string #类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录 path: string #Pod所在宿主机的目录，将被用于同期中mount的目录 secret: #类型为secret的存储卷，挂载集群与定义的secre对象到容器内部 scretname: string items: - key: string path: string configMap: #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部 name: string items: - key: string","categories":[{"name":"Kubernates","slug":"Kubernates","permalink":"http://luxiaowan.github.io/categories/Kubernates/"}],"tags":[]},{"title":"Git修改已提交用户信息","slug":"Git修改已提交用户信息","date":"2020-02-11T08:46:00.000Z","updated":"2020-02-11T10:03:16.813Z","comments":true,"path":"2020/02/11/Git修改已提交用户信息/","link":"","permalink":"http://luxiaowan.github.io/2020/02/11/Git修改已提交用户信息/","excerpt":"","text":"修改Git用户名和邮箱 修改某一项目配置 终端进入到项目所在目录下，执行以下命令 123git config user.name \"cc\"git config user.email \"cc@cc.cc\" 修改全局配置 打开终端，执行以下命令 123git config --global user.name \"cc\"git config --global user.email \"cc@cc.cc\" 修改项目最近一次提交信息 修改提交用户 1git commit --amend --author=\"username &lt;email&gt;\" 修改提交备注信息 1git commit --amend 然后执行之后跳转到新的页面 修改顶部备注信息然后保存即可","categories":[{"name":"Git","slug":"Git","permalink":"http://luxiaowan.github.io/categories/Git/"}],"tags":[]},{"title":"Linux禁止root用户远程登录","slug":"Linux禁止root用户远程登录","date":"2020-02-11T03:35:00.000Z","updated":"2020-02-11T05:05:54.080Z","comments":true,"path":"2020/02/11/Linux禁止root用户远程登录/","link":"","permalink":"http://luxiaowan.github.io/2020/02/11/Linux禁止root用户远程登录/","excerpt":"","text":"添加一个新用户 添加新用户 useradd cc 设置新用户密码 passwd cc 修改/etc/sudoers文件 找到## Allow root to run any commands anywhere 在root ALL=(ALL) ALL下方添加语句cc ALL=(ALL) ALL 此文件为readonly文件，保存使用wq!命令 修改/etc/ssh/sshd_config文件 找到PermitRootLogin yes修改为PermitRootLogin no 保存之后执行service sshd restart命令即可","categories":[{"name":"Linux","slug":"Linux","permalink":"http://luxiaowan.github.io/categories/Linux/"}],"tags":[]},{"title":"MacBook连接iPhone不稳定","slug":"MacBook连接iPhone不稳定","date":"2020-02-11T01:32:00.000Z","updated":"2020-02-11T01:34:27.294Z","comments":true,"path":"2020/02/11/MacBook连接iPhone不稳定/","link":"","permalink":"http://luxiaowan.github.io/2020/02/11/MacBook连接iPhone不稳定/","excerpt":"","text":"问题 MacBook经常在使用USB连接iPhone的时候不稳定，连接一跳一跳的 解决 在电脑终端下运行sudo killall -STOP -c usbd，然后输入电脑密码，然后重新插上连接线，就OK了","categories":[{"name":"杂谈","slug":"杂谈","permalink":"http://luxiaowan.github.io/categories/杂谈/"}],"tags":[]},{"title":"BigDecimal出现java.lang.ArithmeticException Non-Terminating Decimal Expansion","slug":"BigDecimal出现java.lang.ArithmeticException-Non-terminating-decimal-expansion","date":"2020-01-16T04:25:00.000Z","updated":"2020-04-12T17:04:34.576Z","comments":true,"path":"2020/01/16/BigDecimal出现java.lang.ArithmeticException-Non-terminating-decimal-expansion/","link":"","permalink":"http://luxiaowan.github.io/2020/01/16/BigDecimal出现java.lang.ArithmeticException-Non-terminating-decimal-expansion/","excerpt":"","text":"错误 java.lang.ArithmeticException: Non-terminating decimal expansion; no exact representable decimal result 原因 出现无限循环小数了 解决 BigDecimal#divide(num, scale) BigDecimal#divide(num, scale, roundingMode) 简单粗暴霸气，双击666~","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[]},{"title":"头条、美团、滴滴、京东等大厂面试题","slug":"刷题","date":"2019-12-24T16:35:00.000Z","updated":"2019-12-24T16:36:27.409Z","comments":true,"path":"2019/12/25/刷题/","link":"","permalink":"http://luxiaowan.github.io/2019/12/25/刷题/","excerpt":"","text":"头条 聊项目，画项目架构图，画一个用户从发起请求到接收到响应，中间经过哪些服务，每个服务做什么事情的流程图。 讲项目中的难点、挑战，你是如何解决的？ Redis 中有几种类型 &amp; 各自底层怎么实现的 &amp; 项目中哪个地方用了什么类型，怎么使用的？ Redis如何实现分布式锁，zk如何实现分布式锁，两者的区别。如果service还没执行完，分布式锁在Redis中已经过期了，怎么解决这种问题？ synchronized底层实现，加在方法上和加在同步代码块中编译后的区别、类锁、对象锁。 锁升级的过程。 Java运行时区域及各个区域的作用、对GC的了解、Java内存模型及为什么要这么设计？ 对索引的理解，组合索引，索引的最佳实践 countDownLatch用过没有，在项目中如何使用的，对AQS的了解。 写生产者消费者问题，考虑高并发的情况，可以使用Java 类库，白纸写代码。 设计一个发号器，考虑集群和高并发的情况，要求发号器生成的id是递增趋势，通过id可以区分出来是今天生成的id还是昨天生成的id，但是生成的id中不能直接带有日期，要具有一定的混淆功能，白纸写代码。 一个二位数组，每个元素都可以往上下左右四个方向走，寻找最长递增路径。如下图所示，最长递增路径即红色字体路径。白纸写代码。 ![image-20190924230411189](/Users/chuan/Library/Application Support/typora-user-images/image-20190924230411189.png) 美团 数据库和缓存的一致性问题。先更新数据库，再更新缓存，若更新完数据库了，还没有更新缓存，此时有请求过来了，访问到了缓存中的数据，怎么办？ 聚簇索引/非聚簇索引，MySQL索引底层实现，为什么不用B-Tree，为什么不用hash，叶子结点存放的是数据还是指向数据的内存地址，使用索引需要注意的几个地方？ MySQL默认的事务隔离级别，MVCC、RR怎么实现的？RC如何实现的？ MySQL间隙锁有没有了解，死锁有没有了解，写一段会造成死锁的SQL语句，死锁发生了如何解决，MySQL有没有提供什么机制去解决死锁 谈下对GC的了解，何为垃圾，有哪些GC算法，有哪些垃圾回收器，cms和g1的区别，还有一个直击灵魂的问题，看过cms的源码吗？ 有没有排查过线上OOM的问题，如何排查的？ 有没有使用过JVM自带的工具，如何使用的？ 假设有下图所示的一个Full GC 的图，纵向是内存使用情况，横向是时间，你如何排查这个Full GC 的问题，怎么去解决你说出来的这些问题？ ![image-20190924230348754](/Users/chuan/Library/Application Support/typora-user-images/image-20190924230348754.png) 说说对Java中集合类的理解，项目中用过哪些，哪个地方用的，如何使用的？ 对CAS的理解，CAS带来的问题，如何解决这些问题？ volatile底层、synchronized底层、锁升级的过程、MESI Ehcache支持哪些缓存？ JUC有研究没有，讲一讲？ 聊项目，画项目架构图，画一个用户从发起请求到接收到响应，中间经过哪些服务，每个服务做什么事情的流程图。 讲项目中的难点、挑战，如何解决的，项目这一块会问的特别细。 如何保证RocketMQ 消息的顺序性，如何解决重复消费问题。 项目中如何保证接口的幂等操作。 讲一讲对Redis 的了解，项目中如何使用的，哪个地方使用的，为什么要使用？ 哨兵机制、Redis 两种备份方式的区别，项目中用的哪种，为什么？ 讲一讲对分布式锁的了解 项目中系统监控怎么做的？ 如何理解Spring中的AOP 和 IOC，以及DI，读过Spring源码没有？ 读过MyBatis源码没有？ 说一个你了解最多的框架，说出你的理解。 如何理解分布式事务，为什么会出现这个问题，如何去解决，了解哪些分布式事务中间件？ 聊一聊对分库分表的理解。 Hystrix功能和在项目中怎么使用的？Hystrix怎么检测断路器是否要开启/关闭？Hystrix实现原理？除Hystrix之外的其他熔断限流中间件有了解没有，了解多少说多少？ Dubbo有了解没有？ 怎么理解Java 中和 MySQL中的乐观锁、悲观锁？ 一致性hash 滴滴 聊项目，画项目架构图，画一个用户从发起请求到接收到响应，中间经过哪些服务，每个服务做什么事情的流程图，讲数据库设计。 处理过线上OOM问题没有，如何处理的？ 遇到过线上服务器CPU飙高的情况没有，如何处理的？ 线上有没有遇到其他问题，如何处理的？ 对线程池的理解，项目中哪个地方使用了，如何使用的，用的Excutor框架中的哪个实现类，为什么用这个 对CAS的理解，CAS带来的问题，如何解决这些问题? volatile底层、synchronized底层、锁升级的过程、MESI 对MySQL索引的理解、对组合索引的理解、索引的最佳实践 分布式锁的实现、对比Redis分布式锁 &amp; ZK分布式锁 唯一ID如何实现的，Snowflake实现原理，Snowflake有哪些问题，如何避免根据订单号可以推算出今天的订单量 如果线上一个功能是用栈结构实现的，使用过程中要注意哪些问题，为什么？ 怎么理解线程安全？ 怎么理解接口幂等？项目中如何保证的接口幂等？ 怎么理解微服务，服务如何划分，可以从哪几个方面去划分，为什么这样划分，微服务带来了哪些好处，哪些坏处，如何看待这个问题？ 如何理解网关，网关带来的好处和坏处，如何解决 Hystrix功能 &amp; 在项目中怎么使用的 &amp; Hystrix怎么检测断路器是否要开启/关闭 &amp;Hystrix实现原理 怎么理解命令模式和观察者模式，手写一个观察者模式或者命令模式的代码，策略模式也行 掌握哪些设计模式，常用哪些，项目中如何使用的，为什么用这个，不用那个？手写一个线程安全的单例模式 如何设计一个秒杀系统？ 如果我现在就是要实现每秒10w请求，不能熔断限流，如何去设计？ 假设现在双十一零点，大量下单请求，如何对这些订单进行分库分表，为什么？ 服务A调用服务B中一个接口，服务B调用服务C中一个接口，如何实现若服务B响应服务A成功，则服务C一定响应服务B成功，需要考虑系统性能问题？ 递归使用中有什么需要注意的地方，递归写法一般可以用什么去替换？ 有两个表，table a，table b，写SQL查询出仅在table a中的数据、仅在table b中的数据、既在table a 又在table b 中的数据？ Spring 源码有了解没有？ MyBatis源码有了解没有？ MySQL事务隔离级别、MVCC？ 京东 一个final修饰的属性，定义的时候没有初始化，在无参构造函数中初始化，可以吗，为什么 说说对Java中集合类的理解，项目中用过哪些，哪个地方用的，如何使用的，为什么不用其他的集合类 HashMap，concurrentHashMap底层实现 List删除是怎么实现的，遍历的时候可以删除吗？为什么? Redis中有哪些数据结构，了解过其底层怎么实现的吗，和Java中相似的数据结构的对比？ Redis是单线程的还是多线程的，为什么这么快？ Redis Hash中某个key过大，变为String类型的大key，怎么处理，使用中如何避免出现这种问题? 设计模式在项目中哪个地方用到了，怎么使用的，能不能画一个你熟悉的设计模式的UML图，手写单例模式，手写静态内部类实现的单例模式。 讲一讲MySQL索引，实际工作中，哪些场景用了B+Tree索引，哪些场景用了hash索引？ explain 可以看到哪些信息，什么信息说明什么，explain的结果列讲一下 Spring源码看过没有，会多少讲多少？ MyBatis源码看过没有，会多少讲多少？ CAS的缺点，如何解决？ AQS、countDownLatch如何实现？ 线程池如何实现，核心线程数和最大线程数设置成多少，为什么这么设置，项目中哪个地方使用了线程池，使用时需要注意什么 MySQL事务隔离级别，幻读，脏读，项目中用什么事务隔离级别，为什么？ volatile底层原理、synchronized实现机制 对XA、TCC的理解，了解哪些分布式事务框架，有什么缺点？ Feign 和 Dubbo，了解多少说多少？ Eureka 和 Zookeeper，了解多少说多少？ Hystrix 和 sentinel，了解多少说多少？ Spring Cloud Alibaba，了解多少说多少？ 对分库分表、读写分离的了解，了解多少说多少？ 画一下Java 线程几个状态及状态之间互相转换的图？ 聊项目，画项目架构图，画一个用户从发起请求到接收到响应，中间经过哪些服务，每个服务做什么事情的流程图，讲数据库设计具体到部分表中有哪些字段？ 部门体量比较大，可能需要加班，到凌晨两三点的那种，也可能通宵，通宵是大促期间，你能接受吗？ 也会加班到十点，这个不是大促期间，但也不是每天，非常态情况，你能接受吗，你在哪里住，过来要多久，有男朋友吗？ 火币 Kafka 如何保证消息顺序消费、在consumer group 中新增一个consumer 会提高消费消息的速度吗、那如果我想提高消息消费的速度，我要怎么办？ Redis几种数据结构及底层，项目中如何使用的Redis？ 哨兵机制、选举算法 一致性hash Redis是单线程的还是多线程的，为什么速度这么快？ 多路复用的几种方式以及区别？ 对线程池的理解，在项目中如何使用的，多个线程之间如何共享数据，多个进程之间如何共享数据？ HashMap、concurrentHashMap的区别及底层实现、HashMap和HashTable 的区别？ 什么是红黑树，什么是B-Tree，为什么HashMap中用红黑树不用其他树？ 对MySQL索引的理解，为什么MySQL索引中用B+Tree，不用B-Tree 或者其他树，为什么不用hash 索引？ 数据库和缓存的双写一致性问题？ 每日一淘 用过哪些Object类的方法，如何使用的 Java如何实现序列化的，Serialization底层如何实现的 countDownLatch如何实现的 项目中监控报警机制如何做的，说说你的了解 线上服务器CPU飙高，如何处理这个问题 服务A调用服务B，用户请求服务A，发现返回较慢，如何定位这个问题 TIME_WAIT是什么状态还记得吗，什么情况下网络会出现这个状态 linkedme 内核态和用户态、cas 和 sout 哪个用到了内核态和用户态的切换 哪些典型的应用用的是UDP？ 线程池有了解吗，项目中如何使用的？ 计算密集型/IO密集型任务分别如何设置线程池的核心线程数和最大线程数，为什么这么设置？ 假如我下午5点要和5个人一起开会，但是这5个人现在都出去了，不在公司，但是今天会回来，问，我如何开这场会，用Java 并发方面的知识回答。 算法题 [1,1,2,2,3,4,4,5,5,5] 找出不重复的元素（黄包车） 反转链表，要求时间复杂度O(N)，空间复杂度O(1) （火币） 非递归实现斐波那契数列 （爱奇艺） 这一周股市价格为[2,6,1,4,8]，求哪一天买入哪一天卖出，可获得最大收益，最大收益为多少 （爱奇艺） 按照箭头方向查找二叉树 （金山云） ![image-20190924230728819](/Users/chuan/Library/Application Support/typora-user-images/image-20190924230728819.png) 表a b c之间用ID关联，求阴影部分的数据 （金山云） ![image-20190924230750484](/Users/chuan/Library/Application Support/typora-user-images/image-20190924230750484.png) 一个整形无序数组，里面三个数只和等于一个目标值，求这三个数 （小米） 链表问题 （小米） ![image-20190924230830166](/Users/chuan/Library/Application Support/typora-user-images/image-20190924230830166.png) 扑克牌问题 （小米） 有十张扑克牌，从上面开始抽，抽出一张放桌子上，然后再抽出一张放扑克牌的最下面，这样循环往复的操作，直到手里的牌都没有了。这时，桌子上牌的顺序正好是1 2 3 4 5 6 7 8 9 10。要求写代码求出原顺序 手写大顶堆 （linkedMe） 手写LRU 算法 （火币） 字符串相加 （滴滴） 两个数字类型的字符串，直接转int或者double肯定都放不下，然后求这两个数的和，返回值还是字符串，15分钟时间，要求无Bug 寻找目标值位置 （滴滴） 有一个二维数组，数组横向有序，纵向有序，求目标值的位置，10分钟时间 求字符串“efabcbaefehiabcba”中最长的回文数，不去重（美团） 反转int类型的值x，不要借用String，只用int 即可。&amp;&amp; 针对该程序，写出其应有的测试用例 （美团） top K 问题（每日一淘）","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://luxiaowan.github.io/tags/面试/"}]},{"title":"工程师和程序员的区别","slug":"工程师和程序员的区别","date":"2019-12-20T17:03:00.000Z","updated":"2019-12-20T17:03:48.136Z","comments":true,"path":"2019/12/21/工程师和程序员的区别/","link":"","permalink":"http://luxiaowan.github.io/2019/12/21/工程师和程序员的区别/","excerpt":"","text":"开一个杂谈的类目，探讨平时工作中遇到的趣事，可能偶尔也会转载一些其他地方看到的有意思的行业趣事，纯属一乐。 开端 今天下班后，公司里一个工作多年的小朋友找我闲聊，然后聊着聊着不知为啥聊到了行业上： 12345678小朋友：串串，你为什么不愿意带团队？串 串：不感兴趣！小朋友：你打算敲代码到35岁吗😅串 串：40小朋友：就算技术上再牛逼，程序员在中国，到了四十也到头了吧串 串：工程师和程序员是两码事小朋友：本质区别在哪呢？你指有架构的能力吗？串 串：。。。。。。 我想说 其实聊到这里，我突然发现很多做开发的同学对程序员和工程师这两个职业的认知好像并不是那么的分明，其实上面的对话还没有结束，我把工程师和程序员对等为进程和线程，做开发的同学应该都知道进程和线程的区别(说不了解的回去把操作系统再细学一遍)，为什么这么比喻呢(其实这个比喻也不恰当，就是想把两者的区别扩大化一下，各位不要挑这个比喻啦，挑了我也不改)，因为进程是由很多线程组成的，每一个线程都只处理进程中很小很小的一个模块，可能仅仅就是去把磁盘上的数据读到内存中而已，进程就像是一片森林，线程只是一棵树，我想表达的意思就是工程师所要掌握的知识技能，要比程序员多的多的多，也就是需要有大局观、知识广度、知识深度、行军线路等，而程序员是什么？你可以认为会写代码的都叫程序员，但只会写代码的程序员不能叫做工程师!!! 我还想说 我们来看一下BOSS直聘上招聘程序员和工程师岗位JD的区别，这里可能要有图： 看到没，同样是高级岗位，但是岗位JD差别却很大，我们来分析一下区别： 1234Java高级程序员： Java高级研发工程师： 无学历要求，会写代码就行 最低科班出身 能用技术框架写代码，其他低要求 业务、产品、研发均要擅长，自我驱动，技术攻坚，架构设计 强调技术要求，罗列一堆技术框架 无技术框架要求，对软技能提到多次(代码洁癖、计算机原理、算法、行业经验、职责、质量、业务/产品/设计/研发) 我们从分析结果可以看出，一样的职级，一样的岗位，一样的技术，对程序员的要求是你会写代码就行，对工程师的要求是你不仅要会写代码，还要懂业务、产品、设计、产品和研发质量、数据结构和算法、计算机原理、软件工程学，现在是不是很清晰的知道了程序员和工程师的区别？ 我再说一下 很多人会问：为什么国外的程序员到60岁还可以写代码，而中国的程序员35岁就要被淘汰？ 其实行业淘汰的是程序员而不是工程师，一个有思想有眼界有能力的工程师是每一个公司都想聘用的，这一类人只会越老越吃香。 那么程序员为什么到35岁就会被淘汰呢？因为35岁还只是会写代码的人，身体已经被加班摧残的&quot;风烛残年&quot;了，每一年都会有一大批毕业生/培训生走入社会，他们身体健康、强壮、任劳任怨、爱加班，身为35岁的&quot;老年人&quot;，除了年龄大、发际线高、体重超标、脑子迟钝，还有哪一点比得过那么一群小鲜肉？哦~还有上有老下有小的山一般大的压力!!! 那么工程师为什么越老越吃香呢？因为工程师靠的不是写代码，靠的是自我沉淀和行业经验。 总结下 大家一开始都是从程序员做起的，为什么有的人可以成为工程师，有的人还是程序员呢？这就在于个人平时的积累了。 玩 最近一年面试了二十多个7、8年工作经验的人，很多人连最基本的技术知识和数据结构都不知道，记忆最深的就是有一个跟着7年的人跟我说：我会写代码，我能实现业务需求，不就行了么，我要知道那些原理干啥？ 是不是又很多人也这么想的？这么想就对啦，等着30岁就被淘汰吧，35都不用等啦！ 知道的越多，不知道的就越多","categories":[{"name":"杂谈","slug":"杂谈","permalink":"http://luxiaowan.github.io/categories/杂谈/"}],"tags":[]},{"title":"Docker安装phpmyadmin","slug":"Docker安装phpmyadmin","date":"2019-12-20T09:00:00.000Z","updated":"2019-12-20T15:07:26.882Z","comments":true,"path":"2019/12/20/Docker安装phpmyadmin/","link":"","permalink":"http://luxiaowan.github.io/2019/12/20/Docker安装phpmyadmin/","excerpt":"","text":"我们在云服务器上的MySQL数据库在暴露端口和开启远程连接的情况下，是非常不安全的，昨天我的一台服务器就遭到了勒索病毒的攻击，让我支付0.06比特币赎回我的数据。还好那个库中没有重要数据，只是一个弃用了半年的数据库，密码我自己都忘记了，还好通过手段找回了密码，登上去之后就傻眼了。那么如果我们不想开启远程登录还想操作数据库，怎么办？怎么办？用phpmyadmin就可以解决了，不过phpmyadmin也有一定的风险，下面看下怎么玩。 使用Docker发布phpmyadmin并且连接已经存在的MySQL容器 首先下载phpmyadmin的docker镜像 1234567# 先查询镜像仓库里有哪些镜像docker search phpmyadmin# 拉取star最多的镜像or拉取你想用的镜像docker pull docker.io/phpmyadmin/phpmyadmin拉取镜像需要一段时间，这个要看服务器的带宽网速了 启动镜像，连接到已存在的MySQL容器 1234567891011121314# 启动镜像docker run --name myadmin -p 80:80 -d --link mysql-db:db docker.io/phpmyadmin/phpmyadmin# 修改容器配置文件## 将配置文件复制到宿主机中docker cp myadmin:/etc/phpmyadmin/config.inc.php .## 修改配置文件信息(这里修改的db就是在启动的时候--link后面指定的别名)$cfg['Servers'][$i]['host'] = 'localhost' ——&gt; $cfg['Servers'][$i]['host'] = 'db'## 将修改后的配置文件复制回容器中docker cp ./config.inc.php myadmin:/etc/phpmyadmin/# 重启phpmyadmin容器 此处就可以连接了，当然你也可以修改配置文件限制连接的用户，然后在MySQL中给连接用户授权 使用docker-compose创建 安装docker-compose，这里就不赘述了，回头专门用篇文章来解释 编写docker-compose.yml文件 1234567891011121314151617181920212223242526272829version: \"2\"services: mysql: image: hub.c.163.com/library/mysql container_name: test-mysql restart: always ports: - \"3306:3306\" environment: MYSQL_USER: \"root\" MYSQL_PASSWORD: \"root\" MYSQL_ROOT_PASSWORD: \"root\" networks: - net-mysql phpmyadmin: image: docker.io/phpmyadmin/phpmyadmin container_name: test-myadmin ports: - \"80:80\" environment: MYSQL_USER: \"root\" MYSQL_PASSWORD: \"root\" MYSQL_ROOT_PASSWORD: \"root\" networks: - net-mysqlnetworks: net-mysql: 发布容器 12# 使用命令发布容器docker-compose up -d 然后就可以使用了，不需要修改任何配置文件","categories":[{"name":"Docker","slug":"Docker","permalink":"http://luxiaowan.github.io/categories/Docker/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/tags/MySQL/"},{"name":"Docker","slug":"Docker","permalink":"http://luxiaowan.github.io/tags/Docker/"}]},{"title":"Nacos基本使用","slug":"Nacos基本使用","date":"2019-12-20T01:30:00.000Z","updated":"2019-12-20T15:07:26.882Z","comments":true,"path":"2019/12/20/Nacos基本使用/","link":"","permalink":"http://luxiaowan.github.io/2019/12/20/Nacos基本使用/","excerpt":"","text":"啥 Nacos是什么 Nacos解决什么问题 Nacos怎么使用 Nacos架构图 Nacos作为配置中心 Nacos作为服务注册中心 以上问题，在Nacos官网都有详细的说明，要学会查看官方文档，其他地方写的基本也都是copy的官方文档，所以我这里就不再赘述了，直接上官网地址给你们：https://nacos.io/zh-cn/docs/what-is-nacos.html 在使用过程中如果遇到什么问题，可以去提issue：https://github.com/alibaba/nacos/issues","categories":[{"name":"Nacos","slug":"Nacos","permalink":"http://luxiaowan.github.io/categories/Nacos/"}],"tags":[]},{"title":"忘记MySQL的root用户密码怎么办","slug":"忘记MySQL的root用户密码怎么办","date":"2019-12-19T16:53:00.000Z","updated":"2019-12-20T15:07:26.883Z","comments":true,"path":"2019/12/20/忘记MySQL的root用户密码怎么办/","link":"","permalink":"http://luxiaowan.github.io/2019/12/20/忘记MySQL的root用户密码怎么办/","excerpt":"","text":"在工作中，如果我们忘记了数据库的密码，那么我们该怎么办？其实方法很多，下面我们主要说一下如何修改宿主机上的MySQL以及Docker容器中的MySQL。 1. Docker容器中的MySQL 启动MySQL的容器 通过docker命令进入到容器中docker exec -it container_id /bin/bash 找到docker.cnf配置文件，大概在/etc/mysql/conf.d/目录下 打开docker.cnf文件，在最后一行后添加skip-grant-tables跳过用户权限验证 退出docker容器，然后重启容器docker restart container_id 再次进入到容器中，执行如下命令： 12345678910111. 进入mysql控制台 mysql2. 进入mysql数据库 use mysql;3. 修改root用户密码 MySQL5.7+: update user set authentication_string=password(&apos;newpwd&apos;) where user=&apos;root&apos;; MySQL5.6-: update user set password=password(&apos;newpwd&apos;) where user=&apos;root&apos;;4. 刷新权限 flush privileges; 将docker.cnf中添加的那一行skip-grant-tables删除，退出docker容器，然后重启容器，搞定 2. 宿主机中的MySQL 找到my.ini配置文件，并在[mysqld]组下加入skip-grant-tables跳过用户权限验证 修改密码，同上步骤6 将my.ini文件中添加的那一行删除，重启MySQL服务service mysqld restart，搞定","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/categories/MySQL/"}],"tags":[]},{"title":"Nacos编译报错Use STAR or POSIX Extensions to Overcome This Limit","slug":"Nacos编译报错Use-STAR-or-POSIX-extensions-to-overcome-this-limit","date":"2019-12-19T04:23:00.000Z","updated":"2019-12-19T16:51:55.995Z","comments":true,"path":"2019/12/19/Nacos编译报错Use-STAR-or-POSIX-extensions-to-overcome-this-limit/","link":"","permalink":"http://luxiaowan.github.io/2019/12/19/Nacos编译报错Use-STAR-or-POSIX-extensions-to-overcome-this-limit/","excerpt":"","text":"Nacas安装 可以通过下载源码进行编译和下载发行包两种方式来启动Nacos 源码 1234567git clone https://github.com/alibaba/nacos.gitcd nacos/mvn -Prelease-nacos clean install -U ls -al distribution/target/// change the $version to your actual pathcd distribution/target/nacos-server-$version/nacos/bin 发行包 您可以从 最新稳定版本 下载 nacos-server-$version.zip 包。 12unzip nacos-server-$version.zip 或者 tar -xvf nacos-server-$version.tar.gzcd nacos/bin 如何启动在这里就不做说明了，我们重点说一下通过源码安装时出现的一个编译错误 错误信息 Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:3.0.0:single (make-assembly) on project nacos-distribution: Execution make-assembly of goal org.apache.maven.plugins:maven-assembly-plugin:3.0.0:single failed: group id ‘1326029969’ is too big ( &gt; 2097151 ). Use STAR or POSIX extensions to overcome this limit -&gt; [Help 1] 这个错误主要出现在编译distribution这个目录时出现的，执行命令是mvn -Prelease-nacos clean install -U，这个错误导致nacos-server-1.2.0-SNAPSHOT生成失败 解决办法 在distribution目录下的pom.xml文件中找到id为release-nacos的profile，在plugin标签的configuration内加上&lt;tarLongFileMode&gt;posix&lt;/tarLongFileMode&gt;即可解决","categories":[{"name":"Nacos","slug":"Nacos","permalink":"http://luxiaowan.github.io/categories/Nacos/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/tags/Java/"},{"name":"Maven","slug":"Maven","permalink":"http://luxiaowan.github.io/tags/Maven/"},{"name":"基础应用","slug":"基础应用","permalink":"http://luxiaowan.github.io/tags/基础应用/"}]},{"title":"阿里面试题","slug":"阿里面试题","date":"2019-12-08T15:55:00.000Z","updated":"2019-12-19T17:07:06.991Z","comments":true,"path":"2019/12/08/阿里面试题/","link":"","permalink":"http://luxiaowan.github.io/2019/12/08/阿里面试题/","excerpt":"","text":"阿里巴巴一面 先介绍一下自己吧 说一下自己的优缺点 具体讲一下之前做过的项目 你觉得项目里给里最大的挑战是什么？ HashMap 了解么吗？说一下 put 方法过程 HashMap是键值对存储数据结构，内部以key-value+链表的形式存储数据，jdk1.8之前，是以Entry创建数据节点，jdk1.8之后以Node创建数据节点。 put方法中要先检查存储数据的数组是否已初始化，计算key的哈希值，若该哈希值所在槽无对象，则直接设置进去；若该槽有值，则校验key值是否相等，若相等则直接替换，并将旧值返回；若key不相等，则校验节点的类型是否为TreeNode，若为TreeNode则走TreeNode的putTreeVal；若为普通Node节点，则迭代该槽上的Node节点，通过当前节点的next属性获取下一节点，若next的key与put的key相等(== &amp;&amp; equals)，则替换next的value；若next为null，则在此处存储，并且当前节点的next指向新创建的Node对象；最终返回原值或null HashMap 是不是线程安全？ 不是线程安全的，在高并发情况下会出现脏数据的情况 如果想使用线程安全的Map，则可以使用Collections类中的synchronizedMap方法去创建一个Collections内部类SynchronizedMap的实例，该内部类中的方法是使用synchronized关键字实现的线程并发锁；Hashtable类同；或者如果项目使用的是jdk1.5及以上版本的话，可以使用ConcurrentHashMap类，该类是使用了分段锁和CAS的方式实现的并发控制，比synchronized更加灵活高效，分段锁可以实现在写的同时放任其他线程读取非本段的数据，CAS方式则仅仅锁住被操作的节点。 HashMap 为什么不用平衡树？ 平衡树在极端情况下会出现非常高的树，在查找的过程中会变慢，而红黑树在插入数据的时候会通过自旋转缩短树的高度 AQS 知道吗？知道哪一些呢？讲一讲。 AQS是AbstractQueuedSynchronizer类的缩写，JUC包中的锁基本实现于AQS，可以实现共享锁和排它锁，AQS是读写锁ReentrantLock的基类，内部通过acquire和acquireShared方法分别实现排它锁和共享锁。 CLH 同步队列是怎么实现非公平和公平的？ CLH队列是通过链表的形式将每一个节点的连接到一起，公平是指在头结点被处理的时候，其他节点都处于wait状态，当处理完之后，将后续所有的节点全部唤醒，所有队列是公平的；即使队列中所有的节点都被唤醒，也不会出现谁先竞争到资源谁执行的情况，永远都是先执行离对列头最近的无中断标志的节点，所以CLH队列又是非公平的。 ReentrantLock 和 synchronized 的区别 两者都是本地锁，都是为了防止并发情况下出现数据错乱的情况，都是可重入锁。ReentrantLock有共享锁和排它锁两种锁机制，而synchronized只能是排它锁，synchronized在jdk1.5之后自带锁升级机制，包括了偏向锁、轻量级锁和重量级锁，但ReentrantLock只是重量级锁。 讲一下 JVM 的内存结构 JVM内存结构分为线程共享内存区和线程私有内存区，线程共享内存区包括方法区/元空间、堆，线程私有内存区包括程序计数器、Java栈、本地方法栈，元空间是在jdk1.8之后用来代替方法区的，可以通过-XX:+MetaspaceSize=10g -XX:+MaxMetaspaceSize=20g来指定元空间的大小，方法区/元空间用于存放类的定义信息、常量等信息，堆用于存放new出来的对象，Java栈和本地方法栈为线程私有主要存储线程中的对象引用和方法调用信息等，程序计数器则为了记录所在线程当前执行的指令位置。 JVM 里 new 对象时，堆会发生抢占吗？你是怎么去设计 JVM 的堆的线程安全的？ 不会发生抢占，JVM可以将new出来的对象存在堆上也可以存入线程栈中，也就是通过逃逸分析决定对象是分配到线程共享的堆上还是栈上分配，可以通过-XX:+DoEscapeAnalysis -XX:+EliminateLocks来开启逃逸分析，但是仅在-server模式下有效，可以通过java -version来查看若jre是server版本，则默认就是server模式。JVM堆的线程安全通过使用volatile关键字、ThreadLocal类或者加锁来实现。 讲一下 Redis 的数据结构 Redis内部是以key-value的形式存储数据的，每一个key-value都会以redisObject结构体，结构体中包括数据类型、编码方式、过期策略、引用数量、值结构体实例，值的结构体中包括的key的值， Redis 缓存同步问题 讲一讲 MySQL 的索引结构 MySQL的索引有B+树和Hash两种结构，在InnoDB存储引擎中，默认支持B+树结构，不支持Hash结构，但是我们可以使用Hash结构，InnoDB通过B+树实现自适应Hash来满足我们使用Hash结构的索引。MySQL之所以选择B+树作为存储结构，是因为其比AVL树高度更可控，比B-树在查询效率上更快。 讲一下 Redis 分布式锁的实现 Redis通过使用setnx+expire或者set key value nx ex time来设置分布式锁，也可以使用lua脚本创建分布式锁。三种方式中，setnx+expire可能会发生异常情况导致锁的key设置过期时间失败，最终锁无法自动释放而影响具体业务处理；分布式锁之所以设置过期时间，是为了防止在创建了锁之后未释放锁而产生的永久锁，这种情况将会导致其他线程永久性的加锁失败。 实际生产环境中，我们一般都是使用Redis集群为应用提供缓存服务，如果在多写的情况下，Master尚未同步到全部Slave之前，会出现同时有多个线程向不同的写服务器发起加锁请求，为了预防这种情况，在加锁的时候，可以向所有的节点同时发起写请求，这样保证了数据的强一致性。 ConcurrentHashMap 如何保证线程安全？ ConcurrentHashMap在jdk1.8之前使用Segment Lock(分段锁)的方式对数据段进行加锁，在该数据段加锁的时候，不会影响其他数据段的数据读写，从而达到提高并发的效率；在jdk1.8之后，使用CAS对具体的某个Node进行加锁，此方法仅仅对线程正在操作的那条数据加锁，不会影响到Map中其他数据的读写。 数据库索引了解吗？讲一下 MySQL索引有主键索引、普通索引两种，又可细分为单索引和复合索引，InnoDB中的索引存储结构默认为B+树，MyIsam中主键索引存储的是数据在磁盘上的位置，InnoDB中主键索引存储的是实际数据，普通索引中存储的是主键数据，所以在使用普通索引查找数据时，是先通过二分法查询到主键值，然后再到主键索引树中根据主键值查询出具体的数据，若是复合索引且所查询的字段均在复合索引中，此类索引查询称为覆盖索引，也就是不需要通过回表查询主键索引树即可返回。 常见排序算法 TCP 三次握手，四次挥手。 TCP在通信时需要先建立连接，为了保证数据发送的稳定性及可靠性，需要进行预通信，也就是我们平时与人见面时的握手礼仪，第一次握手是client端向server端发送一个通知(seq=x,SYN=1)，告诉server端我要向你发送数据了，是否做好了接收准备，然后client端状态变为SYN_SENT，server端若已经做好准备，则向client端回应一条消息告诉它我已经做好了准备，可以将数据发送过来了，此为第二次握手(seq=y,SYN=1,ACK=1,ack=x+1)，server端状态变为SYN_RECV，client端接收到server端回应的消息后，向server端发送确认包(seq=x+1,ACK=1,ack=y+1)，然后client和server端同时变更状态为ESTABLISHED，到此完成三次握手。 在通信结束后需要关闭连接，client端向server端发送断开请求(第一次挥手)，server端接收到请求后回应client端，然后server端进入到等待关闭状态(第二次挥手)，client接收到回应后向server端发送确认断开请求(第三次挥手)，server端收到确认消息后回应client进行断开(第四次挥手)，若最后client迟迟未接收到server的确认断开回应，则会重试一次，重试仍然未收到回应则自动断开。 深入问了乐观锁，悲观锁及其实现 悲观锁和乐观锁在读写效率上有很大的区别，悲观锁是在操作数据时，被操作的数据不可被其他线程/连接访问，除非等当前操作的事务提交或释放锁，在MySQL中，可以通过select for update来实现悲观锁；乐观锁在效率上比悲观锁高很多，可以通过版本进行数据控制，比如MySQL中的MVCC。 阿里巴巴二面 自我介绍 + 项目介绍。 你在项目中担任什么样的角色？ 那你觉得你比别人的优势在哪里？你用了哪些别人没有的东西吗？ 说一下 HashMap 的数据结构 红黑树和 AVL 树有什么区别？ 树高度 如何才能得到一个线程安全的 HashMap？ 讲一下 JVM 常用垃圾回收器 JVM常用垃圾收集器有CMS、G1， Redis 分布式锁 再描述一下你之前的项目吧 你觉得这个项目的亮点在哪里呢？ 你设计的数据库遵循的范式？ Java 怎么加载类？ linux 常用命令有哪些？ Spring 的 IOC, AOP。 讲一下 ORM 框架 Hibernate 设计模式了解吗？讲一下 自己实现一个二阶段提交，如何设计？ 阿里巴巴三面 在项目中，并发量大的情况下，如何才能够保证数据的一致性？ ElasticSearch 为什么检索快，它的底层数据结构是怎么样的？ JVM 内存模型 Netty 应用在哪些中间件和框架中呢？ 线程池的参数 讲一下 B 树和 B+ 树的区别 为什么要用 Redis 做缓存？ 了解 SpringBoot 吗？那讲一下 SpringBoot 的启动流程吧 如何解决 bean 的循环依赖问题？ Java 有哪些队列？ 讲一讲 Spring 和 Springboot 的区别 最近看了什么书？为什么？ 你平时是怎么学习 Java 的呢？ wait() 和 sleep() 的区别 原子变量的实现原理 CAS 的问题，讲一下解决方案。 有没有更好的计数器解决策略 讲一讲 NIO 和 BIO 的区别 Nginx 负载均衡时是如何判断某个节点挂掉了？ 讲一下 Redis 的数据类型和使用场景 k8s 的储存方式是怎样的？ Spring AOP 原理是什么？怎么使用？什么是切点，什么是切面？最好是举个例子 算法题：给一堆硬币的 array，返回所有的组合 阿里巴巴总监面 算法：给一个 set 打印出所有子集；多线程从多个文件中读入数据，写到同一个文件中； 判断 ip 是否在给定范围内；打乱一副扑克牌，不能用额外空间，证明为什么是随机的。 TCP 和 UDP 区别 线程池的原理以及各种线程池的应用场景 线程池中使用有限的阻塞队列和无限的阻塞队列的区别 如果你发现你的 SQL 语句始终走另一个索引，但是你希望它走你想要的索引，怎么办？ MySQL 执行计划 数据库索引为什么用 B+ 树？ 你在做 SQL 优化主要从哪几个方面做，用到哪些方法工具？ 有没有想问的？ 阿里巴巴 HR 面 自我介绍 平时怎么学习的？ 有什么兴趣爱好吗？ 怎么看待 996？ 怎么平衡工作和学习？ …… 有没有什么想问的","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://luxiaowan.github.io/tags/面试/"},{"name":"阿里","slug":"阿里","permalink":"http://luxiaowan.github.io/tags/阿里/"}]},{"title":"使用Docker发布项目","slug":"使用Docker发布项目","date":"2019-12-04T03:50:00.000Z","updated":"2020-03-24T05:09:46.712Z","comments":true,"path":"2019/12/04/使用Docker发布项目/","link":"","permalink":"http://luxiaowan.github.io/2019/12/04/使用Docker发布项目/","excerpt":"","text":"1、下载项目 此处使用公开的github上的项目：git clone https://github.com/luxiaowan/simple-eureka-server.git 2、编写Dockerfile #使用自己构建的jdk镜像 &lt; 查看 &gt; 1234567891011FROM docker.kevinlu.cc/env/jdk-8u191:190114MAINTAINER ccADD ./target/simple-eureka-server-1.0.jar /root/startup/ WORKDIR /root/startupEXPOSE 8080CMD [\"java\", \"-jar\", \"simple-eureka-server-1.0.jar\"] 3、构建项目 docker build -t simple-eureka-server:7 . 发现最后报错了，找不到我们的jar包，那是因为我们还没有编译打包我们的项目 所以在执行Dockerfile之前要先对项目进行编译打包 因为项目是Maven管理的，所以我们使用 mvn clean package进行打包(第一次使用Maven会有点慢，因为要下载Maven的基础库) &lt; 安装Maven &gt; 打包完成之后再进行构建，发现成功了，然后使用docker images查看刚构建的镜像 4、运行镜像 docker run -d -P 8080:8080 --name eureka-server simple-eureka-server:7 5、查看 http://IP:8080","categories":[{"name":"Docker","slug":"Docker","permalink":"http://luxiaowan.github.io/categories/Docker/"}],"tags":[]},{"title":"Docker网络模式简讲","slug":"Docker网络模式简讲","date":"2019-12-03T04:59:00.000Z","updated":"2020-03-24T05:10:17.425Z","comments":true,"path":"2019/12/03/Docker网络模式简讲/","link":"","permalink":"http://luxiaowan.github.io/2019/12/03/Docker网络模式简讲/","excerpt":"","text":"docker有四种网络模式：Bridge、Container、Host、None docker默认的网络模式为Bridge，通过在镜像启动时使用–net进行指定，不特殊声明则为默认模式。 docker进程启动时会在宿主机上创建一个名为docker0的虚拟网桥，此主机上启动的所有的docker镜像容器都会链接到这个虚拟网桥上。（虚拟网桥就是为了将镜像容器的虚拟机网络与主机网络建立桥接的一个交换机，与交换机的工作方式类似：：此处不再累赘，这都不懂就自刎了吧） 四大家族：Bridge、Container、Host、None 1、Bridge模式：默认、使用–net=bridge bridge模式是将每个docker镜像容器隔离开运行，分别映射到宿主机的端口上，每个容器都会被分配一个独立的网络命名空间，并且会把网络命名空间的IP映射到宿主机的docker0上。 a.启动命令： docker run --net=bridge --name xiaolu_one -dt xiaolu b.原理图： c.宿主服务器： d. docker容器： 2、host模式：–net=host host模式是与宿主机共用同一个网络空间，容器不会虚拟出自己的网卡、配置自己的IP等，也不会映射到docker0虚拟网桥，容器的IP、端口都与宿主机共用，极其容易发生冲突，所以不推荐使用此模式。虽然网络空间是与宿主机共用，但是文件系统、进程列表与宿主机是隔离的。 a.启动命令： docker run --net=host --name xiaolu_two -dt xiaolu b.原理图： c. 宿主服务器： d. docker容器： Tip：可以看出宿主机和docker容器查看到的内容一样 3、Container模式：–net=container:docker_container_name Container模式是将新创建的容器指定与一个已经存在的容器共用一个网络空间，新创建的容器不会创建自己的网卡、配置自己的IP，容器内的端口和IP都与指定容器使用同一个，但是新建容器的文件系统、进程列表是与宿主容器相隔离。 a.启动命令 123docker run --name xiaolu_three -dt xiaoludocker run --net=container:xiaolu_three --name xiaolu_four -dt xiaolu b.原理图 c.宿主服务器： d.Container容器： e. docker容器： Tip：可以看到docker容器看到的网络信息和Container容器的一样 4、None模式：–net=none None模式表示docker容器拥有自己的网络空间，但是并不为docker容器进行任何网络配置，也就是说这个docker容器没有网卡、IP、路由等信息，需要手动为docker容器配置。 a.启动命令： docker run --net=none --name xiaolu_five -dt xiaolu b.原理图","categories":[{"name":"Docker","slug":"Docker","permalink":"http://luxiaowan.github.io/categories/Docker/"}],"tags":[]},{"title":"使用Docker构建自己的JDK容器","slug":"使用Docker构建自己的JDK容器","date":"2019-12-02T16:10:00.000Z","updated":"2020-03-24T05:10:49.862Z","comments":true,"path":"2019/12/03/使用Docker构建自己的JDK容器/","link":"","permalink":"http://luxiaowan.github.io/2019/12/03/使用Docker构建自己的JDK容器/","excerpt":"","text":"★为了在Dockerfile中使用FROM自己的jdk，在此构建一个独立专属的 1、下载需要构建的jdk压缩包，使用压缩包即可，此处使用了jdk_8u191&lt; 下载 &gt;，根据自己需要，别乱下 2、因为jdk属于是系统环境配置，所以此处需要借助系统镜像去构建，此处借助centos:7来构建，查找可用的centos镜像： 2.1、docker search centos 第一个是官方的镜像，直接使用这个即可：docker pull docker.io/centos:7 (这个命令就不解释了，看不懂的此文也可用就此打住了) 2.2、下载完成之后使用命令查看当前存在的镜像：docker images -a 3、编写构建jdk镜像的Dockerfile，内容如下： 1234567891011FROM docker.io/centos:7MAINTAINER ccADD jdk-8u191-linux-x64.tar.gz /opt/localhost/ ------这个路径即为jdk的安装路径ENV JAVA_HOME /opt/localhost/jdk1.8.0_191 ------设置环境变量ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarENV PATH $PATH:$JAVA_HOME/bin 4、构建jdk的docker镜像 docker build -t docker.kevinlu.cc/env/jdk-8u191:190114 . -f Dockerfile 这个命令就没啥好讲的了，不认识的可以看&lt; 这里 &gt; 构建完成之后通过docker images命令查看镜像信息，构建的时候就会将构建信息打印到控制台： 可以看到最后的Successfully built 47a3b1aa0e55，这个47a3b1aa0e55就是镜像的IMAGE ID，同一个镜像多次构建，生成的IMAGE ID相同 5、运行jdk镜像 运行jdk镜像与其他普通镜像有些许不同，因为jdk属于是系统环境配置，所以运行命令为： docker run -d --name jdk8u191 -it 47a3b1aa0e55 /bin/bash ★创建容器的时候一定要使用 -it /bin/bash，不然jdk的容器起不来。 6、验证 docker ps查看当前运行的容器 docker exec -it jdk8u191 /bin/bash进入jdk容器内 java -version查看当前环境中jdk版本 大功告成！ 7、使用 &lt; 查看 &gt;","categories":[{"name":"Docker","slug":"Docker","permalink":"http://luxiaowan.github.io/categories/Docker/"}],"tags":[]},{"title":"Git常用命令","slug":"Git常用命令","date":"2019-12-02T09:23:00.000Z","updated":"2019-12-03T16:12:26.788Z","comments":true,"path":"2019/12/02/Git常用命令/","link":"","permalink":"http://luxiaowan.github.io/2019/12/02/Git常用命令/","excerpt":"","text":"","categories":[{"name":"Git","slug":"Git","permalink":"http://luxiaowan.github.io/categories/Git/"}],"tags":[]},{"title":"Docker基本命令使用","slug":"Docker基本命令使用","date":"2019-12-01T04:20:00.000Z","updated":"2020-03-24T05:11:15.111Z","comments":true,"path":"2019/12/01/Docker基本命令使用/","link":"","permalink":"http://luxiaowan.github.io/2019/12/01/Docker基本命令使用/","excerpt":"","text":"1.查看镜像 ​ docker images 2.查看启动中的容器 ​ docker ps 3.删除容器 ​ docker rm containerid 4.删除镜像 ​ docker rmi imageid 5.启动容器 ​ docker run -p 8080:8080 --name tomcat_one -dt xxx.xxx.xxx/tomcat 6.停止容器 ​ docker stop containerid ​ docker kill containerid 7.进入容器 ​ docker exec -it containerid bash 8.制作镜像(Dcokerfile所在目录) ​ docker build . -t image_name:latest 9.查看容器启动日志 ​ docker logs containerid","categories":[{"name":"Docker","slug":"Docker","permalink":"http://luxiaowan.github.io/categories/Docker/"}],"tags":[]},{"title":"Dockerfile详解","slug":"Dockerfile详解","date":"2019-11-30T16:20:00.000Z","updated":"2020-03-24T05:11:17.734Z","comments":true,"path":"2019/12/01/Dockerfile详解/","link":"","permalink":"http://luxiaowan.github.io/2019/12/01/Dockerfile详解/","excerpt":"","text":"Dockerfile分为四部分：基础镜像信息、维护者信息、镜像操作指令、容器启动时执行指令。 基础镜像信息：FROM ubutu 格式为FROM或FROM: ，Dockerfile第一条必须为FROM指令，如果一个Dockerfile要指定多个镜像，则每个镜像使用一个FROM指令 2.维护者信息：MAINTAINER xiaolu xiaolu@qq.com 格式为MAINTAINER，指定维护者信息，可省略 3.镜像操作指令：RUN apt-get update &amp;&amp; apt-get install -y vim 格式为RUN或RUN [&quot;executable&quot;, &quot;Param1&quot;, &quot;Param2&quot;] 格式RUN：在shell终端运行，即/bin/sh -C 格式RUN [“executable”, “Param1”, “Param2”]：使用exec执行 每条run指令在当前基础镜像执行，并且重新提交新镜像，当命令比较长时可用&quot;/&quot;换行 4.容器启动时执行指令：CMD /usr/sbin/nginx 每个容器只能执行一条CMD命令，多个CMD命令时，只会执行最后一条。 支持三种格式： 12345CMD [\"executable\", \"Param1\", \"Param2\"]：使用exec执行，推荐CMD command param1 param2：使用/bin/sh上执行CMD [\"Param1\", \"Param2\"]：提供给ENTRYPOINT做默认参数 5.暴露端口指令：EXPOSE，例如：EXPOSE 80 22 8080 格式为：EXPOSE port1 port2 port3 告诉Docker服务端容器暴露的端口号，供互联系统使用。 在启动Docker时，主机会自动分配一个端口号转发到指定的端口，可用通过-P/-p，指定主机具体端口号进行映射。 6.设置环境变量ENV 格式为ENV，指定一个环境变量，会被后续的RUN指令使用，并且会在容器运行过程中保持。 1234567ENV YC_NAME yc_frameworkENV YC_VERSION 1.0RUN wget -C http://www.kevinlu.cc/$YC_NAME/$YC_VERSION | mvn install /usr/src/$YC_NAME-$YC_VERSIONENV PATH /usr/src/$YC_NAME-$YC_VERSION/bin:$PATH 7.复制指定的文件到容器中ADD ADD hom* /usr/file 若/usr/file目录不存在，则自动创建 源目录可以是Dockerfile所在目录的一个相对路径；也可以是一个URL；也可以是一个tar文件（自动解压为目录） 8.复制本地主机的文件到容器COPY 格式：COPY file directory 拷贝的文件为Dockerfile所在目录的相对路径 9.容器启动后执行指令：ENTRYPOINT 格式：ENTRYPOINT [&quot;executable&quot;, &quot;Param1&quot;, &quot;Param2&quot;] ​ ENTRYPOINT command param1 param2 配置容器启动后需要执行的指令，并且不会被docker run提供的参数覆盖 每个Dockerfile中只能有一个ENTRYPOINT，如果有多个，则只会执行最后一个 10.指定工作目录WORKDIR 格式：WORKDIR /a/b/c 为后续的RUN、CMD、ENTRYPOINT指令配置工作目录 可以使用多个WORKDIR指令，后续的命令的参数为相对路径时，会基于之前命令指定的目录 例如： 123456789WORKDIR /aWORKDIR bWORKDIR cWORKDIR dRUN pwd&lt;打印出/a/b/c/d&gt;","categories":[{"name":"Docker","slug":"Docker","permalink":"http://luxiaowan.github.io/categories/Docker/"}],"tags":[]},{"title":"JVM之OOM","slug":"Java之OOM","date":"2019-11-28T14:23:00.000Z","updated":"2019-12-03T16:04:43.133Z","comments":true,"path":"2019/11/28/Java之OOM/","link":"","permalink":"http://luxiaowan.github.io/2019/11/28/Java之OOM/","excerpt":"","text":"java.lang.StackOverflowError 栈溢出错误，这个错误很容易模拟，且看下面的代码： 1234567891011121314151617public static void main(String[] args) &#123; new StackOverflowTest().test();&#125;private static int high = 0;private void test() &#123; try &#123; ++high; test(); &#125; finally &#123; System.out.println(\"栈的深度为: \" + high); &#125;&#125;---JVM ARGS: -server -Xmn2m -Xss1m -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+DoEscapeAnalysis -XX:+EliminateAllocations -XX:-UseTLAB 释 我们都知道方法的调用是通过入栈和计算出栈来实现的，所以我们在方法递归调用一定次数时，必然会发生栈溢出，栈溢出后，程序自动停止，是一类不可捕获和恢复的Error类型的错误，所以我们在使用递归算法时，应当注意递归的深度，防止出现栈溢出错误导致服务错误 java.lang.OutOfMemoryError:java heap space JVM堆空间不足引起的内存溢出错误，这类错误比较常见，此处就不做太多的解释，出现这类错误，你就去看GC日志，看看新生代、老年代、永久代/Metaspace的使用情况，如果是想查看GC的情况，使用如下JVM指令： -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:./gclog.log，gc的所有信息都会输出到gclog.log文件中 gclog.log 123456789101112131415161718192021222324252627*JVM信息*Java HotSpot(TM) 64-Bit Server VM (25.161-b12) for bsd-amd64 JRE (1.8.0_161-b12), built on Dec 19 2017 16:22:20 by \"java_re\" with gcc 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2336.11.00)Memory: 4k page, physical 16777216k(2991720k free)/proc/meminfo:*JVM ARGS*CommandLine flags: -XX:+DoEscapeAnalysis -XX:+EliminateAllocations -XX:InitialHeapSize=268435456 -XX:MaxHeapSize=4294967296 -XX:MaxNewSize=2097152 -XX:NewSize=2097152 -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:ThreadStackSize=1024 -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGC -XX:-UseTLAB *GC日志信息*0.125: [GC (Allocation Failure) [PSYoungGen: 1023K-&gt;512K(1536K)] 1023K-&gt;536K(261632K), 0.0010704 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] 0.157: [GC (Allocation Failure) [PSYoungGen: 1535K-&gt;493K(1536K)] 1559K-&gt;847K(261632K), 0.0010655 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] ...0.360: [GC (Allocation Failure) [PSYoungGen: 1247K-&gt;256K(1536K)] 2614K-&gt;1727K(261632K), 0.0008285 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] Heap *年轻代* PSYoungGen total 1536K, used 396K [0x00000007bfe00000, 0x00000007c0000000, 0x00000007c0000000) eden space 1024K, 13% used [0x00000007bfe00000,0x00000007bfe23268,0x00000007bff00000) from space 512K, 50% used [0x00000007bff00000,0x00000007bff40000,0x00000007bff80000) to space 512K, 0% used [0x00000007bff80000,0x00000007bff80000,0x00000007c0000000) *老年代* ParOldGen total 260096K, used 1471K [0x00000006c0000000, 0x00000006cfe00000, 0x00000007bfe00000) object space 260096K, 0% used [0x00000006c0000000,0x00000006c016fc00,0x00000006cfe00000) *Metaspace空间，jdk8+* Metaspace used 3402K, capacity 4500K, committed 4864K, reserved 1056768K class space used 368K, capacity 388K, committed 512K, reserved 1048576K 代码 12345678910111213public static void main(String[] args) &#123; new StackOverflowTest().heapSpace();&#125;private void heapSpace() &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); while (true) &#123; list.add(new String(\"abc\")); &#125;&#125;--- JVM ARGS: -server -Xmn2m -Xss1m -Xms1m -Xmx1m -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:-DoEscapeAnalysis -XX:-EliminateAllocations -XX:-UseTLAB java.lang.OutOfMemoryError:GC overhead limit exceeded 超出了GC开销限制引起的内存溢出，这个错误不是特别常见，Sun 官方对此的定义：超过98%的时间用来做GC并且回收了不到2%的堆内存时会抛出此异常，可以使用参数-XX:-UseGCOverheadLimit 禁用这个检查，但是这个参数解决不了内存问题，只是把错误的信息延后，替换成 java.lang.OutOfMemoryError: Java heap space java.lang.OutOfMemoryError:Metaspace Metaspace内存溢出，Metaspace是jdk8+特有的东西，用来代替之前的PermGen，主要存储class名称、字段、方法、字节码、常量池、JIT优化代码等等，我们可以使用-XX:MetaspaceSize和-XX:MaxMetaspaceSize来指定其大小，一般情况下Metaspace不会发生OOM，Metaspace的使用量与JVM加载的class数量有很大关系： 代码 12345678910111213141516static ClassPool cp = ClassPool.getDefault();public static void main(String[] args) throws CannotCompileException &#123; int i = 0; try &#123; for (;; i++) &#123; Class cz = cp.makeClass(\"com.example.demo.bean.DemoBean\" + i).toClass(); &#125; &#125; catch (Exception e) &#123; &#125; finally &#123; System.out.println(i); &#125;&#125;--- JVM ARGS: -XX:MetaspaceSize=10m -XX:MaxMetaspaceSize=10m -XX:+PrintGCDetails -XX:+PrintGCTimeStamps 输出 1234567891011121314151617181920210.598: [GC (Metadata GC Threshold) [PSYoungGen: 39345K-&gt;10741K(76288K)] 39345K-&gt;15811K(251392K), 0.0111319 secs] [Times: user=0.05 sys=0.01, real=0.01 secs] 0.609: [Full GC (Metadata GC Threshold) [PSYoungGen: 10741K-&gt;0K(76288K)] [ParOldGen: 5069K-&gt;15550K(139776K)] 15811K-&gt;15550K(216064K), [Metaspace: 9735K-&gt;9735K(1056768K)], 0.0504762 secs] [Times: user=0.29 sys=0.01, real=0.05 secs] ...0.754: [GC (Last ditch collection) [PSYoungGen: 0K-&gt;0K(82944K)] 15477K-&gt;15477K(472064K), 0.0008113 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] 0.755: [Full GC (Last ditch collection) [PSYoungGen: 0K-&gt;0K(82944K)] [ParOldGen: 15477K-&gt;15477K(607232K)] 15477K-&gt;15477K(690176K), [Metaspace: 9733K-&gt;9733K(1056768K)], 0.0204189 secs] [Times: user=0.08 sys=0.00, real=0.02 secs] 5341Exception in thread \"main\" java.lang.OutOfMemoryError: Metaspace at javassist.ClassPool.toClass(ClassPool.java:1170) at javassist.ClassPool.toClass(ClassPool.java:1113) at javassist.ClassPool.toClass(ClassPool.java:1071) at javassist.CtClass.toClass(CtClass.java:1275) at com.example.demo.jvm.MetaspceOOMTest.main(MetaspceOOMTest.java:13)Heap PSYoungGen total 82944K, used 2390K [0x000000076ab00000, 0x0000000772c00000, 0x00000007c0000000) eden space 82432K, 2% used [0x000000076ab00000,0x000000076ad55ab0,0x000000076fb80000) from space 512K, 0% used [0x0000000772b80000,0x0000000772b80000,0x0000000772c00000) to space 10752K, 0% used [0x0000000771700000,0x0000000771700000,0x0000000772180000) ParOldGen total 607232K, used 15477K [0x00000006c0000000, 0x00000006e5100000, 0x000000076ab00000) object space 607232K, 2% used [0x00000006c0000000,0x00000006c0f1d4c8,0x00000006e5100000) Metaspace used 9770K, capacity 10084K, committed 10240K, reserved 1056768K class space used 3165K, capacity 3214K, committed 3328K, reserved 1048576K 我们将Metaspace的初始大小和最大值都设置为10m，最终i的值大概会在5340左右的时候报OOM，从FGC的日志可以看出，Metaspace在整个GC阶段都未进行任务的内存回收，直至被全部用完，具体的关于Metaspace的介绍可以看下PerfMa社区的这篇文章：https://club.perfma.com/article/210111 java.lang.OutOfMemoryError:Direct buffer memory ByteBuffer. allocateDirect (int capability)是分配操作系统的本地内存，不在GC管辖范围之内，由于不需要内存拷贝所以速度相对较快，但如果不断分配本地内存，堆内存就会很少使用，那么JVM就不需要进行GC，那创建的DirectByteBuffer对象就不会被回收，就会出现堆内存充足但本地内存不足的情况，继续尝试分配本地内存就会出现OOM。 代码 1234567public static void main(String[] args) &#123; System.out.println(\"当前direct大小: \" + (VM.maxDirectMemory() / 1024 / 1024) + \" MB\"); ByteBuffer bb = ByteBuffer.allocateDirect(Math.toIntExact(VM.maxDirectMemory() + 10));&#125;--- JVM ARGS: -XX:MaxDirectMemorySize=10m 这里我们需要通过JVM参数-XX:MaxDirectMemorySize=10将JVM本地最大使用内存设置为10MB，不然如果你本地剩余内存很大，那么就很难模拟出此错误 输出 123456当前direct大小: 10 MBException in thread \"main\" java.lang.OutOfMemoryError: Direct buffer memory at java.nio.Bits.reserveMemory(Bits.java:694) at java.nio.DirectByteBuffer.&lt;init&gt;(DirectByteBuffer.java:123) at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311) at com.example.demo.jvm.DirectBufferOOMTest.main(DirectBufferOOMTest.java:11) java.lang.OutOfMemoryError:unable create new native thread 线程创建的太多，导致无法继续创建线程，出现这个问题就要去使用jstack导出线程栈查看具体情况 代码 12345678910111213public static void main(String[] args) &#123; while (true) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(100000); &#125; catch (InterruptedException e) &#123; &#125; &#125; &#125;).start(); &#125;&#125; 这一段代码必然会出现该ERROR，不论你的机器有多牛掰，你会发现出现了OOM之后，进程并未终止，这个时候你可以用jps命令查看进程号，然后使用jstack pid查看线程栈，会发现有非常多的线程处于TIMED_WAITING (sleeping)状态： 12345\"Thread-256\" #267 prio=5 os_prio=31 tid=0x00007fccdd8cc000 nid=0x27d03 waiting on condition [0x0000700019b85000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at com.example.demo.jvm.NativeThreadOOMTest$1.run(NativeThreadOOMTest.java:11) at java.lang.Thread.run(Thread.java:748)","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[]},{"title":"SpringBoot官方配置文档(英文版)","slug":"SpringBoot官方配置文档(英文版)","date":"2019-11-22T17:13:00.000Z","updated":"2019-11-25T17:18:31.849Z","comments":true,"path":"2019/11/23/SpringBoot官方配置文档(英文版)/","link":"","permalink":"http://luxiaowan.github.io/2019/11/23/SpringBoot官方配置文档(英文版)/","excerpt":"","text":"BANNER banner.charset=UTF-8 # Banner file encoding. banner.location=classpath:banner.txt # Banner file location. banner.image.location=classpath:banner.gif # Banner image file location (jpg/png can also be used). banner.image.width= # Width of the banner image in chars (default 76) banner.image.height= # Height of the banner image in chars (default based on image height) banner.image.margin= # Left hand image margin in chars (default 2) banner.image.invert= # If images should be inverted for dark terminal themes (default false) LOGGING logging.config= # Location of the logging configuration file. For instance classpath:logback.xml for Logback logging.exception-conversion-word=%wEx # Conversion word used when logging exceptions. logging.file= # Log file name. For instance myapp.log logging.level.*= # Log levels severity mapping. For instance logging.level.org.springframework=DEBUG logging.path= # Location of the log file. For instance /var/log logging.pattern.console= # Appender pattern for output to the console. Only supported with the default logback setup. logging.pattern.file= # Appender pattern for output to the file. Only supported with the default logback setup. logging.pattern.level= # Appender pattern for log level (default %5p). Only supported with the default logback setup. logging.register-shutdown-hook=false # Register a shutdown hook for the logging system when it is initialized. AOP spring.aop.auto=true # Add @EnableAspectJAutoProxy. spring.aop.proxy-target-class=true # Whether subclass-based (CGLIB) proxies are to be created (true) as opposed to standard Java interface-based proxies (false). IDENTITY (ContextIdApplicationContextInitializer) spring.application.index= # Application index. spring.application.name= # Application name. ADMIN (SpringApplicationAdminJmxAutoConfiguration) spring.application.admin.enabled=false # Enable admin features for the application. spring.application.admin.jmx-name=org.springframework.boot:type=Admin,name=SpringApplication # JMX name of the application admin MBean. AUTO-CONFIGURATION spring.autoconfigure.exclude= # Auto-configuration classes to exclude. SPRING CORE spring.beaninfo.ignore=true # Skip search of BeanInfo classes. SPRING CACHE (CacheProperties) spring.cache.cache-names= # Comma-separated list of cache names to create if supported by the underlying cache manager. spring.cache.caffeine.spec= # The spec to use to create caches. Check CaffeineSpec for more details on the spec format. spring.cache.couchbase.expiration=0 # Entry expiration in milliseconds. By default the entries never expire. spring.cache.ehcache.config= # The location of the configuration file to use to initialize EhCache. spring.cache.infinispan.config= # The location of the configuration file to use to initialize Infinispan. spring.cache.jcache.config= # The location of the configuration file to use to initialize the cache manager. spring.cache.jcache.provider= # Fully qualified name of the CachingProvider implementation to use to retrieve the JSR-107 compliant cache manager. Only needed if more than one JSR-107 implementation is available on the classpath. spring.cache.type= # Cache type, auto-detected according to the environment by default. SPRING CONFIG - using environment property only (ConfigFileApplicationListener) spring.config.location= # Config file locations. spring.config.name=application # Config file name. HAZELCAST (HazelcastProperties) spring.hazelcast.config= # The location of the configuration file to use to initialize Hazelcast. PROJECT INFORMATION (ProjectInfoProperties) spring.info.build.location=classpath:META-INF/build-info.properties # Location of the generated build-info.properties file. spring.info.git.location=classpath:git.properties # Location of the generated git.properties file. JMX spring.jmx.default-domain= # JMX domain name. spring.jmx.enabled=true # Expose management beans to the JMX domain. spring.jmx.server=mbeanServer # MBeanServer bean name. Email (MailProperties) spring.mail.default-encoding=UTF-8 # Default MimeMessage encoding. spring.mail.host= # SMTP server host. For instance smtp.example.com spring.mail.jndi-name= # Session JNDI name. When set, takes precedence to others mail settings. spring.mail.password= # Login password of the SMTP server. spring.mail.port= # SMTP server port. spring.mail.properties.*= # Additional JavaMail session properties. spring.mail.protocol=smtp # Protocol used by the SMTP server. spring.mail.test-connection=false # Test that the mail server is available on startup. spring.mail.username= # Login user of the SMTP server. APPLICATION SETTINGS (SpringApplication) spring.main.banner-mode=console # Mode used to display the banner when the application runs. spring.main.sources= # Sources (class name, package name or XML resource location) to include in the ApplicationContext. spring.main.web-application-type= # Flag to explicitly request a specific type of web application. Auto-detected based on the classpath if not set. FILE ENCODING (FileEncodingApplicationListener) spring.mandatory-file-encoding= # Expected character encoding the application must use. INTERNATIONALIZATION (MessageSourceAutoConfiguration) spring.messages.always-use-message-format=false # Set whether to always apply the MessageFormat rules, parsing even messages without arguments. spring.messages.basename=messages # Comma-separated list of basenames, each following the ResourceBundle convention. spring.messages.cache-seconds=-1 # Loaded resource bundle files cache expiration, in seconds. When set to -1, bundles are cached forever. spring.messages.encoding=UTF-8 # Message bundles encoding. spring.messages.fallback-to-system-locale=true # Set whether to fall back to the system Locale if no files for a specific Locale have been found. OUTPUT spring.output.ansi.enabled=detect # Configure the ANSI output. PID FILE (ApplicationPidFileWriter) spring.pid.fail-on-write-error= # Fail if ApplicationPidFileWriter is used but it cannot write the PID file. spring.pid.file= # Location of the PID file to write (if ApplicationPidFileWriter is used). PROFILES spring.profiles.active= # Comma-separated list (or list if using YAML) of active profiles. spring.profiles.include= # Unconditionally activate the specified comma separated profiles (or list of profiles if using YAML). Reactor spring.reactor.stacktrace-mode.enabled=false # Set whether Reactor should collect stacktrace information at runtime. SENDGRID (SendGridAutoConfiguration) spring.sendgrid.api-key= # SendGrid api key (alternative to username/password) spring.sendgrid.proxy.host= # SendGrid proxy host spring.sendgrid.proxy.port= # SendGrid proxy port EMBEDDED SERVER CONFIGURATION (ServerProperties) server.address= # Network address to which the server should bind to. server.compression.enabled=false # If response compression is enabled. server.compression.excluded-user-agents= # List of user-agents to exclude from compression. server.compression.mime-types= # Comma-separated list of MIME types that should be compressed. For instance text/html,text/css,application/json server.compression.min-response-size= # Minimum response size that is required for compression to be performed. For instance 2048 server.connection-timeout= # Time in milliseconds that connectors will wait for another HTTP request before closing the connection. When not set, the connector’s container-specific default will be used. Use a value of -1 to indicate no (i.e. infinite) timeout. server.display-name=application # Display name of the application. server.max-http-header-size=0 # Maximum size in bytes of the HTTP message header. server.error.include-exception=false # Include the “exception” attribute. server.error.include-stacktrace=never # When to include a “stacktrace” attribute. server.error.path=/error # Path of the error controller. server.error.whitelabel.enabled=true # Enable the default error page displayed in browsers in case of a server error. server.jetty.acceptors= # Number of acceptor threads to use. server.jetty.accesslog.append=false # Append to log. server.jetty.accesslog.date-format=dd/MMM/yyyy:HH:mm:ss Z # Timestamp format of the request log. server.jetty.accesslog.enabled=false # Enable access log. server.jetty.accesslog.extended-format=false # Enable extended NCSA format. server.jetty.accesslog.file-date-format= # Date format to place in log file name. server.jetty.accesslog.filename= # Log filename. If not specified, logs will be redirected to “System.err”. server.jetty.accesslog.locale= # Locale of the request log. server.jetty.accesslog.log-cookies=false # Enable logging of the request cookies. server.jetty.accesslog.log-latency=false # Enable logging of request processing time. server.jetty.accesslog.log-server=false # Enable logging of the request hostname. server.jetty.accesslog.retention-period=31 # Number of days before rotated log files are deleted. server.jetty.accesslog.time-zone=GMT # Timezone of the request log. server.jetty.max-http-post-size=0 # Maximum size in bytes of the HTTP post or put content. server.jetty.selectors= # Number of selector threads to use. server.port=8080 # Server HTTP port. server.server-header= # Value to use for the Server response header (no header is sent if empty) server.use-forward-headers= # If X-Forwarded-* headers should be applied to the HttpRequest. server.servlet.context-parameters.= # Servlet context init parameters server.servlet.context-path= # Context path of the application. server.servlet.jsp.class-name=org.apache.jasper.servlet.JspServlet # The class name of the JSP servlet. server.servlet.jsp.init-parameters.= # Init parameters used to configure the JSP servlet server.servlet.jsp.registered=true # Whether or not the JSP servlet is registered server.servlet.path=/ # Path of the main dispatcher servlet. server.session.cookie.comment= # Comment for the session cookie. server.session.cookie.domain= # Domain for the session cookie. server.session.cookie.http-only= # “HttpOnly” flag for the session cookie. server.session.cookie.max-age= # Maximum age of the session cookie in seconds. server.session.cookie.name= # Session cookie name. server.session.cookie.path= # Path of the session cookie. server.session.cookie.secure= # “Secure” flag for the session cookie. server.session.persistent=false # Persist session data between restarts. server.session.store-dir= # Directory used to store session data. server.session.timeout= # Session timeout in seconds. server.session.tracking-modes= # Session tracking modes (one or more of the following: “cookie”, “url”, “ssl”). server.ssl.ciphers= # Supported SSL ciphers. server.ssl.client-auth= # Whether client authentication is wanted (“want”) or needed (“need”). Requires a trust store. server.ssl.enabled= # Enable SSL support. server.ssl.enabled-protocols= # Enabled SSL protocols. server.ssl.key-alias= # Alias that identifies the key in the key store. server.ssl.key-password= # Password used to access the key in the key store. server.ssl.key-store= # Path to the key store that holds the SSL certificate (typically a jks file). server.ssl.key-store-password= # Password used to access the key store. server.ssl.key-store-provider= # Provider for the key store. server.ssl.key-store-type= # Type of the key store. server.ssl.protocol=TLS # SSL protocol to use. server.ssl.trust-store= # Trust store that holds SSL certificates. server.ssl.trust-store-password= # Password used to access the trust store. server.ssl.trust-store-provider= # Provider for the trust store. server.ssl.trust-store-type= # Type of the trust store. server.tomcat.accept-count= # Maximum queue length for incoming connection requests when all possible request processing threads are in use. server.tomcat.accesslog.buffered=true # Buffer output such that it is only flushed periodically. server.tomcat.accesslog.directory=logs # Directory in which log files are created. Can be relative to the tomcat base dir or absolute. server.tomcat.accesslog.enabled=false # Enable access log. server.tomcat.accesslog.file-date-format=.yyyy-MM-dd # Date format to place in log file name. server.tomcat.accesslog.pattern=common # Format pattern for access logs. server.tomcat.accesslog.prefix=access_log # Log file name prefix. server.tomcat.accesslog.rename-on-rotate=false # Defer inclusion of the date stamp in the file name until rotate time. server.tomcat.accesslog.request-attributes-enabled=false # Set request attributes for IP address, Hostname, protocol and port used for the request. server.tomcat.accesslog.rotate=true # Enable access log rotation. server.tomcat.accesslog.suffix=.log # Log file name suffix. server.tomcat.additional-tld-skip-patterns= # Comma-separated list of additional patterns that match jars to ignore for TLD scanning. server.tomcat.background-processor-delay=30 # Delay in seconds between the invocation of backgroundProcess methods. server.tomcat.basedir= # Tomcat base directory. If not specified a temporary directory will be used. server.tomcat.internal-proxies=10\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}|\\ 192\\.168\\.\\d{1,3}\\.\\d{1,3}|\\ 169\\.254\\.\\d{1,3}\\.\\d{1,3}|\\ 127\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}|\\ 172\\.1[6-9]{1}\\.\\d{1,3}\\.\\d{1,3}|\\ 172\\.2[0-9]{1}\\.\\d{1,3}\\.\\d{1,3}|\\ 172\\.3[0-1]{1}\\.\\d{1,3}\\.\\d{1,3} # regular expression matching trusted IP addresses. server.tomcat.max-connections= # Maximum number of connections that the server will accept and process at any given time. server.tomcat.max-http-header-size=0 # Maximum size in bytes of the HTTP message header. server.tomcat.max-http-post-size=0 # Maximum size in bytes of the HTTP post content. server.tomcat.max-threads=0 # Maximum amount of worker threads. server.tomcat.min-spare-threads=0 # Minimum amount of worker threads. server.tomcat.port-header=X-Forwarded-Port # Name of the HTTP header used to override the original port value. server.tomcat.protocol-header= # Header that holds the incoming protocol, usually named “X-Forwarded-Proto”. server.tomcat.protocol-header-https-value=https # Value of the protocol header that indicates that the incoming request uses SSL. server.tomcat.redirect-context-root= # Whether requests to the context root should be redirected by appending a / to the path. server.tomcat.remote-ip-header= # Name of the http header from which the remote ip is extracted. For instance X-FORWARDED-FOR server.tomcat.uri-encoding=UTF-8 # Character encoding to use to decode the URI. server.undertow.accesslog.dir= # Undertow access log directory. server.undertow.accesslog.enabled=false # Enable access log. server.undertow.accesslog.pattern=common # Format pattern for access logs. server.undertow.accesslog.prefix=access_log. # Log file name prefix. server.undertow.accesslog.rotate=true # Enable access log rotation. server.undertow.accesslog.suffix=log # Log file name suffix. server.undertow.buffer-size= # Size of each buffer in bytes. server.undertow.direct-buffers= # Allocate buffers outside the Java heap. server.undertow.io-threads= # Number of I/O threads to create for the worker. server.undertow.eager-filter-init=true # Whether servlet filters should be initialized on startup. server.undertow.max-http-post-size=0 # Maximum size in bytes of the HTTP post content. server.undertow.worker-threads= # Number of worker threads. FREEMARKER (FreeMarkerAutoConfiguration) spring.freemarker.allow-request-override=false # Set whether HttpServletRequest attributes are allowed to override (hide) controller generated model attributes of the same name. spring.freemarker.allow-session-override=false # Set whether HttpSession attributes are allowed to override (hide) controller generated model attributes of the same name. spring.freemarker.cache=false # Enable template caching. spring.freemarker.charset=UTF-8 # Template encoding. spring.freemarker.check-template-location=true # Check that the templates location exists. spring.freemarker.content-type=text/html # Content-Type value. spring.freemarker.enabled=true # Enable MVC view resolution for this technology. spring.freemarker.expose-request-attributes=false # Set whether all request attributes should be added to the model prior to merging with the template. spring.freemarker.expose-session-attributes=false # Set whether all HttpSession attributes should be added to the model prior to merging with the template. spring.freemarker.expose-spring-macro-helpers=true # Set whether to expose a RequestContext for use by Spring’s macro library, under the name “springMacroRequestContext”. spring.freemarker.prefer-file-system-access=true # Prefer file system access for template loading. File system access enables hot detection of template changes. spring.freemarker.prefix= # Prefix that gets prepended to view names when building a URL. spring.freemarker.request-context-attribute= # Name of the RequestContext attribute for all views. spring.freemarker.settings.*= # Well-known FreeMarker keys which will be passed to FreeMarker’s Configuration. spring.freemarker.suffix= # Suffix that gets appended to view names when building a URL. spring.freemarker.template-loader-path=classpath:/templates/ # Comma-separated list of template paths. spring.freemarker.view-names= # White list of view names that can be resolved. GROOVY TEMPLATES (GroovyTemplateAutoConfiguration) spring.groovy.template.allow-request-override=false # Set whether HttpServletRequest attributes are allowed to override (hide) controller generated model attributes of the same name. spring.groovy.template.allow-session-override=false # Set whether HttpSession attributes are allowed to override (hide) controller generated model attributes of the same name. spring.groovy.template.cache= # Enable template caching. spring.groovy.template.charset=UTF-8 # Template encoding. spring.groovy.template.check-template-location=true # Check that the templates location exists. spring.groovy.template.configuration.*= # See GroovyMarkupConfigurer spring.groovy.template.content-type=test/html # Content-Type value. spring.groovy.template.enabled=true # Enable MVC view resolution for this technology. spring.groovy.template.expose-request-attributes=false # Set whether all request attributes should be added to the model prior to merging with the template. spring.groovy.template.expose-session-attributes=false # Set whether all HttpSession attributes should be added to the model prior to merging with the template. spring.groovy.template.expose-spring-macro-helpers=true # Set whether to expose a RequestContext for use by Spring’s macro library, under the name “springMacroRequestContext”. spring.groovy.template.prefix= # Prefix that gets prepended to view names when building a URL. spring.groovy.template.request-context-attribute= # Name of the RequestContext attribute for all views. spring.groovy.template.resource-loader-path=classpath:/templates/ # Template path. spring.groovy.template.suffix=.tpl # Suffix that gets appended to view names when building a URL. spring.groovy.template.view-names= # White list of view names that can be resolved. SPRING HATEOAS (HateoasProperties) spring.hateoas.use-hal-as-default-json-media-type=true # Specify if application/hal+json responses should be sent to requests that accept application/json. HTTP message conversion spring.http.converters.preferred-json-mapper=jackson # Preferred JSON mapper to use for HTTP message conversion. Set to “gson” to force the use of Gson when both it and Jackson are on the classpath. HTTP encoding (HttpEncodingProperties) spring.http.encoding.charset=UTF-8 # Charset of HTTP requests and responses. Added to the “Content-Type” header if not set explicitly. spring.http.encoding.enabled=true # Enable http encoding support. spring.http.encoding.force= # Force the encoding to the configured charset on HTTP requests and responses. spring.http.encoding.force-request= # Force the encoding to the configured charset on HTTP requests. Defaults to true when “force” has not been specified. spring.http.encoding.force-response= # Force the encoding to the configured charset on HTTP responses. spring.http.encoding.mapping= # Locale to Encoding mapping. MULTIPART (MultipartProperties) spring.servlet.multipart.enabled=true # Enable support of multipart uploads. spring.servlet.multipart.file-size-threshold=0 # Threshold after which files will be written to disk. Values can use the suffixes “MB” or “KB” to indicate megabytes or kilobytes respectively. spring.servlet.multipart.location= # Intermediate location of uploaded files. spring.servlet.multipart.max-file-size=1MB # Max file size. Values can use the suffixes “MB” or “KB” to indicate megabytes or kilobytes respectively. spring.servlet.multipart.max-request-size=10MB # Max request size. Values can use the suffixes “MB” or “KB” to indicate megabytes or kilobytes respectively. spring.servlet.multipart.resolve-lazily=false # Whether to resolve the multipart request lazily at the time of file or parameter access. JACKSON (JacksonProperties) spring.jackson.date-format= # Date format string or a fully-qualified date format class name. For instance yyyy-MM-dd HH:mm:ss. spring.jackson.default-property-inclusion= # Controls the inclusion of properties during serialization. spring.jackson.deserialization.= # Jackson on/off features that affect the way Java objects are deserialized. spring.jackson.generator.= # Jackson on/off features for generators. spring.jackson.joda-date-time-format= # Joda date time format string. If not configured, “date-format” will be used as a fallback if it is configured with a format string. spring.jackson.locale= # Locale used for formatting. spring.jackson.mapper.= # Jackson general purpose on/off features. spring.jackson.parser.= # Jackson on/off features for parsers. spring.jackson.property-naming-strategy= # One of the constants on Jackson’s PropertyNamingStrategy. Can also be a fully-qualified class name of a PropertyNamingStrategy subclass. spring.jackson.serialization.*= # Jackson on/off features that affect the way Java objects are serialized. spring.jackson.time-zone= # Time zone used when formatting dates. For instance America/Los_Angeles JERSEY (JerseyProperties) spring.jersey.application-path= # Path that serves as the base URI for the application. Overrides the value of “@ApplicationPath” if specified. spring.jersey.filter.order=0 # Jersey filter chain order. spring.jersey.init.*= # Init parameters to pass to Jersey via the servlet or filter. spring.jersey.servlet.load-on-startup=-1 # Load on startup priority of the Jersey servlet. spring.jersey.type=servlet # Jersey integration type. SPRING LDAP (LdapProperties) spring.ldap.urls= # LDAP URLs of the server. spring.ldap.base= # Base suffix from which all operations should originate. spring.ldap.username= # Login user of the server. spring.ldap.password= # Login password of the server. spring.ldap.base-environment.*= # LDAP specification settings. EMBEDDED LDAP (EmbeddedLdapProperties) spring.ldap.embedded.base-dn= # The base DN spring.ldap.embedded.credential.username= # Embedded LDAP username. spring.ldap.embedded.credential.password= # Embedded LDAP password. spring.ldap.embedded.ldif=classpath:schema.ldif # Schema (LDIF) script resource reference. spring.ldap.embedded.port= # Embedded LDAP port. spring.ldap.embedded.validation.enabled=true # Enable LDAP schema validation. spring.ldap.embedded.validation.schema= # Path to the custom schema. SPRING MOBILE DEVICE VIEWS (DeviceDelegatingViewResolverAutoConfiguration) spring.mobile.devicedelegatingviewresolver.enable-fallback=false # Enable support for fallback resolution. spring.mobile.devicedelegatingviewresolver.enabled=false # Enable device view resolver. spring.mobile.devicedelegatingviewresolver.mobile-prefix=mobile/ # Prefix that gets prepended to view names for mobile devices. spring.mobile.devicedelegatingviewresolver.mobile-suffix= # Suffix that gets appended to view names for mobile devices. spring.mobile.devicedelegatingviewresolver.normal-prefix= # Prefix that gets prepended to view names for normal devices. spring.mobile.devicedelegatingviewresolver.normal-suffix= # Suffix that gets appended to view names for normal devices. spring.mobile.devicedelegatingviewresolver.tablet-prefix=tablet/ # Prefix that gets prepended to view names for tablet devices. spring.mobile.devicedelegatingviewresolver.tablet-suffix= # Suffix that gets appended to view names for tablet devices. SPRING MOBILE SITE PREFERENCE (SitePreferenceAutoConfiguration) spring.mobile.sitepreference.enabled=true # Enable SitePreferenceHandler. MUSTACHE TEMPLATES (MustacheAutoConfiguration) spring.mustache.allow-request-override= # Set whether HttpServletRequest attributes are allowed to override (hide) controller generated model attributes of the same name. spring.mustache.allow-session-override= # Set whether HttpSession attributes are allowed to override (hide) controller generated model attributes of the same name. spring.mustache.cache= # Enable template caching. spring.mustache.charset= # Template encoding. spring.mustache.check-template-location= # Check that the templates location exists. spring.mustache.content-type= # Content-Type value. spring.mustache.enabled= # Enable MVC view resolution for this technology. spring.mustache.expose-request-attributes= # Set whether all request attributes should be added to the model prior to merging with the template. spring.mustache.expose-session-attributes= # Set whether all HttpSession attributes should be added to the model prior to merging with the template. spring.mustache.expose-spring-macro-helpers= # Set whether to expose a RequestContext for use by Spring’s macro library, under the name “springMacroRequestContext”. spring.mustache.prefix=classpath:/templates/ # Prefix to apply to template names. spring.mustache.request-context-attribute= # Name of the RequestContext attribute for all views. spring.mustache.suffix=.mustache # Suffix to apply to template names. spring.mustache.view-names= # White list of view names that can be resolved. SPRING MVC (WebMvcProperties) spring.mvc.async.request-timeout= # Amount of time (in milliseconds) before asynchronous request handling times out. spring.mvc.date-format= # Date format to use. For instance dd/MM/yyyy. spring.mvc.dispatch-trace-request=false # Dispatch TRACE requests to the FrameworkServlet doService method. spring.mvc.dispatch-options-request=true # Dispatch OPTIONS requests to the FrameworkServlet doService method. spring.mvc.favicon.enabled=true # Enable resolution of favicon.ico. spring.mvc.formcontent.putfilter.enabled=true # Enable Spring’s HttpPutFormContentFilter. spring.mvc.ignore-default-model-on-redirect=true # If the content of the “default” model should be ignored during redirect scenarios. spring.mvc.locale= # Locale to use. By default, this locale is overridden by the “Accept-Language” header. spring.mvc.locale-resolver=accept-header # Define how the locale should be resolved. spring.mvc.log-resolved-exception=false # Enable warn logging of exceptions resolved by a “HandlerExceptionResolver”. spring.mvc.media-types.*= # Maps file extensions to media types for content negotiation. spring.mvc.message-codes-resolver-format= # Formatting strategy for message codes. For instance PREFIX_ERROR_CODE. spring.mvc.servlet.load-on-startup=-1 # Load on startup priority of the Spring Web Services servlet. spring.mvc.static-path-pattern=/** # Path pattern used for static resources. spring.mvc.throw-exception-if-no-handler-found=false # If a “NoHandlerFoundException” should be thrown if no Handler was found to process a request. spring.mvc.view.prefix= # Spring MVC view prefix. spring.mvc.view.suffix= # Spring MVC view suffix. SPRING RESOURCES HANDLING (ResourceProperties) spring.resources.add-mappings=true # Enable default resource handling. spring.resources.cache-period= # Cache period for the resources served by the resource handler, in seconds. spring.resources.chain.cache=true # Enable caching in the Resource chain. spring.resources.chain.enabled= # Enable the Spring Resource Handling chain. Disabled by default unless at least one strategy has been enabled. spring.resources.chain.gzipped=false # Enable resolution of already gzipped resources. spring.resources.chain.html-application-cache=false # Enable HTML5 application cache manifest rewriting. spring.resources.chain.strategy.content.enabled=false # Enable the content Version Strategy. spring.resources.chain.strategy.content.paths=/** # Comma-separated list of patterns to apply to the Version Strategy. spring.resources.chain.strategy.fixed.enabled=false # Enable the fixed Version Strategy. spring.resources.chain.strategy.fixed.paths=/** # Comma-separated list of patterns to apply to the Version Strategy. spring.resources.chain.strategy.fixed.version= # Version string to use for the Version Strategy. spring.resources.static-locations=classpath:/META-INF/resources/,classpath:/resources/,classpath:/static/,classpath:/public/ # Locations of static resources. SPRING SESSION (SessionProperties) spring.session.hazelcast.flush-mode=on-save # Sessions flush mode. spring.session.hazelcast.map-name=spring:session:sessions # Name of the map used to store sessions. spring.session.jdbc.initializer.enabled= # Create the required session tables on startup if necessary. Enabled automatically if the default table name is set or a custom schema is configured. spring.session.jdbc.schema=classpath:org/springframework/session/jdbc/schema-@@platform@@.sql # Path to the SQL file to use to initialize the database schema. spring.session.jdbc.table-name=SPRING_SESSION # Name of database table used to store sessions. spring.session.redis.flush-mode=on-save # Sessions flush mode. spring.session.redis.namespace= # Namespace for keys used to store sessions. spring.session.store-type= # Session store type. SPRING SOCIAL (SocialWebAutoConfiguration) spring.social.auto-connection-views=false # Enable the connection status view for supported providers. SPRING SOCIAL FACEBOOK (FacebookAutoConfiguration) spring.social.facebook.app-id= # your application’s Facebook App ID spring.social.facebook.app-secret= # your application’s Facebook App Secret SPRING SOCIAL LINKEDIN (LinkedInAutoConfiguration) spring.social.linkedin.app-id= # your application’s LinkedIn App ID spring.social.linkedin.app-secret= # your application’s LinkedIn App Secret SPRING SOCIAL TWITTER (TwitterAutoConfiguration) spring.social.twitter.app-id= # your application’s Twitter App ID spring.social.twitter.app-secret= # your application’s Twitter App Secret THYMELEAF (ThymeleafAutoConfiguration) spring.thymeleaf.cache=true # Enable template caching. spring.thymeleaf.check-template=true # Check that the template exists before rendering it. spring.thymeleaf.check-template-location=true # Check that the templates location exists. spring.thymeleaf.enabled=true # Enable Thymeleaf view resolution for Web frameworks. spring.thymeleaf.encoding=UTF-8 # Template files encoding. spring.thymeleaf.excluded-view-names= # Comma-separated list of view names that should be excluded from resolution. spring.thymeleaf.mode=HTML5 # Template mode to be applied to templates. See also StandardTemplateModeHandlers. spring.thymeleaf.prefix=classpath:/templates/ # Prefix that gets prepended to view names when building a URL. spring.thymeleaf.reactive.max-chunk-size= # Maximum size of data buffers used for writing to the response, in bytes. spring.thymeleaf.reactive.media-types= # Media types supported by the view technology. spring.thymeleaf.servlet.content-type=text/html # Content-Type value written to HTTP responses. spring.thymeleaf.suffix=.html # Suffix that gets appended to view names when building a URL. spring.thymeleaf.template-resolver-order= # Order of the template resolver in the chain. spring.thymeleaf.view-names= # Comma-separated list of view names that can be resolved. SPRING WEB FLUX (WebFluxProperties) spring.webflux.static-path-pattern=/** # Path pattern used for static resources. SPRING WEB SERVICES (WebServicesProperties) spring.webservices.path=/services # Path that serves as the base URI for the services. spring.webservices.servlet.init= # Servlet init parameters to pass to Spring Web Services. spring.webservices.servlet.load-on-startup=-1 # Load on startup priority of the Spring Web Services servlet. SECURITY (SecurityProperties) security.basic.authorize-mode=role # Security authorize mode to apply. security.basic.enabled=true # Enable basic authentication. security.basic.path=/** # Comma-separated list of paths to secure. security.basic.realm=Spring # HTTP basic realm name. security.enable-csrf=false # Enable Cross Site Request Forgery support. security.filter-order=0 # Security filter chain order. security.filter-dispatcher-types=ASYNC, FORWARD, INCLUDE, REQUEST # Security filter chain dispatcher types. security.headers.cache=true # Enable cache control HTTP headers. security.headers.content-security-policy= # Value for content security policy header. security.headers.content-security-policy-mode=default # Content security policy mode. security.headers.content-type=true # Enable “X-Content-Type-Options” header. security.headers.frame=true # Enable “X-Frame-Options” header. security.headers.hsts=all # HTTP Strict Transport Security (HSTS) mode (none, domain, all). security.headers.xss=true # Enable cross site scripting (XSS) protection. security.ignored= # Comma-separated list of paths to exclude from the default secured paths. security.require-ssl=false # Enable secure channel for all requests. security.sessions=stateless # Session creation policy (always, never, if_required, stateless). security.user.name=user # Default user name. security.user.password= # Password for the default user name. A random password is logged on startup by default. security.user.role=USER # Granted roles for the default user name. SECURITY OAUTH2 CLIENT (OAuth2ClientProperties) security.oauth2.client.client-id= # OAuth2 client id. security.oauth2.client.client-secret= # OAuth2 client secret. A random secret is generated by default SECURITY OAUTH2 RESOURCES (ResourceServerProperties) security.oauth2.resource.filter-order= # The order of the filter chain used to authenticate tokens. security.oauth2.resource.id= # Identifier of the resource. security.oauth2.resource.jwt.key-uri= # The URI of the JWT token. Can be set if the value is not available and the key is public. security.oauth2.resource.jwt.key-value= # The verification key of the JWT token. Can either be a symmetric secret or PEM-encoded RSA public key. security.oauth2.resource.prefer-token-info=true # Use the token info, can be set to false to use the user info. security.oauth2.resource.service-id=resource # security.oauth2.resource.token-info-uri= # URI of the token decoding endpoint. security.oauth2.resource.token-type= # The token type to send when using the userInfoUri. security.oauth2.resource.user-info-uri= # URI of the user endpoint. SECURITY OAUTH2 SSO (OAuth2SsoProperties) security.oauth2.sso.filter-order= # Filter order to apply if not providing an explicit WebSecurityConfigurerAdapter security.oauth2.sso.login-path=/login # Path to the login page, i.e. the one that triggers the redirect to the OAuth2 Authorization Server FLYWAY (FlywayProperties) flyway.allow-mixed-migrations= # flyway.baseline-description= # flyway.baseline-on-migrate= # flyway.baseline-version=1 # version to start migration flyway.check-location=false # Check that migration scripts location exists. flyway.clean-disabled= # flyway.clean-on-validation-error= # flyway.enabled=true # Enable flyway. flyway.encoding= # flyway.ignore-failed-future-migration= # flyway.ignore-future-migrations= # flyway.ignore-missing-migrations= # flyway.init-sqls= # SQL statements to execute to initialize a connection immediately after obtaining it. flyway.installed-by= # flyway.locations=classpath:db/migration # locations of migrations scripts flyway.out-of-order= # flyway.password= # JDBC password if you want Flyway to create its own DataSource flyway.placeholder-prefix= # flyway.placeholder-replacement= # flyway.placeholder-suffix= # flyway.placeholders.*= # flyway.repeatable-sql-migration-prefix= # flyway.schemas= # schemas to update flyway.skip-default-callbacks= # flyway.skip-default-resolvers= # flyway.sql-migration-prefix=V # flyway.sql-migration-separator= # flyway.sql-migration-suffix=.sql # flyway.table= # flyway.target= # flyway.url= # JDBC url of the database to migrate. If not set, the primary configured data source is used. flyway.user= # Login user of the database to migrate. flyway.validate-on-migrate= # LIQUIBASE (LiquibaseProperties) liquibase.change-log=classpath:/db/changelog/db.changelog-master.yaml # Change log configuration path. liquibase.check-change-log-location=true # Check the change log location exists. liquibase.contexts= # Comma-separated list of runtime contexts to use. liquibase.default-schema= # Default database schema. liquibase.drop-first=false # Drop the database schema first. liquibase.enabled=true # Enable liquibase support. liquibase.labels= # Comma-separated list of runtime labels to use. liquibase.parameters.*= # Change log parameters. liquibase.password= # Login password of the database to migrate. liquibase.rollback-file= # File to which rollback SQL will be written when an update is performed. liquibase.url= # JDBC url of the database to migrate. If not set, the primary configured data source is used. liquibase.user= # Login user of the database to migrate. COUCHBASE (CouchbaseProperties) spring.couchbase.bootstrap-hosts= # Couchbase nodes (host or IP address) to bootstrap from. spring.couchbase.bucket.name=default # Name of the bucket to connect to. spring.couchbase.bucket.password= # Password of the bucket. spring.couchbase.env.endpoints.key-value=1 # Number of sockets per node against the Key/value service. spring.couchbase.env.endpoints.query=1 # Number of sockets per node against the Query (N1QL) service. spring.couchbase.env.endpoints.view=1 # Number of sockets per node against the view service. spring.couchbase.env.ssl.enabled= # Enable SSL support. Enabled automatically if a “keyStore” is provided unless specified otherwise. spring.couchbase.env.ssl.key-store= # Path to the JVM key store that holds the certificates. spring.couchbase.env.ssl.key-store-password= # Password used to access the key store. spring.couchbase.env.timeouts.connect=5000 # Bucket connections timeout in milliseconds. spring.couchbase.env.timeouts.key-value=2500 # Blocking operations performed on a specific key timeout in milliseconds. spring.couchbase.env.timeouts.query=7500 # N1QL query operations timeout in milliseconds. spring.couchbase.env.timeouts.socket-connect=1000 # Socket connect connections timeout in milliseconds. spring.couchbase.env.timeouts.view=7500 # Regular and geospatial view operations timeout in milliseconds. DAO (PersistenceExceptionTranslationAutoConfiguration) spring.dao.exceptiontranslation.enabled=true # Enable the PersistenceExceptionTranslationPostProcessor. CASSANDRA (CassandraProperties) spring.data.cassandra.cluster-name= # Name of the Cassandra cluster. spring.data.cassandra.compression=none # Compression supported by the Cassandra binary protocol. spring.data.cassandra.connect-timeout-millis= # Socket option: connection time out. spring.data.cassandra.consistency-level= # Queries consistency level. spring.data.cassandra.contact-points=localhost # Comma-separated list of cluster node addresses. spring.data.cassandra.fetch-size= # Queries default fetch size. spring.data.cassandra.keyspace-name= # Keyspace name to use. spring.data.cassandra.load-balancing-policy= # Class name of the load balancing policy. spring.data.cassandra.port= # Port of the Cassandra server. spring.data.cassandra.password= # Login password of the server. spring.data.cassandra.reactive-repositories.enabled=true # Enable Cassandra reactive repositories. spring.data.cassandra.read-timeout-millis= # Socket option: read time out. spring.data.cassandra.reconnection-policy= # Reconnection policy class. spring.data.cassandra.repositories.enabled= # Enable Cassandra repositories. spring.data.cassandra.retry-policy= # Class name of the retry policy. spring.data.cassandra.serial-consistency-level= # Queries serial consistency level. spring.data.cassandra.schema-action=none # Schema action to take at startup. spring.data.cassandra.ssl=false # Enable SSL support. spring.data.cassandra.username= # Login user of the server. DATA COUCHBASE (CouchbaseDataProperties) spring.data.couchbase.auto-index=false # Automatically create views and indexes. spring.data.couchbase.consistency=read-your-own-writes # Consistency to apply by default on generated queries. spring.data.couchbase.repositories.enabled=true # Enable Couchbase repositories. ELASTICSEARCH (ElasticsearchProperties) spring.data.elasticsearch.cluster-name=elasticsearch # Elasticsearch cluster name. spring.data.elasticsearch.cluster-nodes= # Comma-separated list of cluster node addresses. If not specified, starts a client node. spring.data.elasticsearch.properties.*= # Additional properties used to configure the client. spring.data.elasticsearch.repositories.enabled=true # Enable Elasticsearch repositories. DATA LDAP spring.data.ldap.repositories.enabled=true # Enable LDAP repositories. MONGODB (MongoProperties) spring.data.mongodb.authentication-database= # Authentication database name. spring.data.mongodb.database=test # Database name. spring.data.mongodb.field-naming-strategy= # Fully qualified name of the FieldNamingStrategy to use. spring.data.mongodb.grid-fs-database= # GridFS database name. spring.data.mongodb.host=localhost # Mongo server host. Cannot be set with uri. spring.data.mongodb.password= # Login password of the mongo server. Cannot be set with uri. spring.data.mongodb.port=27017 # Mongo server port. Cannot be set with uri. spring.data.mongodb.reactive-repositories.enabled=true # Enable Mongo reactive repositories. spring.data.mongodb.repositories.enabled=true # Enable Mongo repositories. spring.data.mongodb.uri=mongodb://localhost/test # Mongo database URI. Cannot be set with host, port and credentials. spring.data.mongodb.username= # Login user of the mongo server. Cannot be set with uri. DATA REDIS spring.data.redis.repositories.enabled=true # Enable Redis repositories. NEO4J (Neo4jProperties) spring.data.neo4j.auto-index=none # Auto index mode. spring.data.neo4j.embedded.enabled=true # Enable embedded mode if the embedded driver is available. spring.data.neo4j.open-in-view=false # Register OpenSessionInViewInterceptor. Binds a Neo4j Session to the thread for the entire processing of the request. spring.data.neo4j.password= # Login password of the server. spring.data.neo4j.repositories.enabled=true # Enable Neo4j repositories. spring.data.neo4j.uri= # URI used by the driver. Auto-detected by default. spring.data.neo4j.username= # Login user of the server. DATA REST (RepositoryRestProperties) spring.data.rest.base-path= # Base path to be used by Spring Data REST to expose repository resources. spring.data.rest.default-page-size= # Default size of pages. spring.data.rest.detection-strategy=default # Strategy to use to determine which repositories get exposed. spring.data.rest.enable-enum-translation= # Enable enum value translation via the Spring Data REST default resource bundle. spring.data.rest.limit-param-name= # Name of the URL query string parameter that indicates how many results to return at once. spring.data.rest.max-page-size= # Maximum size of pages. spring.data.rest.page-param-name= # Name of the URL query string parameter that indicates what page to return. spring.data.rest.return-body-on-create= # Return a response body after creating an entity. spring.data.rest.return-body-on-update= # Return a response body after updating an entity. spring.data.rest.sort-param-name= # Name of the URL query string parameter that indicates what direction to sort results. SOLR (SolrProperties) spring.data.solr.host=http://127.0.0.1:8983/solr # Solr host. Ignored if “zk-host” is set. spring.data.solr.repositories.enabled=true # Enable Solr repositories. spring.data.solr.zk-host= # ZooKeeper host address in the form HOST:PORT. DATASOURCE (DataSourceAutoConfiguration &amp; DataSourceProperties) spring.datasource.continue-on-error=false # Do not stop if an error occurs while initializing the database. spring.datasource.data= # Data (DML) script resource references. spring.datasource.data-username= # User of the database to execute DML scripts (if different). spring.datasource.data-password= # Password of the database to execute DML scripts (if different). spring.datasource.dbcp2.= # Commons DBCP2 specific settings spring.datasource.driver-class-name= # Fully qualified name of the JDBC driver. Auto-detected based on the URL by default. spring.datasource.generate-unique-name=false # Generate a random datasource name. spring.datasource.hikari.= # Hikari specific settings spring.datasource.initialize=true # Populate the database using ‘data.sql’. spring.datasource.jmx-enabled=false # Enable JMX support (if provided by the underlying pool). spring.datasource.jndi-name= # JNDI location of the datasource. Class, url, username &amp; password are ignored when set. spring.datasource.name=testdb # Name of the datasource. spring.datasource.password= # Login password of the database. spring.datasource.platform=all # Platform to use in the schema resource (schema-${platform}.sql). spring.datasource.schema= # Schema (DDL) script resource references. spring.datasource.schema-username= # User of the database to execute DDL scripts (if different). spring.datasource.schema-password= # Password of the database to execute DDL scripts (if different). spring.datasource.separator=; # Statement separator in SQL initialization scripts. spring.datasource.sql-script-encoding= # SQL scripts encoding. spring.datasource.tomcat.*= # Tomcat datasource specific settings spring.datasource.type= # Fully qualified name of the connection pool implementation to use. By default, it is auto-detected from the classpath. spring.datasource.url= # JDBC url of the database. spring.datasource.username= # Login user of the database. spring.datasource.xa.data-source-class-name= # XA datasource fully qualified name. spring.datasource.xa.properties= # Properties to pass to the XA data source. JEST (Elasticsearch HTTP client) (JestProperties) spring.elasticsearch.jest.connection-timeout=3000 # Connection timeout in milliseconds. spring.elasticsearch.jest.multi-threaded=true # Enable connection requests from multiple execution threads. spring.elasticsearch.jest.password= # Login password. spring.elasticsearch.jest.proxy.host= # Proxy host the HTTP client should use. spring.elasticsearch.jest.proxy.port= # Proxy port the HTTP client should use. spring.elasticsearch.jest.read-timeout=3000 # Read timeout in milliseconds. spring.elasticsearch.jest.uris=http://localhost:9200 # Comma-separated list of the Elasticsearch instances to use. spring.elasticsearch.jest.username= # Login user. H2 Web Console (H2ConsoleProperties) spring.h2.console.enabled=false # Enable the console. spring.h2.console.path=/h2-console # Path at which the console will be available. spring.h2.console.settings.trace=false # Enable trace output. spring.h2.console.settings.web-allow-others=false # Enable remote access. JOOQ (JooqAutoConfiguration) spring.jooq.sql-dialect= # SQLDialect JOOQ used when communicating with the configured datasource. For instance POSTGRES JPA (JpaBaseConfiguration, HibernateJpaAutoConfiguration) spring.data.jpa.repositories.enabled=true # Enable JPA repositories. spring.jpa.database= # Target database to operate on, auto-detected by default. Can be alternatively set using the “databasePlatform” property. spring.jpa.database-platform= # Name of the target database to operate on, auto-detected by default. Can be alternatively set using the “Database” enum. spring.jpa.generate-ddl=false # Initialize the schema on startup. spring.jpa.hibernate.ddl-auto= # DDL mode. This is actually a shortcut for the “hibernate.hbm2ddl.auto” property. Default to “create-drop” when using an embedded database, “none” otherwise. spring.jpa.hibernate.naming.implicit-strategy= # Hibernate 5 implicit naming strategy fully qualified name. spring.jpa.hibernate.naming.physical-strategy= # Hibernate 5 physical naming strategy fully qualified name. spring.jpa.hibernate.use-new-id-generator-mappings= # Use Hibernate’s newer IdentifierGenerator for AUTO, TABLE and SEQUENCE. spring.jpa.open-in-view=true # Register OpenEntityManagerInViewInterceptor. Binds a JPA EntityManager to the thread for the entire processing of the request. spring.jpa.properties.*= # Additional native properties to set on the JPA provider. spring.jpa.show-sql=false # Enable logging of SQL statements. JTA (JtaAutoConfiguration) spring.jta.enabled=true # Enable JTA support. spring.jta.log-dir= # Transaction logs directory. spring.jta.transaction-manager-id= # Transaction manager unique identifier. ATOMIKOS (AtomikosProperties) spring.jta.atomikos.connectionfactory.borrow-connection-timeout=30 # Timeout, in seconds, for borrowing connections from the pool. spring.jta.atomikos.connectionfactory.ignore-session-transacted-flag=true # Whether or not to ignore the transacted flag when creating session. spring.jta.atomikos.connectionfactory.local-transaction-mode=false # Whether or not local transactions are desired. spring.jta.atomikos.connectionfactory.maintenance-interval=60 # The time, in seconds, between runs of the pool’s maintenance thread. spring.jta.atomikos.connectionfactory.max-idle-time=60 # The time, in seconds, after which connections are cleaned up from the pool. spring.jta.atomikos.connectionfactory.max-lifetime=0 # The time, in seconds, that a connection can be pooled for before being destroyed. 0 denotes no limit. spring.jta.atomikos.connectionfactory.max-pool-size=1 # The maximum size of the pool. spring.jta.atomikos.connectionfactory.min-pool-size=1 # The minimum size of the pool. spring.jta.atomikos.connectionfactory.reap-timeout=0 # The reap timeout, in seconds, for borrowed connections. 0 denotes no limit. spring.jta.atomikos.connectionfactory.unique-resource-name=jmsConnectionFactory # The unique name used to identify the resource during recovery. spring.jta.atomikos.datasource.borrow-connection-timeout=30 # Timeout, in seconds, for borrowing connections from the pool. spring.jta.atomikos.datasource.default-isolation-level= # Default isolation level of connections provided by the pool. spring.jta.atomikos.datasource.login-timeout= # Timeout, in seconds, for establishing a database connection. spring.jta.atomikos.datasource.maintenance-interval=60 # The time, in seconds, between runs of the pool’s maintenance thread. spring.jta.atomikos.datasource.max-idle-time=60 # The time, in seconds, after which connections are cleaned up from the pool. spring.jta.atomikos.datasource.max-lifetime=0 # The time, in seconds, that a connection can be pooled for before being destroyed. 0 denotes no limit. spring.jta.atomikos.datasource.max-pool-size=1 # The maximum size of the pool. spring.jta.atomikos.datasource.min-pool-size=1 # The minimum size of the pool. spring.jta.atomikos.datasource.reap-timeout=0 # The reap timeout, in seconds, for borrowed connections. 0 denotes no limit. spring.jta.atomikos.datasource.test-query= # SQL query or statement used to validate a connection before returning it. spring.jta.atomikos.datasource.unique-resource-name=dataSource # The unique name used to identify the resource during recovery. spring.jta.atomikos.properties.checkpoint-interval=500 # Interval between checkpoints. spring.jta.atomikos.properties.console-file-count=1 # Number of debug logs files that can be created. spring.jta.atomikos.properties.console-file-limit=-1 # How many bytes can be stored at most in debug logs files. spring.jta.atomikos.properties.console-file-name=tm.out # Debug logs file name. spring.jta.atomikos.properties.console-log-level=warn # Console log level. spring.jta.atomikos.properties.default-jta-timeout=10000 # Default timeout for JTA transactions. spring.jta.atomikos.properties.enable-logging=true # Enable disk logging. spring.jta.atomikos.properties.force-shutdown-on-vm-exit=false # Specify if a VM shutdown should trigger forced shutdown of the transaction core. spring.jta.atomikos.properties.log-base-dir= # Directory in which the log files should be stored. spring.jta.atomikos.properties.log-base-name=tmlog # Transactions log file base name. spring.jta.atomikos.properties.max-actives=50 # Maximum number of active transactions. spring.jta.atomikos.properties.max-timeout=300000 # Maximum timeout (in milliseconds) that can be allowed for transactions. spring.jta.atomikos.properties.output-dir= # Directory in which to store the debug log files. spring.jta.atomikos.properties.serial-jta-transactions=true # Specify if sub-transactions should be joined when possible. spring.jta.atomikos.properties.service= # Transaction manager implementation that should be started. spring.jta.atomikos.properties.threaded-two-phase-commit=false # Use different (and concurrent) threads for two-phase commit on the participating resources. spring.jta.atomikos.properties.transaction-manager-unique-name= # Transaction manager’s unique name. BITRONIX spring.jta.bitronix.connectionfactory.acquire-increment=1 # Number of connections to create when growing the pool. spring.jta.bitronix.connectionfactory.acquisition-interval=1 # Time, in seconds, to wait before trying to acquire a connection again after an invalid connection was acquired. spring.jta.bitronix.connectionfactory.acquisition-timeout=30 # Timeout, in seconds, for acquiring connections from the pool. spring.jta.bitronix.connectionfactory.allow-local-transactions=true # Whether or not the transaction manager should allow mixing XA and non-XA transactions. spring.jta.bitronix.connectionfactory.apply-transaction-timeout=false # Whether or not the transaction timeout should be set on the XAResource when it is enlisted. spring.jta.bitronix.connectionfactory.automatic-enlisting-enabled=true # Whether or not resources should be enlisted and delisted automatically. spring.jta.bitronix.connectionfactory.cache-producers-consumers=true # Whether or not produces and consumers should be cached. spring.jta.bitronix.connectionfactory.defer-connection-release=true # Whether or not the provider can run many transactions on the same connection and supports transaction interleaving. spring.jta.bitronix.connectionfactory.ignore-recovery-failures=false # Whether or not recovery failures should be ignored. spring.jta.bitronix.connectionfactory.max-idle-time=60 # The time, in seconds, after which connections are cleaned up from the pool. spring.jta.bitronix.connectionfactory.max-pool-size=10 # The maximum size of the pool. 0 denotes no limit. spring.jta.bitronix.connectionfactory.min-pool-size=0 # The minimum size of the pool. spring.jta.bitronix.connectionfactory.password= # The password to use to connect to the JMS provider. spring.jta.bitronix.connectionfactory.share-transaction-connections=false # Whether or not connections in the ACCESSIBLE state can be shared within the context of a transaction. spring.jta.bitronix.connectionfactory.test-connections=true # Whether or not connections should be tested when acquired from the pool. spring.jta.bitronix.connectionfactory.two-pc-ordering-position=1 # The position that this resource should take during two-phase commit (always first is Integer.MIN_VALUE, always last is Integer.MAX_VALUE). spring.jta.bitronix.connectionfactory.unique-name=jmsConnectionFactory # The unique name used to identify the resource during recovery. spring.jta.bitronix.connectionfactory.use-tm-join=true Whether or not TMJOIN should be used when starting XAResources. spring.jta.bitronix.connectionfactory.user= # The user to use to connect to the JMS provider. spring.jta.bitronix.datasource.acquire-increment=1 # Number of connections to create when growing the pool. spring.jta.bitronix.datasource.acquisition-interval=1 # Time, in seconds, to wait before trying to acquire a connection again after an invalid connection was acquired. spring.jta.bitronix.datasource.acquisition-timeout=30 # Timeout, in seconds, for acquiring connections from the pool. spring.jta.bitronix.datasource.allow-local-transactions=true # Whether or not the transaction manager should allow mixing XA and non-XA transactions. spring.jta.bitronix.datasource.apply-transaction-timeout=false # Whether or not the transaction timeout should be set on the XAResource when it is enlisted. spring.jta.bitronix.datasource.automatic-enlisting-enabled=true # Whether or not resources should be enlisted and delisted automatically. spring.jta.bitronix.datasource.cursor-holdability= # The default cursor holdability for connections. spring.jta.bitronix.datasource.defer-connection-release=true # Whether or not the database can run many transactions on the same connection and supports transaction interleaving. spring.jta.bitronix.datasource.enable-jdbc4-connection-test= # Whether or not Connection.isValid() is called when acquiring a connection from the pool. spring.jta.bitronix.datasource.ignore-recovery-failures=false # Whether or not recovery failures should be ignored. spring.jta.bitronix.datasource.isolation-level= # The default isolation level for connections. spring.jta.bitronix.datasource.local-auto-commit= # The default auto-commit mode for local transactions. spring.jta.bitronix.datasource.login-timeout= # Timeout, in seconds, for establishing a database connection. spring.jta.bitronix.datasource.max-idle-time=60 # The time, in seconds, after which connections are cleaned up from the pool. spring.jta.bitronix.datasource.max-pool-size=10 # The maximum size of the pool. 0 denotes no limit. spring.jta.bitronix.datasource.min-pool-size=0 # The minimum size of the pool. spring.jta.bitronix.datasource.prepared-statement-cache-size=0 # The target size of the prepared statement cache. 0 disables the cache. spring.jta.bitronix.datasource.share-transaction-connections=false # Whether or not connections in the ACCESSIBLE state can be shared within the context of a transaction. spring.jta.bitronix.datasource.test-query= # SQL query or statement used to validate a connection before returning it. spring.jta.bitronix.datasource.two-pc-ordering-position=1 # The position that this resource should take during two-phase commit (always first is Integer.MIN_VALUE, always last is Integer.MAX_VALUE). spring.jta.bitronix.datasource.unique-name=dataSource # The unique name used to identify the resource during recovery. spring.jta.bitronix.datasource.use-tm-join=true Whether or not TMJOIN should be used when starting XAResources. spring.jta.bitronix.properties.allow-multiple-lrc=false # Allow multiple LRC resources to be enlisted into the same transaction. spring.jta.bitronix.properties.asynchronous2-pc=false # Enable asynchronously execution of two phase commit. spring.jta.bitronix.properties.background-recovery-interval-seconds=60 # Interval in seconds at which to run the recovery process in the background. spring.jta.bitronix.properties.current-node-only-recovery=true # Recover only the current node. spring.jta.bitronix.properties.debug-zero-resource-transaction=false # Log the creation and commit call stacks of transactions executed without a single enlisted resource. spring.jta.bitronix.properties.default-transaction-timeout=60 # Default transaction timeout in seconds. spring.jta.bitronix.properties.disable-jmx=false # Enable JMX support. spring.jta.bitronix.properties.exception-analyzer= # Set the fully qualified name of the exception analyzer implementation to use. spring.jta.bitronix.properties.filter-log-status=false # Enable filtering of logs so that only mandatory logs are written. spring.jta.bitronix.properties.force-batching-enabled=true # Set if disk forces are batched. spring.jta.bitronix.properties.forced-write-enabled=true # Set if logs are forced to disk. spring.jta.bitronix.properties.graceful-shutdown-interval=60 # Maximum amount of seconds the TM will wait for transactions to get done before aborting them at shutdown time. spring.jta.bitronix.properties.jndi-transaction-synchronization-registry-name= # JNDI name of the TransactionSynchronizationRegistry. spring.jta.bitronix.properties.jndi-user-transaction-name= # JNDI name of the UserTransaction. spring.jta.bitronix.properties.journal=disk # Name of the journal. Can be ‘disk’, ‘null’ or a class name. spring.jta.bitronix.properties.log-part1-filename=btm1.tlog # Name of the first fragment of the journal. spring.jta.bitronix.properties.log-part2-filename=btm2.tlog # Name of the second fragment of the journal. spring.jta.bitronix.properties.max-log-size-in-mb=2 # Maximum size in megabytes of the journal fragments. spring.jta.bitronix.properties.resource-configuration-filename= # ResourceLoader configuration file name. spring.jta.bitronix.properties.server-id= # ASCII ID that must uniquely identify this TM instance. Default to the machine’s IP address. spring.jta.bitronix.properties.skip-corrupted-logs=false # Skip corrupted transactions log entries. spring.jta.bitronix.properties.warn-about-zero-resource-transaction=true # Log a warning for transactions executed without a single enlisted resource. NARAYANA (NarayanaProperties) spring.jta.narayana.default-timeout=60 # Transaction timeout in seconds. spring.jta.narayana.expiry-scanners=com.arjuna.ats.internal.arjuna.recovery.ExpiredTransactionStatusManagerScanner # Comma-separated list of expiry scanners. spring.jta.narayana.log-dir= # Transaction object store directory. spring.jta.narayana.one-phase-commit=true # Enable one phase commit optimisation. spring.jta.narayana.periodic-recovery-period=120 # Interval in which periodic recovery scans are performed in seconds. spring.jta.narayana.recovery-backoff-period=10 # Back off period between first and second phases of the recovery scan in seconds. spring.jta.narayana.recovery-db-pass= # Database password to be used by recovery manager. spring.jta.narayana.recovery-db-user= # Database username to be used by recovery manager. spring.jta.narayana.recovery-jms-pass= # JMS password to be used by recovery manager. spring.jta.narayana.recovery-jms-user= # JMS username to be used by recovery manager. spring.jta.narayana.recovery-modules= # Comma-separated list of recovery modules. spring.jta.narayana.transaction-manager-id=1 # Unique transaction manager id. spring.jta.narayana.xa-resource-orphan-filters= # Comma-separated list of orphan filters. EMBEDDED MONGODB (EmbeddedMongoProperties) spring.mongodb.embedded.features=SYNC_DELAY # Comma-separated list of features to enable. spring.mongodb.embedded.storage.database-dir= # Directory used for data storage. spring.mongodb.embedded.storage.oplog-size= # Maximum size of the oplog in megabytes. spring.mongodb.embedded.storage.repl-set-name= # Name of the replica set. spring.mongodb.embedded.version=2.6.10 # Version of Mongo to use. REDIS (RedisProperties) spring.redis.cluster.max-redirects= # Maximum number of redirects to follow when executing commands across the cluster. spring.redis.cluster.nodes= # Comma-separated list of “host:port” pairs to bootstrap from. spring.redis.database=0 # Database index used by the connection factory. spring.redis.url= # Connection URL, will override host, port and password (user will be ignored), e.g. redis://user:password@example.com:6379 spring.redis.host=localhost # Redis server host. spring.redis.jedis.pool.max-active=8 # Max number of connections that can be allocated by the pool at a given time. Use a negative value for no limit. spring.redis.jedis.pool.max-idle=8 # Max number of “idle” connections in the pool. Use a negative value to indicate an unlimited number of idle connections. spring.redis.jedis.pool.max-wait=-1 # Maximum amount of time (in milliseconds) a connection allocation should block before throwing an exception when the pool is exhausted. Use a negative value to block indefinitely. spring.redis.jedis.pool.min-idle=0 # Target for the minimum number of idle connections to maintain in the pool. This setting only has an effect if it is positive. spring.redis.lettuce.pool.max-active=8 # Max number of connections that can be allocated by the pool at a given time. Use a negative value for no limit. spring.redis.lettuce.pool.max-idle=8 # Max number of “idle” connections in the pool. Use a negative value to indicate an unlimited number of idle connections. spring.redis.lettuce.pool.max-wait=-1 # Maximum amount of time (in milliseconds) a connection allocation should block before throwing an exception when the pool is exhausted. Use a negative value to block indefinitely. spring.redis.lettuce.pool.min-idle=0 # Target for the minimum number of idle connections to maintain in the pool. This setting only has an effect if it is positive. spring.redis.lettuce.shutdown-timeout=2000 # Shutdown timeout in milliseconds. spring.redis.password= # Login password of the redis server. spring.redis.port=6379 # Redis server port. spring.redis.sentinel.master= # Name of Redis server. spring.redis.sentinel.nodes= # Comma-separated list of host:port pairs. spring.redis.ssl=false # Enable SSL support. spring.redis.timeout=0 # Connection timeout in milliseconds. TRANSACTION (TransactionProperties) spring.transaction.default-timeout= # Default transaction timeout in seconds. spring.transaction.rollback-on-commit-failure= # Perform the rollback on commit failures. ACTIVEMQ (ActiveMQProperties) spring.activemq.broker-url= # URL of the ActiveMQ broker. Auto-generated by default. For instance tcp://localhost:61616 spring.activemq.in-memory=true # Specify if the default broker URL should be in memory. Ignored if an explicit broker has been specified. spring.activemq.password= # Login password of the broker. spring.activemq.user= # Login user of the broker. spring.activemq.packages.trust-all=false # Trust all packages. spring.activemq.packages.trusted= # Comma-separated list of specific packages to trust (when not trusting all packages). spring.activemq.pool.configuration.*= # See PooledConnectionFactory. spring.activemq.pool.enabled=false # Whether a PooledConnectionFactory should be created instead of a regular ConnectionFactory. spring.activemq.pool.expiry-timeout=0 # Connection expiration timeout in milliseconds. spring.activemq.pool.idle-timeout=30000 # Connection idle timeout in milliseconds. spring.activemq.pool.max-connections=1 # Maximum number of pooled connections. ARTEMIS (ArtemisProperties) spring.artemis.embedded.cluster-password= # Cluster password. Randomly generated on startup by default. spring.artemis.embedded.data-directory= # Journal file directory. Not necessary if persistence is turned off. spring.artemis.embedded.enabled=true # Enable embedded mode if the Artemis server APIs are available. spring.artemis.embedded.persistent=false # Enable persistent store. spring.artemis.embedded.queues= # Comma-separated list of queues to create on startup. spring.artemis.embedded.server-id= # Server id. By default, an auto-incremented counter is used. spring.artemis.embedded.topics= # Comma-separated list of topics to create on startup. spring.artemis.host=localhost # Artemis broker host. spring.artemis.mode= # Artemis deployment mode, auto-detected by default. spring.artemis.password= # Login password of the broker. spring.artemis.port=61616 # Artemis broker port. spring.artemis.user= # Login user of the broker. SPRING BATCH (BatchProperties) spring.batch.initializer.enabled= # Create the required batch tables on startup if necessary. Enabled automatically if no custom table prefix is set or if a custom schema is configured. spring.batch.job.enabled=true # Execute all Spring Batch jobs in the context on startup. spring.batch.job.names= # Comma-separated list of job names to execute on startup (For instance job1,job2). By default, all Jobs found in the context are executed. spring.batch.schema=classpath:org/springframework/batch/core/schema-@@platform@@.sql # Path to the SQL file to use to initialize the database schema. spring.batch.table-prefix= # Table prefix for all the batch meta-data tables. SPRING INTEGRATION (IntegrationProperties) spring.integration.jdbc.initializer.enabled=false # Create the required integration tables on startup. spring.integration.jdbc.schema=classpath:org/springframework/integration/jdbc/schema-@@platform@@.sql # Path to the SQL file to use to initialize the database schema. JMS (JmsProperties) spring.jms.jndi-name= # Connection factory JNDI name. When set, takes precedence to others connection factory auto-configurations. spring.jms.listener.acknowledge-mode= # Acknowledge mode of the container. By default, the listener is transacted with automatic acknowledgment. spring.jms.listener.auto-startup=true # Start the container automatically on startup. spring.jms.listener.concurrency= # Minimum number of concurrent consumers. spring.jms.listener.max-concurrency= # Maximum number of concurrent consumers. spring.jms.pub-sub-domain=false # Specify if the default destination type is topic. spring.jms.template.default-destination= # Default destination to use on send/receive operations that do not have a destination parameter. spring.jms.template.delivery-delay= # Delivery delay to use for send calls in milliseconds. spring.jms.template.delivery-mode= # Delivery mode. Enable QoS when set. spring.jms.template.priority= # Priority of a message when sending. Enable QoS when set. spring.jms.template.qos-enabled= # Enable explicit QoS when sending a message. spring.jms.template.receive-timeout= # Timeout to use for receive calls in milliseconds. spring.jms.template.time-to-live= # Time-to-live of a message when sending in milliseconds. Enable QoS when set. APACHE KAFKA (KafkaProperties) spring.kafka.bootstrap-servers= # Comma-delimited list of host:port pairs to use for establishing the initial connection to the Kafka cluster. spring.kafka.client-id= # Id to pass to the server when making requests; used for server-side logging. spring.kafka.consumer.auto-commit-interval= # Frequency in milliseconds that the consumer offsets are auto-committed to Kafka if ‘enable.auto.commit’ true. spring.kafka.consumer.auto-offset-reset= # What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server. spring.kafka.consumer.bootstrap-servers= # Comma-delimited list of host:port pairs to use for establishing the initial connection to the Kafka cluster. spring.kafka.consumer.client-id= # Id to pass to the server when making requests; used for server-side logging. spring.kafka.consumer.enable-auto-commit= # If true the consumer’s offset will be periodically committed in the background. spring.kafka.consumer.fetch-max-wait= # Maximum amount of time in milliseconds the server will block before answering the fetch request if there isn’t sufficient data to immediately satisfy the requirement given by “fetch.min.bytes”. spring.kafka.consumer.fetch-min-size= # Minimum amount of data the server should return for a fetch request in bytes. spring.kafka.consumer.group-id= # Unique string that identifies the consumer group this consumer belongs to. spring.kafka.consumer.heartbeat-interval= # Expected time in milliseconds between heartbeats to the consumer coordinator. spring.kafka.consumer.key-deserializer= # Deserializer class for keys. spring.kafka.consumer.max-poll-records= # Maximum number of records returned in a single call to poll(). spring.kafka.consumer.ssl.key-password= # Password of the private key in the key store file. spring.kafka.consumer.ssl.keystore-location= # Location of the key store file. spring.kafka.consumer.ssl.keystore-password= # Store password for the key store file. spring.kafka.consumer.ssl.truststore-location= # Location of the trust store file. spring.kafka.consumer.ssl.truststore-password= # Store password for the trust store file. spring.kafka.consumer.value-deserializer= # Deserializer class for values. spring.kafka.jaas.control-flag=required # Control flag for login configuration. spring.kafka.jaas.enabled= # Enable JAAS configuration. spring.kafka.jaas.login-module=com.sun.security.auth.module.Krb5LoginModule # Login module. spring.kafka.jaas.options= # Additional JAAS options. spring.kafka.listener.ack-count= # Number of records between offset commits when ackMode is “COUNT” or “COUNT_TIME”. spring.kafka.listener.ack-mode= # Listener AckMode; see the spring-kafka documentation. spring.kafka.listener.ack-time= # Time in milliseconds between offset commits when ackMode is “TIME” or “COUNT_TIME”. spring.kafka.listener.concurrency= # Number of threads to run in the listener containers. spring.kafka.listener.poll-timeout= # Timeout in milliseconds to use when polling the consumer. spring.kafka.producer.acks= # Number of acknowledgments the producer requires the leader to have received before considering a request complete. spring.kafka.producer.batch-size= # Number of records to batch before sending. spring.kafka.producer.bootstrap-servers= # Comma-delimited list of host:port pairs to use for establishing the initial connection to the Kafka cluster. spring.kafka.producer.buffer-memory= # Total bytes of memory the producer can use to buffer records waiting to be sent to the server. spring.kafka.producer.client-id= # Id to pass to the server when making requests; used for server-side logging. spring.kafka.producer.compression-type= # Compression type for all data generated by the producer. spring.kafka.producer.key-serializer= # Serializer class for keys. spring.kafka.producer.retries= # When greater than zero, enables retrying of failed sends. spring.kafka.producer.ssl.key-password= # Password of the private key in the key store file. spring.kafka.producer.ssl.keystore-location= # Location of the key store file. spring.kafka.producer.ssl.keystore-password= # Store password for the key store file. spring.kafka.producer.ssl.truststore-location= # Location of the trust store file. spring.kafka.producer.ssl.truststore-password= # Store password for the trust store file. spring.kafka.producer.value-serializer= # Serializer class for values. spring.kafka.properties.*= # Additional properties used to configure the client. spring.kafka.ssl.key-password= # Password of the private key in the key store file. spring.kafka.ssl.keystore-location= # Location of the key store file. spring.kafka.ssl.keystore-password= # Store password for the key store file. spring.kafka.ssl.truststore-location= # Location of the trust store file. spring.kafka.ssl.truststore-password= # Store password for the trust store file. spring.kafka.template.default-topic= # Default topic to which messages will be sent. RABBIT (RabbitProperties) spring.rabbitmq.addresses= # Comma-separated list of addresses to which the client should connect. spring.rabbitmq.cache.channel.checkout-timeout= # Number of milliseconds to wait to obtain a channel if the cache size has been reached. spring.rabbitmq.cache.channel.size= # Number of channels to retain in the cache. spring.rabbitmq.cache.connection.mode=channel # Connection factory cache mode. spring.rabbitmq.cache.connection.size= # Number of connections to cache. spring.rabbitmq.connection-timeout= # Connection timeout, in milliseconds; zero for infinite. spring.rabbitmq.dynamic=true # Create an AmqpAdmin bean. spring.rabbitmq.host=localhost # RabbitMQ host. spring.rabbitmq.listener.direct.acknowledge-mode= # Acknowledge mode of container. spring.rabbitmq.listener.direct.auto-startup=true # Start the container automatically on startup. spring.rabbitmq.listener.direct.consumers-per-queue= # Number of consumers per queue. spring.rabbitmq.listener.direct.default-requeue-rejected= # Whether rejected deliveries are requeued by default; default true. spring.rabbitmq.listener.direct.idle-event-interval= # How often idle container events should be published in milliseconds. spring.rabbitmq.listener.direct.prefetch= # Number of messages to be handled in a single request. It should be greater than or equal to the transaction size (if used). spring.rabbitmq.listener.simple.acknowledge-mode= # Acknowledge mode of container. spring.rabbitmq.listener.simple.auto-startup=true # Start the container automatically on startup. spring.rabbitmq.listener.simple.concurrency= # Minimum number of listener invoker threads. spring.rabbitmq.listener.simple.default-requeue-rejected= # Whether or not to requeue delivery failures. spring.rabbitmq.listener.simple.idle-event-interval= # How often idle container events should be published in milliseconds. spring.rabbitmq.listener.simple.max-concurrency= # Maximum number of listener invoker. spring.rabbitmq.listener.simple.prefetch= # Number of messages to be handled in a single request. It should be greater than or equal to the transaction size (if used). spring.rabbitmq.listener.simple.retry.enabled=false # Whether or not publishing retries are enabled. spring.rabbitmq.listener.simple.retry.initial-interval=1000 # Interval between the first and second attempt to deliver a message. spring.rabbitmq.listener.simple.retry.max-attempts=3 # Maximum number of attempts to deliver a message. spring.rabbitmq.listener.simple.retry.max-interval=10000 # Maximum interval between attempts. spring.rabbitmq.listener.simple.retry.multiplier=1.0 # A multiplier to apply to the previous delivery retry interval. spring.rabbitmq.listener.simple.retry.stateless=true # Whether or not retry is stateless or stateful. spring.rabbitmq.listener.simple.transaction-size= # Number of messages to be processed in a transaction; number of messages between acks. For best results it should be less than or equal to the prefetch count. spring.rabbitmq.listener.type=simple # Listener container type. spring.rabbitmq.password= # Login to authenticate against the broker. spring.rabbitmq.port=5672 # RabbitMQ port. spring.rabbitmq.publisher-confirms=false # Enable publisher confirms. spring.rabbitmq.publisher-returns=false # Enable publisher returns. spring.rabbitmq.requested-heartbeat= # Requested heartbeat timeout, in seconds; zero for none. spring.rabbitmq.ssl.enabled=false # Enable SSL support. spring.rabbitmq.ssl.key-store= # Path to the key store that holds the SSL certificate. spring.rabbitmq.ssl.key-store-password= # Password used to access the key store. spring.rabbitmq.ssl.trust-store= # Trust store that holds SSL certificates. spring.rabbitmq.ssl.trust-store-password= # Password used to access the trust store. spring.rabbitmq.ssl.algorithm= # SSL algorithm to use. By default configure by the rabbit client library. spring.rabbitmq.template.mandatory=false # Enable mandatory messages. spring.rabbitmq.template.receive-timeout=0 # Timeout for receive() methods. spring.rabbitmq.template.reply-timeout=5000 # Timeout for sendAndReceive() methods. spring.rabbitmq.template.retry.enabled=false # Set to true to enable retries in the RabbitTemplate. spring.rabbitmq.template.retry.initial-interval=1000 # Interval between the first and second attempt to publish a message. spring.rabbitmq.template.retry.max-attempts=3 # Maximum number of attempts to publish a message. spring.rabbitmq.template.retry.max-interval=10000 # Maximum number of attempts to publish a message. spring.rabbitmq.template.retry.multiplier=1.0 # A multiplier to apply to the previous publishing retry interval. spring.rabbitmq.username= # Login user to authenticate to the broker. spring.rabbitmq.virtual-host= # Virtual host to use when connecting to the broker. ENDPOINTS (AbstractEndpoint subclasses) endpoints.enabled=true # Enable endpoints. endpoints.sensitive= # Default endpoint sensitive setting. endpoints.actuator.enabled=true # Enable the endpoint. endpoints.actuator.path= # Endpoint URL path. endpoints.actuator.sensitive=false # Enable security on the endpoint. endpoints.auditevents.enabled= # Enable the endpoint. endpoints.auditevents.path= # Endpoint path. endpoints.auditevents.sensitive=false # Enable security on the endpoint. endpoints.autoconfig.enabled= # Enable the endpoint. endpoints.autoconfig.id= # Endpoint identifier. endpoints.autoconfig.path= # Endpoint path. endpoints.autoconfig.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.beans.enabled= # Enable the endpoint. endpoints.beans.id= # Endpoint identifier. endpoints.beans.path= # Endpoint path. endpoints.beans.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.configprops.enabled= # Enable the endpoint. endpoints.configprops.id= # Endpoint identifier. endpoints.configprops.keys-to-sanitize=password,secret,key,token,.credentials.,vcap_services # Keys that should be sanitized. Keys can be simple strings that the property ends with or regex expressions. endpoints.configprops.path= # Endpoint path. endpoints.configprops.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.docs.curies.enabled=false # Enable the curie generation. endpoints.docs.enabled=true # Enable actuator docs endpoint. endpoints.docs.path=/docs # endpoints.docs.sensitive=false # endpoints.dump.enabled= # Enable the endpoint. endpoints.dump.id= # Endpoint identifier. endpoints.dump.path= # Endpoint path. endpoints.dump.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.env.enabled= # Enable the endpoint. endpoints.env.id= # Endpoint identifier. endpoints.env.keys-to-sanitize=password,secret,key,token,.credentials.,vcap_services # Keys that should be sanitized. Keys can be simple strings that the property ends with or regex expressions. endpoints.env.path= # Endpoint path. endpoints.env.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.flyway.enabled= # Enable the endpoint. endpoints.flyway.id= # Endpoint identifier. endpoints.flyway.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.health.enabled= # Enable the endpoint. endpoints.health.id= # Endpoint identifier. endpoints.health.mapping.*= # Mapping of health statuses to HttpStatus codes. By default, registered health statuses map to sensible defaults (i.e. UP maps to 200). endpoints.health.path= # Endpoint path. endpoints.health.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.health.time-to-live=1000 # Time to live for cached result, in milliseconds. endpoints.heapdump.enabled= # Enable the endpoint. endpoints.heapdump.path= # Endpoint path. endpoints.heapdump.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.hypermedia.enabled=false # Enable hypermedia support for endpoints. endpoints.info.enabled= # Enable the endpoint. endpoints.info.id= # Endpoint identifier. endpoints.info.path= # Endpoint path. endpoints.info.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.jolokia.enabled=true # Enable Jolokia endpoint. endpoints.jolokia.path=/jolokia # Endpoint URL path. endpoints.jolokia.sensitive=true # Enable security on the endpoint. endpoints.liquibase.enabled= # Enable the endpoint. endpoints.liquibase.id= # Endpoint identifier. endpoints.liquibase.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.logfile.enabled=true # Enable the endpoint. endpoints.logfile.external-file= # External Logfile to be accessed. endpoints.logfile.path=/logfile # Endpoint URL path. endpoints.logfile.sensitive=true # Enable security on the endpoint. endpoints.loggers.enabled=true # Enable the endpoint. endpoints.loggers.id= # Endpoint identifier. endpoints.loggers.path=/logfile # Endpoint path. endpoints.loggers.sensitive=true # Mark if the endpoint exposes sensitive information. endpoints.mappings.enabled= # Enable the endpoint. endpoints.mappings.id= # Endpoint identifier. endpoints.mappings.path= # Endpoint path. endpoints.mappings.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.metrics.enabled= # Enable the endpoint. endpoints.metrics.filter.enabled=true # Enable the metrics servlet filter. endpoints.metrics.filter.gauge-submissions=merged # Http filter gauge submissions (merged, per-http-method) endpoints.metrics.filter.counter-submissions=merged # Http filter counter submissions (merged, per-http-method) endpoints.metrics.id= # Endpoint identifier. endpoints.metrics.path= # Endpoint path. endpoints.metrics.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.shutdown.enabled= # Enable the endpoint. endpoints.shutdown.id= # Endpoint identifier. endpoints.shutdown.path= # Endpoint path. endpoints.shutdown.sensitive= # Mark if the endpoint exposes sensitive information. endpoints.trace.enabled= # Enable the endpoint. endpoints.trace.filter.enabled=true # Enable the trace servlet filter. endpoints.trace.id= # Endpoint identifier. endpoints.trace.path= # Endpoint path. endpoints.trace.sensitive= # Mark if the endpoint exposes sensitive information. ENDPOINTS CORS CONFIGURATION (EndpointCorsProperties) endpoints.cors.allow-credentials= # Set whether credentials are supported. When not set, credentials are not supported. endpoints.cors.allowed-headers= # Comma-separated list of headers to allow in a request. ‘’ allows all headers. endpoints.cors.allowed-methods=GET # Comma-separated list of methods to allow. '’ allows all methods. endpoints.cors.allowed-origins= # Comma-separated list of origins to allow. ‘*’ allows all origins. When not set, CORS support is disabled. endpoints.cors.exposed-headers= # Comma-separated list of headers to include in a response. endpoints.cors.max-age=1800 # How long, in seconds, the response from a pre-flight request can be cached by clients. JMX ENDPOINT (EndpointMBeanExportProperties) endpoints.jmx.domain= # JMX domain name. Initialized with the value of ‘spring.jmx.default-domain’ if set. endpoints.jmx.enabled=true # Enable JMX export of all endpoints. endpoints.jmx.static-names= # Additional static properties to append to all ObjectNames of MBeans representing Endpoints. endpoints.jmx.unique-names=false # Ensure that ObjectNames are modified in case of conflict. JOLOKIA (JolokiaProperties) jolokia.config.*= # See Jolokia manual MANAGEMENT HTTP SERVER (ManagementServerProperties) management.add-application-context-header=false # Add the “X-Application-Context” HTTP header in each response. management.address= # Network address that the management endpoints should bind to. management.context-path= # Management endpoint context-path. For instance /actuator management.cloudfoundry.enabled= # Enable extended Cloud Foundry actuator endpoints management.cloudfoundry.skip-ssl-validation= # Skip SSL verification for Cloud Foundry actuator endpoint security calls management.port= # Management endpoint HTTP port. Uses the same port as the application by default. Configure a different port to use management-specific SSL. management.security.enabled=true # Enable security. management.security.roles=ACTUATOR # Comma-separated list of roles that can access the management endpoint. management.security.sessions=stateless # Session creating policy to use (always, never, if_required, stateless). management.ssl.ciphers= # Supported SSL ciphers. Requires a custom management.port. management.ssl.client-auth= # Whether client authentication is wanted (“want”) or needed (“need”). Requires a trust store. Requires a custom management.port. management.ssl.enabled= # Enable SSL support. Requires a custom management.port. management.ssl.enabled-protocols= # Enabled SSL protocols. Requires a custom management.port. management.ssl.key-alias= # Alias that identifies the key in the key store. Requires a custom management.port. management.ssl.key-password= # Password used to access the key in the key store. Requires a custom management.port. management.ssl.key-store= # Path to the key store that holds the SSL certificate (typically a jks file). Requires a custom management.port. management.ssl.key-store-password= # Password used to access the key store. Requires a custom management.port. management.ssl.key-store-provider= # Provider for the key store. Requires a custom management.port. management.ssl.key-store-type= # Type of the key store. Requires a custom management.port. management.ssl.protocol=TLS # SSL protocol to use. Requires a custom management.port. management.ssl.trust-store= # Trust store that holds SSL certificates. Requires a custom management.port. management.ssl.trust-store-password= # Password used to access the trust store. Requires a custom management.port. management.ssl.trust-store-provider= # Provider for the trust store. Requires a custom management.port. management.ssl.trust-store-type= # Type of the trust store. Requires a custom management.port. HEALTH INDICATORS management.health.db.enabled=true # Enable database health check. management.health.cassandra.enabled=true # Enable cassandra health check. management.health.couchbase.enabled=true # Enable couchbase health check. management.health.defaults.enabled=true # Enable default health indicators. management.health.diskspace.enabled=true # Enable disk space health check. management.health.diskspace.path= # Path used to compute the available disk space. management.health.diskspace.threshold=0 # Minimum disk space that should be available, in bytes. management.health.elasticsearch.enabled=true # Enable elasticsearch health check. management.health.elasticsearch.indices= # Comma-separated index names. management.health.elasticsearch.response-timeout=100 # The time, in milliseconds, to wait for a response from the cluster. management.health.jms.enabled=true # Enable JMS health check. management.health.ldap.enabled=true # Enable LDAP health check. management.health.mail.enabled=true # Enable Mail health check. management.health.mongo.enabled=true # Enable MongoDB health check. management.health.rabbit.enabled=true # Enable RabbitMQ health check. management.health.redis.enabled=true # Enable Redis health check. management.health.solr.enabled=true # Enable Solr health check. management.health.status.order=DOWN, OUT_OF_SERVICE, UP, UNKNOWN # Comma-separated list of health statuses in order of severity. INFO CONTRIBUTORS (InfoContributorProperties) management.info.build.enabled=true # Enable build info. management.info.defaults.enabled=true # Enable default info contributors. management.info.env.enabled=true # Enable environment info. management.info.git.enabled=true # Enable git info. management.info.git.mode=simple # Mode to use to expose git information. TRACING (TraceProperties) management.trace.include=request-headers,response-headers,cookies,errors # Items to be included in the trace. METRICS EXPORT (MetricExportProperties) spring.metrics.export.aggregate.key-pattern= # Pattern that tells the aggregator what to do with the keys from the source repository. spring.metrics.export.aggregate.prefix= # Prefix for global repository if active. spring.metrics.export.delay-millis=5000 # Delay in milliseconds between export ticks. Metrics are exported to external sources on a schedule with this delay. spring.metrics.export.enabled=true # Flag to enable metric export (assuming a MetricWriter is available). spring.metrics.export.excludes= # List of patterns for metric names to exclude. Applied after the includes. spring.metrics.export.includes= # List of patterns for metric names to include. spring.metrics.export.redis.key=keys.spring.metrics # Key for redis repository export (if active). spring.metrics.export.redis.prefix=spring.metrics # Prefix for redis repository if active. spring.metrics.export.send-latest= # Flag to switch off any available optimizations based on not exporting unchanged metric values. spring.metrics.export.statsd.host= # Host of a statsd server to receive exported metrics. spring.metrics.export.statsd.port=8125 # Port of a statsd server to receive exported metrics. spring.metrics.export.statsd.prefix= # Prefix for statsd exported metrics. spring.metrics.export.triggers.*= # Specific trigger properties per MetricWriter bean name. DEVTOOLS (DevToolsProperties) spring.devtools.livereload.enabled=true # Enable a livereload.com compatible server. spring.devtools.livereload.port=35729 # Server port. spring.devtools.restart.additional-exclude= # Additional patterns that should be excluded from triggering a full restart. spring.devtools.restart.additional-paths= # Additional paths to watch for changes. spring.devtools.restart.enabled=true # Enable automatic restart. spring.devtools.restart.exclude=META-INF/maven/,META-INF/resources/,resources/,static/,public/,templates/,/*Test.class,/*Tests.class,git.properties # Patterns that should be excluded from triggering a full restart. spring.devtools.restart.poll-interval=1000 # Amount of time (in milliseconds) to wait between polling for classpath changes. spring.devtools.restart.quiet-period=400 # Amount of quiet time (in milliseconds) required without any classpath changes before a restart is triggered. spring.devtools.restart.trigger-file= # Name of a specific file that when changed will trigger the restart check. If not specified any classpath file change will trigger the restart. REMOTE DEVTOOLS (RemoteDevToolsProperties) spring.devtools.remote.context-path=/.~~spring-boot!~ # Context path used to handle the remote connection. spring.devtools.remote.debug.enabled=true # Enable remote debug support. spring.devtools.remote.debug.local-port=8000 # Local remote debug server port. spring.devtools.remote.proxy.host= # The host of the proxy to use to connect to the remote application. spring.devtools.remote.proxy.port= # The port of the proxy to use to connect to the remote application. spring.devtools.remote.restart.enabled=true # Enable remote restart. spring.devtools.remote.secret= # A shared secret required to establish a connection (required to enable remote support). spring.devtools.remote.secret-header-name=X-AUTH-TOKEN # HTTP header used to transfer the shared secret. TEST spring.test.database.replace=any # Type of existing DataSource to replace. spring.test.mockmvc.print=default # MVC Print option.","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://luxiaowan.github.io/categories/SpringBoot/"}],"tags":[]},{"title":"ThreadLocal是怎么实现线程隔离的","slug":"ThreadLocal是怎么实现线程隔离的","date":"2019-11-20T15:21:56.000Z","updated":"2019-11-25T17:08:11.991Z","comments":true,"path":"2019/11/20/ThreadLocal是怎么实现线程隔离的/","link":"","permalink":"http://luxiaowan.github.io/2019/11/20/ThreadLocal是怎么实现线程隔离的/","excerpt":"","text":"ThreadLocal大家应该都不陌生，见过最多的使用场景应该是和SimpleDateFormat一起使用吧，因为这个SDF非线程安全的，所以需要使用ThreadLocal将它在线程之间隔离开，避免造成脏数据的🐞。那么ThreadLocal是怎么保证线程安全，又是如何操作的呢？ 案例 123456789101112131415161718192021public static void main(String[] args) &#123; ThreadLocal&lt;Integer&gt; threadLocal = new ThreadLocal&lt;&gt;(); new Thread(new Runnable() &#123; @Override public void run() &#123; threadLocal.set(1); threadLocal.set(2); System.out.println(\"cc1: \" + threadLocal.get()); &#125; &#125;, \"cc1\").start(); new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(\"cc2: \" + threadLocal.get()); &#125; &#125;, \"cc2\").start();&#125; 输出: 12cc1: 2cc2: null 哦哟~cc2打印出来null，也就是在cc1线程中设置的值在线程cc2中获取不到，这也就是所谓的线程隔离，我们来看下ThreadLocal具体的代码实现吧： ThreadLocal的set(T t)方法源码 123456789101112public void set(T value) &#123; // 获取当前线程 Thread t = Thread.currentThread(); // 获取当前线程的threadLocals属性，这个属性在Thread类中定义的，为Thread的实例变量 ThreadLocalMap map = getMap(t); // 若线程的ThreadLocalMap已经存在，则调用ThreadLocalMap的set(ThreadLocal&lt;T&gt; key, Object value)方法 // 否则创建新的ThreadLocalMap实例，并set对应的value if (map != null) map.set(this, value); else createMap(t, value);&#125; ThreadLocalMap的set(ThreadLocal key, Object value)方法源码 12345678910111213141516171819202122232425262728private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; Entry[] tab = table; int len = tab.length; // 简单计算key所在的位置 int i = key.threadLocalHashCode &amp; (len-1); // 从key所在位置开始遍历table数组，找到具体key所在的位置 for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; // 获取Entry实例的key值，这里调用的是超类java.lang.ref.Reference中的get(T t)方法 ThreadLocal&lt;?&gt; k = e.get(); // 若k与传入的参数key是同一个，则用参数value替换Entry实例的value，然后结束方法 if (k == key) &#123; e.value = value; return; &#125; // 若获取的k为null，则表示这个变量已经被删除了，则去清理一下table数组，并对数组中元素进行清理并设置新的Entry实例 if (k == null) &#123; replaceStaleEntry(key, value, i); return; &#125; &#125; // 代码走到这一步，说明该线程第一次设置数据，创建新的Entry实例放在table的第i个位置上 tab[i] = new Entry(key, value); int sz = ++size; // 清理table中的元素，若长度达到了扩容阈值，则对table进行扩容，扩容为原数组长度的2倍 if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash();&#125; ThreadLocal的createMap(Thread t, T firstValue)方法源码 12345void createMap(Thread t, T firstValue) &#123; // 创建一个ThreadLocalMap实例，并赋值给当前线程的实例变量threadLocals // 这里就是线程隔离的关键所在，每一个线程中的数据都是由线程独有的threadLocals变量存储的 t.threadLocals = new ThreadLocalMap(this, firstValue);&#125; ThreadLocalMap的构造器源码 123456789101112ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123; // 实例化Entry数组，长度为初始长度16 table = new Entry[INITIAL_CAPACITY]; // 计算key在数组中的位置 int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); // 创建Entry实例，并放在table的i下标位置 table[i] = new Entry(firstKey, firstValue); // 实际长度设置为1 size = 1; // 设置数组扩容阈值（len * 2 / 3） setThreshold(INITIAL_CAPACITY);&#125; 以上便是ThreadLocal达到线程隔离的基本解析，讲解的比较基础，其实就是JDK源码鉴赏，还有什么不懂的地方就自己去看源码吧。 延伸下 ThreadLocal的get()方法源码 12345678910111213public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings(\"unchecked\") T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125; 这段代码比较简单，这里就不在进行解释了，我们着重看一下最后一句setInitialValue()这个方法 1234567891011121314private T setInitialValue() &#123; T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value;&#125;protected T initialValue() &#123; return null;&#125; 会发现和set方法类似，只不过是将一个null当做value而已，所以我们在没给ThreadLocal设置值的情况下调用get方法，则会为其创建一个默认的null值并返回null。 留一个思考题 因为我们每个线程的ThreadLocal的key的hash值都是固定的，那么Thread的threadLocals变量的table中会有多少个非null元素呢？","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[]},{"title":"慎用ArrayList中的SubList","slug":"慎用ArrayList中的SubList","date":"2019-11-10T17:11:00.000Z","updated":"2019-12-19T16:51:55.997Z","comments":true,"path":"2019/11/11/慎用ArrayList中的SubList/","link":"","permalink":"http://luxiaowan.github.io/2019/11/11/慎用ArrayList中的SubList/","excerpt":"","text":"双十一了，大家都省了多少钱啊？ 题外话：此处交给大家一个查看商品历史价格的小方法： 在商品链接的域名后加上三个v就能查看到该商品的历史价格啦 🌰 http://shop.taobao.com/xxxx ↓ http://shop.taobaovvv.com/xxx 步入正题，为什么说我们在实际开发过程中要慎用ArrayList的subList呢？其实这也是阿里军规中的一条，原因其实很简单：不稳定！也许看到这里会觉得&quot;就是创建一个独立的新的SubList的实例，怎么会不稳定！&quot;，如果你是这么想的，那么恭喜你，这篇文章真的能够帮助到你，且往下看： 1. 看看SubList的set方法： 1234567891011121314151617public static void main(String[] args) &#123; List&lt;String&gt; sourceList = new ArrayList&lt;String&gt;() &#123; &#123; add(\"H\"); add(\"E\"); add(\"L\"); add(\"L\"); add(\"O\"); add(\"W\"); add(\"O\"); add(\"R\"); add(\"L\"); add(\"D\"); &#125; &#125;; List&lt;String&gt; subList = sourceList.subList(2, 5); System.out.println(\"sourceList: \" + sourceList); System.out.println(\"sourceList.subList(2, 5)得到: \" + subList); subList.set(1, \"cc\"); System.out.println(\"sourceList: \" + sourceList); System.out.println(\"subList: \" + subList); &#125;&#125; 上面代码的执行结果是什么？先不要看下面的答案，自己想一想。 答案 1234sourceList: [H, E, L, L, O, W, O, R, L, D]subList: [L, L, O]sourceList: [H, E, L, cc, O, W, O, R, L, D]subList: [L, cc, O] 哦吼~！答案和你自己想的有没有出入？奇妙吧，为什么修改了subList中的元素，会影响到sourceList？我们来看下ArrayList的subList方法都做了些什么： JDK源码 12345678/** * Returns a view of the portion of this list between the specified * &#123;@code fromIndex&#125;, inclusive, and &#123;@code toIndex&#125;, exclusive. */public List&lt;E&gt; subList(int fromIndex, int toIndex) &#123; subListRangeCheck(fromIndex, toIndex, size); return new SubList(this, 0, fromIndex, toIndex);&#125; 首先是检查我们的fromIndex和toIndex是否合法，然后调用ArrayList的内部类SubList创建一个SubList的实例。好像还真如我们之前想的一样，创建了一个独立的SubList的对象，没什么不对的，那我们来看一下SubList的构造器中都做了些什么吧。 1234567SubList(AbstractList&lt;E&gt; parent, int offset, int fromIndex, int toIndex) &#123; this.parent = parent; this.parentOffset = fromIndex; this.offset = offset + fromIndex; this.size = toIndex - fromIndex; this.modCount = ArrayList.this.modCount;&#125; 这是个什么鬼？ArrayList的实例对象(也就是parent)竟然作为参数传到了SubList中，SubList的偏移量为0+fromIndex，大小size为toIndex - fromIndex（也就是和String的substring方法一样，fromIndex到(toIndex -1)的数据集），修改次数modCount和ArrayList的modCount相等，那么我们猜测一下：SubList实例的变动，是否和ArrayList有关呢？ 我们看到subList方法的注释中有这么一句话：Returns a view of the portion of this list。难道SubList仅仅是ArrayList的一个被fromIndex和toIndex的区间视图？ 上面的例子中，subList调用了它的set方法，我们来看一下这个set方法内部逻辑是什么： 12345678910public E set(int index, E e) &#123; rangeCheck(index);// 下标校验 checkForComodification();// 校验合法性 // ***重点 // 根据偏移量和下标，获取ArrayList对象的elementData数组中下标为(offset + index)的元素 // offset是什么？从构造器中我们可以看到offset就是0 + fromIndex，也就是我们截取的起始下标，也就是SubList的set方法是直接在原ArrayList实例的内部数组上进行的操作 E oldValue = ArrayList.this.elementData(offset + index); ArrayList.this.elementData[offset + index] = e; return oldValue;&#125; 看到这里就一目了然了，怪不得我们修改了SubList的元素会影响到创建它的对象的值。所以在使用SubList的时候，如果需要修改SubList里面的值，一定要注意一下是否会影响到原List中的数据所涉及的业务，否则这个坑一旦踩上了，不太容易排查啊。 2. 再看看SubList的add方法 1234567891011121314151617public static void main(String[] args) &#123; List&lt;String&gt; sourceList = new ArrayList&lt;String&gt;() &#123; &#123; add(\"H\"); add(\"E\"); add(\"L\"); add(\"L\"); add(\"O\"); add(\"W\"); add(\"O\"); add(\"R\"); add(\"L\"); add(\"D\"); &#125; &#125;; List&lt;String&gt; subList = sourceList.subList(2, 5); System.out.println(\"sourceList: \" + sourceList); System.out.println(\"sourceList.subList(2, 5)得到: \" + subList); subList.add(\"cc\"); System.out.println(\"sourceList: \" + sourceList); System.out.println(\"subList: \" + subList); &#125;&#125; 上面代码的执行结果又是什么呢？如果我们稍微思考一下，大致能正确的分析出结果： 答案 1234sourceList: [H, E, L, L, O, W, O, R, L, D]subList: [L, L, O]sourceList: [H, E, L, L, O, cc, W, O, R, L, D]subList: [L, L, O, cc] 我们向subList中添加一个元素，原列表sourceList在toIndex的位置插入了subList中add的元素，也就是我们在SubList中新增一个元素，同时会将这个元素添加到原List中。 JDK源码 我们查看SubList的源码，发现并没有add(E e)方法，那我们调用的add(“cc”)是调用到哪里去了呢？我们查看SubList类的声明，可以看到它是继承了AbstractList抽象类，所以这里应该是调用了超类里的add(E e)方法， 12345/** AbstractList.java */public boolean add(E e) &#123; add(size(), e); return true;&#125; 这里可以看到是调用了add(int index, E element)方法进行数据新增的，然而SubList里面实现了这个方法，那么我们来看下SubList中的这个方法实现： 123456789101112public void add(int index, E e) &#123; // 校验下标是否越界 rangeCheckForAdd(index); // 校验原List是否有过修改 checkForComodification(); // parent即是在构造器中注入的原List parent.add(parentOffset + index, e); // 同步列表修改次数 this.modCount = parent.modCount; // 本列表的长度+1 this.size++;&#125; 由SubList的源码可以看出，SubList实例的add方法实际上就是在修改原List，包括SubList中所有的方法均是在parent列表上进行操作。 3. 奇葩操作，最坑的坑 仔细分析如下代码： 12345678910111213141516public static void main(String[] args) &#123; List&lt;String&gt; sourceList = new ArrayList&lt;String&gt;() &#123; &#123; add(\"H\"); add(\"E\"); add(\"L\"); add(\"L\"); add(\"O\"); add(\"W\"); add(\"O\"); add(\"R\"); add(\"L\"); add(\"D\"); &#125; &#125;; List&lt;String&gt; subList = sourceList.subList(2, 5); System.out.println(\"sourceList: \" + sourceList); System.out.println(\"subList: \" + subList); sourceList.add(\"cc\"); System.out.println(\"sourceList: \" + sourceList); System.out.println(\"subList: \" + subList);&#125; 这段代码的执行结果是什么？在不执行这段代码的情况下，是不是以为是下面的结果？ 1234sourceList: [H, E, L, L, O, W, O, R, L, D]subList: [L, L, O]sourceList: [H, E, L, L, O, W, O, R, L, D, cc]subList: [L, L, O] 如果你说对，就是这个，那你可就说错咯，实际上在执行到System.out.println(&quot;sourceList: &quot; + sourceList);这一句代码的时候整个程序的输出都是正常的，但在执行最后一句代码的时候，就会报错了，错误信息是： 123456789Exception in thread \"main\" java.util.ConcurrentModificationException at java.util.ArrayList$SubList.checkForComodification(ArrayList.java:1239) at java.util.ArrayList$SubList.listIterator(ArrayList.java:1099) at java.util.AbstractList.listIterator(AbstractList.java:299) at java.util.ArrayList$SubList.iterator(ArrayList.java:1095) at java.util.AbstractCollection.toString(AbstractCollection.java:454) at java.lang.String.valueOf(String.java:2994) at java.lang.StringBuilder.append(StringBuilder.java:131) at cc.kevinlu.sublist.SubListTest.main(SubListTest.java:31) 哦吼~！竟然报错了，我们可以看到是在ArrayList$SubList.checkForComodificatio方法中报的错，我们来看一下这个方法： 12345private void checkForComodification() &#123; // 比较原列表修改次数和SubList的修改次数是否相等 if (ArrayList.this.modCount != this.modCount) throw new ConcurrentModificationException();&#125; 这里抛出异常，说明这两个数是不相等的，那为什么会不相等呢？我们看SubList的add方法中有同步主、'子’列表的语句this.modCount = parent.modCount;，也就是说我们在修改subList的时候，会同步更新主列表的modCount，以保证主、'子’列表始终是一致的。 但是我们在修改主List的时候是不会去同步SubList的modCount的，我们输出SubList的实例实际上就是调用iterator方法，最终是调用了SubList的public ListIterator&lt;E&gt; listIterator(final int index)方法，该方法第一句就是调用checkForComodification方法检查modCount，这里自然就会报错咯！ 4. 填坑 既然有坑，就有填坑的办法，不可能一直把坑放在那，是吧。 如果既想修改subList，又不想影响到原list。那么可以创建一个机遇subList的拷贝: 123451.创建新的List： subList = Lists.newArrayList(subList);2.lambda表达式： sourceList.stream().skip(fromIndex).limit(size).collect(Collectors.toList()); 5. 总结 并不是说使用SubList一定不妥，文章开头我们也说的是慎用，所以，根据具体业务进行选择吧。","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[]},{"title":"Redis基本命令使用::zset篇","slug":"Redis基本命令使用—zset篇","date":"2019-11-08T04:55:00.000Z","updated":"2019-11-21T16:05:02.245Z","comments":true,"path":"2019/11/08/Redis基本命令使用—zset篇/","link":"","permalink":"http://luxiaowan.github.io/2019/11/08/Redis基本命令使用—zset篇/","excerpt":"","text":"Redis有6大基本类型，分别是字符串(String)、列表(List)、集合(set)、哈希结构(hash)、有序集合(zset)和基数(HyperLogLog) redis在线体验地址：http://try.redis.io/ 有序集合(ZSet) 介绍: Redis的有序集合和集合一样是一个简单的字符串集合，但该集合是有序的，且集合内的元素都是唯一的，也就是集合内不会出现重复元素，与集合唯一不同的是，有序集合中每一个元素都有一个double类型的score属性，Redis通过score的大小对元素进行排序的。有序集合中元素不能重复，但是元素的score值可以重复。 常用于做排行榜功能。 Redis中的集合是通过哈希表来实现的，所以获取集合中元素的时间复杂度为O(1)。 创建缓存 创建一个集合缓存，为集合新增元素 命令格式：zadd key score member [score member ...] 🌰 12345678zadd zk1 1 c 1 cc 1 ccc 2 cccc---创建有序集合zk1，元素为：key: c, score: 1key: cc, score: 1key: ccc, score: 1key: cccc, score: 2 创建一个元素或修改元素的score值（一次只能修改一个元素） 命令格式：zincrby key increment member 🌰 123456zincrby zk1 1 cc---1. 为集合zk1的元素cc的score进行+1操作2. 若集合zk1不存在，则创建3. 若元素cc不存在，则创建，且cc的score为1 查询缓存 查询缓存中元素个数 命令格式：zcard key 🌰 1234zcard zk1---查询集合zk1内的元素总个数 查询集合指定范围的元素 命令格式：zrange key start stop [withscores] 🌰 12345678910111213141. 返回元素名称zrange zk1 0 1---返回集合zk1中从下标0到下标1的元素名称，start和stop都是从0开始2. 返回元素和元素scorezrange zk1 0 -1 withscores---返回集合zk1中全部元素的名称和分数member1score1member2score2★常用于查询排行榜及分数 查询集合中某元素的下标（下标从0开始） 命令格式：zrank key member 🌰 1234zrank zk1 c---返回元素c在集合zk1中的下标 查询集合中某元素的分数 命令格式：zscore key member 🌰 12345zscore zk1 c---查询集合zk1中元素c的分数★常用于点赞数类别查询等 查询集合中指定范围的元素，按照score从大到小排序 命令格式：zrevrange key start stop [withscores] 🌰 1234zrevrange zk1 0 3 withscore---返回集合zk1中从1~4位元素，按照score从大到小 查询集合中某元素的排名 命令格式：zrevrank key member 🌰 12345zrevrank zk1 c---返回元素c在集合zk1中的排名★常用于名次查询 查询指定分数范围内的元素，可分页 命令格式：zrevrangebyscore key maxScore minScore [withscores] [limit offset count] 🌰 1234zrevrangebyscore zk1 3 2 withscores limit 0 1---分页返回集合zk1中分数从2~3的元素 查询指定成员区间内的成员 命令格式：zrangebylex key minChar maxChar [limit offset count] 🌰 1234zrangebylex zk1 - (c1 limit 0 12---返回从第一个元素到元素c1之间的位置 指令 是否必须 说明 ZRANGEBYLEX 是 指令 key 是 有序集合键名称 minChar 是 字典中排序位置较小的成员,必须以”[“(包含)开头,或者以”(“(不包含)开头,可使用”-“代替，&quot;-&quot;表示取最小值 maxChar 是 字典中排序位置较大的成员,必须以”[“(包含)开头,或者以”(“(不包含)开头,可使用”+”代替，&quot;+&quot;表示取最大值 limit 否 返回结果是否分页,指令中包含LIMIT后offset、count必须输入 offset 否 返回结果起始位置 count 否 返回结果数量 查询指定分数区间内的元素数 命令格式：zcount key min max 🌰 12345zcount zk1 1 2---返回score值为1~2的所有元素总数★计算排行榜中某一分数区间的数量 查询指定元素区间内的元素总数 命令格式：zlexcount key min max 🌰 1234zlexcount zk1 (c [cccc---查询c~cccc之间的元素总数，不包括c，但包括cccc 移除缓存元素 移除集合中指定的元素 命令格式：zrem key member [member ...] 🌰 1234zrem zk1 cc c1---移除集合zk1中的元素cc、c1 移除指定元素区间的所有成员 命令格式：zremrangebylex key min max 🌰 1234zremrangebylex zk1 [c (ccc---移除集合zk1中元素c到元素ccc之间的所有成员，包括c，但不包括ccc 移除指定排名区间所有成员 命令格式：zremrangebyrank key start stop 🌰 1234zremrangebyrank zk1 0 1---移除集合zk1中0~1下标的所有元素 移除指定分数区间所有成员 命令格式：zremrangebyscore key min max 🌰 1234zremrangebyscore zk1 0 1---移除集合zk1中score值为0~1的所有元素 特殊操作 计算多个集合的并集，并存入新的集合 命令格式：zunionstore destinationKey numkeys key[key ...] 🌰 1234zunionstore zku 2 zk1 zk2---合并集合zk1和zk2，将并集存入zku，集合zku中元素的score为所有参与计算的集合中相同的元素的score之和","categories":[{"name":"Redis","slug":"Redis","permalink":"http://luxiaowan.github.io/categories/Redis/"}],"tags":[]},{"title":"Java命令::jstat","slug":"Java命令—jstat","date":"2019-11-08T04:00:00.000Z","updated":"2019-11-10T15:57:22.417Z","comments":true,"path":"2019/11/08/Java命令—jstat/","link":"","permalink":"http://luxiaowan.github.io/2019/11/08/Java命令—jstat/","excerpt":"","text":"jstat是用于监控虚拟机运行状态信息的命令，可以显示虚拟机进程中的类装载、内存使用、GC情况、JIT编译等运行状态数据，能够在Linux上快速定位虚拟机性能问题。 jstat命令在jdk的bin目录下，目录中还有很多实用的命令 *以下分析是基于jdk1.8+ jstat命令格式： 123456789jstat -&lt;option&gt; &lt;pid&gt; [&lt;interval&gt; [&lt;count&gt;]]--- option: 需要查看的虚拟机信息 pid: Java程序进程号 本地虚拟机: pid 远程虚拟机: [protocol:][//] lvmid [@hostname[:port]/servername] interval: 监控间隔时间，可选，默认立刻执行一次 count: 监控次数，可选，默认无限次 option 说明 class 查看类装载、卸载数量、总空间及类装载所耗时间 gc 查看Java堆状况，包括Eden、survivor、老年代、永久代的容量 gcutil 类似于gc，主要输出各区域空间使用占比 gccause 同gc，会多输出每次gc的原因 gccapacity 同gc，但输出的主要是Java堆各个区域使用到的最大、最小空间 gcnew 查看新生代的使用情况 gcnewcapacity 同gcnew，输出内容主要关注新生代的最大、最小空间 gcold 查看老年代的使用情况 gcoldcapacity 同gcold，输出内容主要关注老年代的最大、最小空间 gcpermcapacity 输出永久代使用到的最大、最小空间 compiler 输出JIT编译器编译过的方法、耗时等信息 printcompilation 输出已经被JIT编译的方法 🌰 jstat -class : 显示加载class的数量及所占空间等信息 12345678910[root@master0 ~]# jstat -class 19080Loaded Bytes Unloaded Bytes Time 11512 22276.9 268 421.6 18.41 ---Loaded: 装载类的数量Bytes: 装载类所占用的字节数Unloaded: 卸载类的数量Bytes: 卸载类所占用的字节数Time: 装载和卸载类所花费的时间 jstat -gc : 显示gc的信息，查看gc的次数和时间 1234567891011121314151617181920212223，等于YGCT + FGCT[root@master0 ~]# jstat -gc 19080 1000 1 S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT 6656.0 6656.0 0.0 3761.4 94208.0 51469.2 73728.0 50832.0 - - - - 20207 92.896 37 7.833 100.729---[容量为字节]S0C: 年轻代第一个survivor区的总容量（survivor 0 capacity）S1C: 年轻代第二个survivor区的总容量（survivor 1 capacity）S0U: 年轻代第一个survivor区的已使用容量（survivor 0 using）S1U: 年轻代第二个survivor区的已使用容量（survivor 1 using）EC: 年轻代Eden区的总容量（Eden capacity）EU: 年轻代Eden区的已使用容量（Eden using）OC: 老年代的总容量（Old capacity）OU: 老年代已使用的容量（Old using）MC: Metaspace的总容量, jdk1.8+MU: Metaspace已使用的容量, jdk1.8+CCSC:压缩类空间容量, jdk1.8+CCSU:压缩类空间已使用的容量, jdk1.8+YGC: 服务启动至今年轻代gc的次数（young gc）YGCT: 服务启动至今年轻代gc使用的时间，秒（young gc time）FGC: 服务启动至今fullgc的次数FGCT: 服务启动至今fullgc使用的时间，秒GCT: 服务启动至今gc用的总时间，秒，等于YGCT + FGCT jstat -gcutil : 统计gc信息 123456789101112131415161718[root@master0 ~]# jstat -gcutil 19080 1000 3 S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 0.00 48.90 20.83 69.35 - - 20221 93.006 37 7.833 100.838 0.00 48.90 20.83 69.35 - - 20221 93.006 37 7.833 100.838 0.00 48.90 20.83 69.35 - - 20221 93.006 37 7.833 100.838---S0: 年轻代第一个survivor已使用容量比例S1: 年轻代第二个survivor已使用容量比例E: 年轻代Eden区已使用容量比例O: 老年代已使用容量比例M: 元空间已使用容量比例CCS: 压缩类空间已使用容量比例YGC: 服务启动至今年轻代gc次数YGCT: 服务启动至今年轻代gc所占用时间，秒FGC: 服务启动至今fullgc次数FGCT: 服务启动至今fullgc所占用时间，秒GCT: 服务启动至今gc总占用时间，秒，等于YGCT + FGCT jstat -gccause : 查看gc原因 12345678[root@master0 ~]# jstat -gccause 19080 S0 S1 E O M CCS YGC YGCT FGC FGCT GCT LGCC GCC 57.32 0.00 13.35 69.36 - - 20222 93.015 37 7.833 100.848 Allocation Failure No GC ---上述可以看到比-gcutil多处了一个LGCC和GCCLGCC: 最近一次gc发生的原因（last gc cause）GCC: 当前gc发生的原因 jstat -gccapacity : 查看虚拟机中对象的使用和容量大小 12345678910[root@master0 ~]# jstat -gccapacity 19080 NGCMN NGCMX NGC S0C S1C EC OGCMN OGCMX OGC OC MCMN MCMX MC CCSMN CCSMX CCSC YGC FGC 20480.0 323584.0 112640.0 7680.0 6656.0 96768.0 40448.0 647168.0 73728.0 73728.0 - - - - - - 20223 37---[容量单位为字节]NGC开头的表示：新生代空间容量OGC开头的表示：老年代空间容量MC开头的表示：元空间容量（Metaspace capacity）CCS开头的表示：类压缩空间 jstat -gcnew : 查看新生代gc情况 123456789[root@master0 ~]# jstat -gcnew 19080 S0C S1C S0U S1U TT MTT DSS EC EU YGC YGCT 7680.0 6656.0 0.0 6354.4 15 15 7680.0 96768.0 95634.9 20223 93.023---[容量单位为字节]TT: 老年化阈值，也可以理解为对象持有次数，就是在被移动到老年代之前，在新生代中存活的次数MTT: 最大老年化阈值DSS: Survivor区所需空间大小 jstat -gcnewcapacity : 查看新生代空间容量 123456789101112[root@master0 ~]# jstat -gcnewcapacity 19080 NGCMN NGCMX NGC S0CMX S0C S1CMX S1C ECMX EC YGC FGC 20480.0 323584.0 112128.0 107520.0 7168.0 107520.0 7168.0 322560.0 97280.0 20225 37---[容量单位为字节]MN表示：最小MX表示：最大NGC开头表示：新生代空间总容量S0C开头：新生代第一个survivor区容量S1C开头：新生代第二个survivor区容量EC开头：新生代Eden区容量 jstat -gcold : 查看老年代gc情况 1234567[root@master0 ~]# jstat -gcold 19080 1000 MC MU CCSC CCSU OC OU YGC FGC FGCT GCT - - - - 73728.0 51144.0 20234 37 7.833 100.943 - - - - 73728.0 51144.0 20234 37 7.833 100.943---[容量单位为字节] jstat -gcoldcapacity : 查看老年代容量 1234[root@master0 ~]# jstat -gcoldcapacity 19080 100 2 OGCMN OGCMX OGC OC YGC FGC FGCT GCT 40448.0 647168.0 73728.0 73728.0 20235 37 7.833 100.949 40448.0 647168.0 73728.0 73728.0 20235 37 7.833 100.949 jstat -gcpermcapacity : 查看perm中对象的信息和容量 1jdk1.8+以上去除了该命令，如果你当前使用的是jdk1.7-，那么自行谷歌吧 jstat -compiler : 查看虚拟机实时编译的信息 1234567891011[root@master0 ~]# jstat -compiler 19080Compiled Failed Invalid Time FailedType FailedMethod 2737 0 0 41.90 0 ---Compiled: 编译任务执行数量Failed: 编译任务执行失败数量Invalid: 编译任务执行失效数量Time: 编译任务消耗的时间FailedType: 最后一个编译失败任务的类型FailedMethod: 最后一个编译失败的任务所在类及方法 jstat -printcompilation : 查看虚拟机已经编译过的方法 123456789[root@master0 ~]# jstat -printcompilation 19080Compiled Size Type Method 2737 1562 1 sun/misc/FloatingDecimal dtoa---Compiled: 编译任务的数量Size: 方法生成字节码的大小（单位：字节）Type: 编译类型Method: 类名和方法名用来标识编译的方法","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[]},{"title":"Redis基本命令使用::list篇","slug":"Redis基本命令使用—list篇","date":"2019-10-31T04:55:00.000Z","updated":"2019-10-31T17:23:56.890Z","comments":true,"path":"2019/10/31/Redis基本命令使用—list篇/","link":"","permalink":"http://luxiaowan.github.io/2019/10/31/Redis基本命令使用—list篇/","excerpt":"","text":"Redis有6大基本类型，分别是字符串(String)、列表(List)、集合(set)、哈希结构(hash)、有序集合(zset)和基数(HyperLogLog) redis在线体验地址：http://try.redis.io/ 列表(List) 介绍: Redis的列表就是一个简单的字符串列表，我们可以往列表的头部和尾部添加新数据，要可以根据下标修改下标对应的值，列表是按照插入顺序有序的（按插入顺序倒序，类似于栈），并且列表可以出现重复数据。可以做消息队列，不过需要注意的是可能需要消息去重(后面有更牛的)。 创建缓存 创建一个列表缓存 命令格式：lpush key value [value ...] 🌰 1234lpush c1 1 2 3 4 5 6 7 8 9---将1~9放入c1列表中，此时列表中存储顺序为9 8 7 6 5 4 3 2 1 向列表左侧新增值 命令格式：lpush key value [value ...] 🌰 1234lpush c1 10---将10放入到c1列表头部，此时列表中存储顺序为10 9 8 7 6 5 4 3 2 1 向列表右侧新增值 命令格式：rpush key value [value ...] 🌰 1234rpush c1 0---将0放入到c1列表尾部，此时列表中存储顺序为10 9 8 7 6 5 4 3 2 1 0 以上两个命令很容易理解，lpush—&gt;left push, rpush—&gt;right push 在列表指定元素前/后插入数据 命令格式：linsert key BEFORE|AFTER pivot value 🌰 123456789101112131415161718192021221) linsert c1 after 0 -1---将-1插入到元素值0之前，此时列表中存储顺序为10 9 8 7 6 5 4 3 2 1 0 -12)linsert c1 before 0 1---将1插入到元素值0之前，此时列表中存储顺序为10 9 8 7 6 5 4 3 2 1 1 0 -13)linsert c1 before 1 3---将3插入到元素值1之前，此时列表中存储顺序为10 9 8 7 6 5 4 3 2 3 1 1 0 -14)linsert c1 after 1 4---将4插入到元素值1之后，此时列表中存储顺序为10 9 8 7 6 5 4 3 2 3 1 4 1 0 -15)linsert c1 after 99 100---失败** 由3、4操作可以得出结论，当执行linsert指令时，列表是从头部开始进行遍历，直到查询到与pivot元素相同的值时停止，根据AFTER、BEFORE选择是插入到元素后还是元素前，所以划重点：若列表中存在pivot的重复元素，linsert只会以第一个遍历到的元素为准** 由5可以得出结论，若指定的pivot元素不存在于列表中，则不进行任何设置 通过索引下标设置值 命令格式：lset key index value 🌰 12345671)lset c1 0 21---将下标为0的元素修改为21，此时列表中存储顺序为21 9 8 7 6 5 4 3 2 3 1 4 1 0 -12)lset c1 99 99---ERR index out of range,说明通过lset命令，不能越界修改元素 向已存在的列表头部添加元素 命令格式：lpushx key value [value ...] 🌰 12345671)lpushx c1 22 23 24---将22、23、24添加到列表c1头部，列表变为：24 23 22 21 9 8 7 6 5 4 3 2 3 1 4 1 0 -12)lpushx c2 1 2 3---因为列表c2不存在，所以设置失败，此时使用lrange查看c2会返回空 向已存在的列表尾部添加元素 命令格式：rpushx key value [value ...] 🌰 12345671)rpushx c1 -2 -3---将-2、-3添加到列表c1尾部，列表变为：24 23 22 21 9 8 7 6 5 4 3 2 3 1 4 1 0 -1 -2 -32)rpushx c2 1 2 3---因为列表c2不存在，所以设置失败，此时使用lrange查看c2会返回空 截取列表 命令格式：ltrim key start end 🌰 123ltrim c1 4 16---列表下标从0开始，截取5~17位的元素，列表c1变为：9 8 7 6 5 4 3 2 3 1 4 1 0 查看缓存 查看列表内所有元素 命令格式：lrange key 0 -1 🌰 1234lrange c1 0 -1---获取列表c1的所有元素 查看列表某一范围内的元素 命令格式：lrange key start end 🌰 1234lrange c1 1 3---查看列表c1中弟2~4位上的元素 弹出列表头部元素 命令格式：lpop key 🌰 1234lpop c1---弹出列表c1的头部元素9，此时列表c1变为：8 7 6 5 4 3 2 3 1 4 1 0，头部的9已经没有了，是不是很适合做消息队列 弹出列表尾部元素 命令格式：rpop key 🌰 1234rpop c1---弹出c1的尾部元素0，此时列表c1变为：8 7 6 5 4 3 2 3 1 4 1，尾部的0已经没有了，是不是很适合做消息队列👀 弹出列表头部元素，若当前列表内无元素，则阻塞，直到获取到或达到超时时间 命令格式：blpop key [key ...] timeout timeout单位为***秒*** 🌰 12345blpop c1 c2 c3 10---弹出列表c1或列表c2/c3的头部元素，只要c1、c2、c3有一个列表中有元素被弹出，则结束阻塞若c1、c2、c3均有元素，则返回第一个满足弹出条件的列表，然后结束阻塞 弹出列表尾部元素，若当前列表内无元素，则阻塞，直到获取到或达到超时时间 命令格式：brpop key [key ...] timeout timeout单位为***秒*** 🌰 12345blpop c1 c2 c3 10---弹出列表c1或列表c2/c3的尾部元素，只要c1、c2、c3有一个列表中有元素被弹出，则结束阻塞若c1、c2、c3均有元素，则返回第一个满足弹出条件的列表，然后结束阻塞 获取列表指定位置的元素 命令格式：lindex key index 🌰 12345671)lindex c1 2---返回列表c1中下标为2的元素，仅仅返回数据，不弹出，时间复杂度O(1)2)lindex c1 -1---返回列表最后一个元素。列表元素下标-1代表列表中最后一个元素，所以列表是可以通过负数下标从后往前遍历 弹出一个列表中的最后一个元素到另外一个列表头部，并返回这个元素——无阻塞 命令格式：rpoplpush source_key destination_key 🌰 1234rpoplpush c1 c2---弹出列表c1的尾部元素插入到列表c2的头部，若c1为空，则返回nil，但不插入到c2中，是不是更适合做队列 弹出一个列表中的最后一个元素到另外一个列表头部，并返回这个元素——阻塞 命令格式：brpoplpush source_key destination_key timeout timeout单位为***秒*** 🌰 1234brpoplpush c3 c2 10---弹出列表c3的尾部元素插入到列表c2的头部，若c3为空，则阻塞等到列表c3中有值，否则等到了10秒后结束阻塞返回nil，是不是更适合做阻塞队列 其他命令 删除指定范围内等于某个值的所有元素 命令格式：lrem key index element 🌰 1234567891011121314151)lrem c1 -2 3---移除列表c1中，从倒数第二个元素到列表头部范围内所有的32)lrem c1 3 2---移除列表c1中，从第四位元素到尾部范围内所有的23)lrem c1 0 1---移除列表c1中所有的14)lrem c1 -1 4---移除列表c1中所有的4 查看列表长度 命令格式：llen key 🌰 1234llen c1---查看列表c1的总长度，若c1不存在，则返回0，不会报错，记住，若列表不存在也不会报错","categories":[{"name":"Redis","slug":"Redis","permalink":"http://luxiaowan.github.io/categories/Redis/"}],"tags":[]},{"title":"Redis基本命令使用::set篇","slug":"Redis基本命令使用—set篇","date":"2019-10-31T04:55:00.000Z","updated":"2019-11-10T15:57:22.418Z","comments":true,"path":"2019/10/31/Redis基本命令使用—set篇/","link":"","permalink":"http://luxiaowan.github.io/2019/10/31/Redis基本命令使用—set篇/","excerpt":"","text":"Redis有6大基本类型，分别是字符串(String)、列表(List)、集合(set)、哈希结构(hash)、有序集合(zset)和基数(HyperLogLog) redis在线体验地址：http://try.redis.io/ 集合(Set) 介绍: Redis的集合就是一个简单的字符串集合，该集合是无序的，且集合内的元素都是唯一的，也就是集合内不会出现重复元素。Redis中的集合是通过哈希表来实现的，所以获取集合中元素的时间复杂度为O(1)。 创建缓存 创建一个集合缓存，为集合新增元素 命令格式：sadd key member [member ...] 🌰 1234sadd s1 1 2 3 0 -1 redis mongodb zookeeper---创建集合s1,元素为：1 2 3 0 -1 redis mongodb zookeeper 移除元素 随机移除集合中的一个元素并返回这个元素 命令格式：spop key 🌰 1234spop s1---移除之后，集合s1中就没有这个元素了 指定移除集合中一个或多个元素 命令格式：srem key member [member ...] 🌰 1234srem s1 -1 0 99---将元素-1、0、99从集合s1中移除，仅移除集合中存在的元素 查询集合 查询集合中元素个数 命令格式：scard key 🌰 1234scard s1---返回集合s1中元素总个数 返回集合中所有元素 命令格式：smembers key 🌰 1234smembers s1---返回集合s1中所有的元素 随机返回集合中的一个或多个元素 命令格式：srandmember key [count] 🌰 1234srandmember s1 10---随机返回集合s1中的10个元素，若不指定数量，则默认返回一个元素 迭代集合中的元素 命令格式：sscan key cursor [MATCH pattern] [COUNT count] 🌰 1234sscan s1 0 match re* count 1---迭代集合中re开头的所有元素，每次返回1个 判断元素是否存在于集合中 命令格式：sismember key member 🌰 1234sismember s1 99---若99存在于s1中，则返回1，不存在则返回0 多集合之间操作 查看多个集合的差集 命令格式：sdiff key [key ...] 🌰 1234sdiff s1 s2 s3---返回集合s1相对于s2、s3的差集，也就是只返回s1中所有不存在于s2、s3中的所有元素 多个集合的差集存储到指定集合中 命令格式：sdiffstore destination key [key ...] 🌰 1234sdiffstore ds1 s1 s2 s3---将s1中不存在于s2、s3中的元素存储到集合ds1中 查看多个集合的并集，去重 命令格式：sunion key [key ...] 🌰 1234sunion s1 s2 s3---将集合s1、s2、s3的元素合并去重后返回，所有元素均唯一 多个集合的并集存储到指定集合中 命令格式：sunionstore destination key [key ...] 🌰 1234sunionstore us1 s1 s2 s3---集合s1、s2、s3的并集存储到集合us1中，并返回集合us1中的元素个数 将一个集合中的某元素移动到另一个集合中 命令格式：smove source destination member 🌰 1234smove s1 s2 -2---将s1中的元素-2移动到集合s2中，若s2不存在，则自动创建 查看多个集合的交集 命令格式：sinter key [key ...]（intersection） 🌰 123456sinter s1 s3---返回集合s1和集合s3的交集，也就是两个集合中都存在的数据实际应用：查看两个人的共同好友；微信里查看和好友的共同群 多个集合的交集存储到指定集合中 命令格式：sinterstore destination key [key ...] 🌰 1234sinterstore is1 s1 s3---将s1和s3的交集元素存储到集合is1中","categories":[{"name":"Redis","slug":"Redis","permalink":"http://luxiaowan.github.io/categories/Redis/"}],"tags":[]},{"title":"Redis基本命令使用::hash篇","slug":"Redis基本命令使用—hash篇","date":"2019-10-30T15:55:00.000Z","updated":"2019-10-31T16:05:56.510Z","comments":true,"path":"2019/10/30/Redis基本命令使用—hash篇/","link":"","permalink":"http://luxiaowan.github.io/2019/10/30/Redis基本命令使用—hash篇/","excerpt":"","text":"Redis有6大基本类型，分别是字符串(String)、列表(List)、集合(set)、哈希结构(hash)、有序集合(zset)和基数(HyperLogLog) redis在线体验地址：http://try.redis.io/ 哈希(Hash) 介绍: Redis hash 是一个存储多个键值对的映射表，适用于存储对象的属性，比如存储用户信息、用户Session信息等。在实际项目中使用的频率比较多，之前主要用于存储用户基本信息、用户临时订单信息、产品信息等。 创建缓存 创建一个缓存 命令格式：hset key field value 🌰 1234hset cc name 'cc'---将cc的name属性的值设置为cc 批量创建缓存 命令格式：hmset key filed value [field value ...] 🌰 1234hmset cc name 'cc' age 19 avatar 'a.png' status 1---设置cc对象的name、age、avatar、status属性的值 设置一个key不存在field的value，若field已存在则不设置 命令格式：hsetnx key field value 🌰 1234hsetnx cc name 'yy'---若对象cc的name属性不存在，则设置cc的name属性值为yy 查看缓存 查看key下所有属性+值 命令格式：hgetall key 🌰 12345678hgetall cc---获取对象cc的所有属性，返回数据格式： field1 value1 field2 value2 查看key下所有的值 命令格式：hvals key 🌰 12345hvals cc---返回cc对象的所有属性的值，仅返回值，不返回属性名称等同于 ”hmget key 所有field“ 命令 查看key下所有的field名称 命令格式：hkeys key 🌰 1234hkeys cc---返回对象cc的所有属性名 查看key的某一field的值 命令格式：hget key field 🌰 1234hget cc name---返回对象cc的name属性的值，若对象无此属性，则返回nil 查看key的多个field的值 命令格式：hmget key field [field ...] 🌰 1234hmget cc name age status---返回对象cc的name、age、status属性 迭代对象的所有属性(适用于大对象) 命令格式：hscan key course [MATCH pattern] [COUNT num] 🌰 1hscan cc 0 MATCH *e COUNT 1 查看对象的属性数 命令格式：hlen key 🌰 1234hlen cc---返回对象cc的属性数量 其他命令 删除一个/多个属性 命令格式：hdel key field [field ...] 🌰 1234hdel cc name age---删除对象cc的name、age属性 查看对象属性是否存在 命令格式：hexists key field 🌰 1234hexists cc name---对象cc若存在属性name，则返回1，不存在则返回0","categories":[{"name":"Redis","slug":"Redis","permalink":"http://luxiaowan.github.io/categories/Redis/"}],"tags":[]},{"title":"Redis基本命令使用::string篇","slug":"Redis基本命令使用—string篇","date":"2019-10-29T15:55:00.000Z","updated":"2019-10-30T16:43:50.550Z","comments":true,"path":"2019/10/29/Redis基本命令使用—string篇/","link":"","permalink":"http://luxiaowan.github.io/2019/10/29/Redis基本命令使用—string篇/","excerpt":"","text":"Redis有6大基本类型，分别是字符串(String)、列表(List)、集合(set)、哈希结构(hash)、有序集合(zset)和基数(HyperLogLog) redis在线体验地址：http://try.redis.io/ 字符串(String) 介绍: 字符串是Redis中最基本的数据类型，数据以二进制的形式存储于内存中，所以Redis的字符串可以是任何形式的数据，比如JPEG图像、序列化的Ruby对象等。 字符串最大可存储512MB的数据，但一般一个字符串容量过大，会直接影响存储和查询的效率。字符串有两种编码方式：raw和embstr，根据字符串长度自动选择使用哪一种编码，目前最新版的长度是44个字节，字符串长度小于等于44个字节，则使用embstr编码，大于44个字节则使用raw编码，两种编码方式这里就不作详解，有兴趣的可以谷歌一下。 创建缓存 创建一个缓存 命令格式：set key value 🌰 1234set cc 'niubility'---创建一个key为cc，值为niubility的缓存 批量创建缓存 命令格式：mset key value [key value ...] 🌰 1234mset cc1 1 cc2 2 cc3 3---创建三个缓存，key:value分别为cc1:1, cc2:2, cc3:3 命令格式：msetnx key value [key value ...] 🌰 12345msetnx cc1 11 cc2 22 cc5 5 cc6 6---此命令只会将尚不存在的key值创建到缓存中，已经存在的key则忽略* cc1、cc2已存在于缓存中，所以不会创建/更新成功，cc5、cc6不存在于缓存中，会创建 创建一个带过期时间的缓存 命令格式：setex key time value 🌰 1234setex cc1 10 234---设置key:value为cc1:234且过期时间为10秒的缓存 设置一个不存在key的value，若key已存在则不设置 命令格式：setnx key value 🌰 1234setnx cc5 12---若cc5的key不存在于缓存中，则创建key:value为cc5:12的缓存，否则不执行创建 组合创建一个缓存(缓存过期时间、是否覆盖) 命令格式：set key value [EX|PX time] [NX|XX] 解析： 1234567 EX：表明过期时间为秒 PX：表明过期时间为毫秒 NX：若key不存在则执行，否则不执行，与XX相反 XX：若key存在则执行，否则不执行，与NX相反 🌰 1234 set cc1 123 EX 20 XX --- 若cc1已存在则创建过期时间为20秒的key:value = cc1:123 将key设置为新值的同时返回原值 命令格式：getset key value 🌰 1234getset cc1 1---若key不存在，则返回nil 读取缓存 读取一个key的缓存值 命令格式：get key 🌰 1234get cc1---读取key=cc1的值 批量读取一批数据 命令格式：mget key [key ...] 🌰 1mget cc1 cc2 cc3 截取字符串并返回 命令格式：getrange key start end 🌰 1234getrange cc1 1 12---字符串下标以0开始，若start超出字符串长度或key不存在，则返回空字符串 其他操作 将value加1( value必须为整数 )[ 可用于阅读量、点赞数等简单统计类的功能应用 ] 命令格式：incr key 🌰 1234incr cc1---每次调用均对value进行+1操作 给value加上某个数( num必须为整数 ) 命令格式：incrby key num 🌰 1234incrby cc1 100---给cc1的值加上100 给value加上某个浮点数 命令格式：incrbyfloat key num 🌰 1incrbyfloat cc1 0.2 给value减1 命令格式：decr key 🌰 1234decr cc1---每次调用均对value进行-1操作 给value减去某个数( num必须为整数 ) 命令格式：descby key num 🌰 1234decrby cc1 100---给cc1的值减去100 查看value的长度 命令格式：strlen key 🌰 1strlen cc1 目前先整理这些，都是一些基础的命令，随后再写一篇Java中使用Jedis操作字符串的随笔。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://luxiaowan.github.io/categories/Redis/"}],"tags":[]},{"title":"Redis数据在内存中是如何存储的","slug":"Redis数据在内存中是如何存储的","date":"2019-10-29T08:50:00.000Z","updated":"2019-10-29T15:02:17.269Z","comments":true,"path":"2019/10/29/Redis数据在内存中是如何存储的/","link":"","permalink":"http://luxiaowan.github.io/2019/10/29/Redis数据在内存中是如何存储的/","excerpt":"","text":"","categories":[{"name":"Redis","slug":"Redis","permalink":"http://luxiaowan.github.io/categories/Redis/"}],"tags":[]},{"title":"MySQL索引什么时候用hash","slug":"MySQL索引什么时候用hash","date":"2019-10-28T11:06:00.000Z","updated":"2019-10-29T14:10:04.447Z","comments":true,"path":"2019/10/28/MySQL索引什么时候用hash/","link":"","permalink":"http://luxiaowan.github.io/2019/10/28/MySQL索引什么时候用hash/","excerpt":"","text":"MySQL索引是在面试中常被问到的知识点，常用的两种索引方法有Hash和B+Tree，树的结构我们改天再扯，今天说收Hash。 为什么使用hash Hash索引可以根据数据的hash值直接定位到索引数据的存储位置，就相当于我知道了数组的下标，然后根据下标去取数据，这个效率可以说是最高的了，使用hash就是为了如此。 支持hash的存储引擎 目前支持hash的引擎有MEMORY(这里需要谷歌)，其他的引擎都通过各自的方式去支持hash方法。如InnoDB有一套自适应hash算法，内部实现还是采用了BT的方式，可以理解为BT索引的索引 InnoDB中hash索引支持的开启/关闭 hash索引虽然非常快速，但是在InnoDB中确实支持的不是很好，并且索引的具体创建是由引擎决定的(创建后存在于内存中)，非DBA可控，所以一般情况下建议关闭hash支持，使用BT也能够满足性能要求。 ​ set global innodb_adaptive_hash_index=off/on hash的使用场景 hash使用场景比较局限 hash索引仅适用于‘=’、‘&lt;=&gt;’和‘in’操作，所以hash仅仅适用于精确查找。 不适用于查询排序，因为hash后的数据并不会像原数据一样保持有序。 不适用于模糊查询，也就是不能使用like关键字。 既然不支持排序，也肯定不支持范围查询咯 解决hash冲突 不论hash的算法多么精确，当数据量大的时候都有可能发生hash碰撞，解决hash碰撞的方法有很多，比如再hash、链表叠加等，MySQL采用的是链表叠加的方式，也就是类似于HashMap解决hash碰撞的方法。所以在发生hash碰撞过多的情况下，使用hash索引会影响查询性能。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/categories/MySQL/"}],"tags":[]},{"title":"Spring-Session和Redis实现Session共享","slug":"Spring-Session和Redis实现Session共享","date":"2019-10-24T11:30:00.000Z","updated":"2019-10-24T15:18:45.220Z","comments":true,"path":"2019/10/24/Spring-Session和Redis实现Session共享/","link":"","permalink":"http://luxiaowan.github.io/2019/10/24/Spring-Session和Redis实现Session共享/","excerpt":"","text":"需求 现在大部分服务都以集群负载均衡的方式部署，几乎很难再遇到单点部署的项目，因为大家都要保证最基本的HA，说到HA，第一要考虑的就是各系统之间的Session共享的问题，如何解决呢？负载均衡当前使用Nginx 分析 不同的POD之间如果需要达到数据共享的目的，那么则需要使用同一个存储媒介，一开始想到使用MySQL来存储登录session，但是每次请求都去MySQL中查询数据，开销还是非常大的；然后最近使用MongoDB比较嗨，想着用MongoDB，但是MongoDB查询起来也不方便，况且我们这个Session也不是量级很大的数据集，最终采用了内存级的Redis来解决这个问题。 使用Redis的基本操作是将jsessionId为key，用户信息为value，使用jedis或者redisTemplate来操作Redis的读写行为。但是这种方式侵入了业务代码，并不是最优解，查了部分资料之后，发现spring-session.jar包中有一个非常有特色的注解@EnableRedisHttpSession，可以不需要侵入业务代码就能使用redis实现session共享的问题。 我们看一下@EnableRedisHttpSession的源码是怎么说的： 将此注释添加到一个单独的类上，该类必须加上@Configuration注解。使用方式在注释里也给出了demo代码。 实现 修改pom.xml文件，引入我们需要的jar包 123456789101112&lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 修改application.yml文件，配置redis信息 1234spring.redis.database=0spring.redis.host=localhostspring.redis.port=6379spring.redis.password=123456 创建配置类 1234567import org.springframework.context.annotation.Configuration;import org.springframework.session.data.redis.config.annotation.web.http.EnableRedisHttpSession;@Configuration@EnableRedisHttpSessionpublic class RedisHttpSessionConfig &#123;&#125; 到此，我们使用Redis实现Session共享的所有配置和代码都已经写完了，可以看到我们没有侵入到任何业务代码中，从头到尾也很简单。 扩展 1234@EnableGemFireHttpSession@EnableSpringHttpSession@EnableMongoHttpSession@EnableJdbcHttpSession","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://luxiaowan.github.io/categories/SpringBoot/"}],"tags":[]},{"title":"Java简单操作MongoDB","slug":"Java简单操作MongoDB","date":"2019-10-24T02:16:00.000Z","updated":"2019-10-24T13:20:12.762Z","comments":true,"path":"2019/10/24/Java简单操作MongoDB/","link":"","permalink":"http://luxiaowan.github.io/2019/10/24/Java简单操作MongoDB/","excerpt":"","text":"前面已经掌握了mongo最基本的一些命令，对各个命令也都实操过，理解各命令的意思，也对mongo有了最基本的理解，但大部分猿还是想使用Java去连接mongo，串串也不例外 在pom.xml中加入mongodb-java-driver.jar的依赖 Maven项目依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt; &lt;version&gt;3.0.4&lt;/version&gt;&lt;/dependency&gt; SpringBoot项目依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-mongodb&lt;/artifactId&gt;&lt;/dependency&gt; 连接mongo 认证连接 12345678// 创建验证信息，根据加密方式选择MongoCredential内对应的加密方式List&lt;MongoCredential&gt; credentials = new ArrayList&lt;&gt;();MongoCredential credential = MongoCredential.createCredential(\"admin\", \"test1\", \"admin\".toCharArray());credentials.add(credential);// 创建mongo服务地址ServerAddress serverAddress = new ServerAddress(\"localhost\", 27017);MongoClient mongoClient = new MongoClient(serverAddress, credentials); 无需认证连接 1MongoClient client = new MongoClient(\"localhost\", 27017); 然后就可以通过MongoClient的实例方法对mongo进行相关操作了 创建数据库连接 1MongoDatabase db = client.getDatabase(\"test1\"); mongo特性是不管数据库事先是否存在，都可以正常创建数据库连接，不会像MySQL一样报错，连接成功后，在mongo服务器上执行show dbs，会发现仍然查不到我们连接的这个数据库，这是正常情况，只有在数据库中有数据的时候，才会查得出来，client.getDatabase(&quot;test&quot;) === use test命令 创建集合 使用数据库连接实例方法创建一个空的集合 1234db.createCollection(\"base_info\");--- 命令：db.createCollection(\"base_info\") 直接向创建的集合中插入数据 12345MongoCollection&lt;Document&gt; collection = db.getCollection(\"base_info\");collection.insertOne(new Document(\"name\", \"lxl\"));--- 命令：db.base_info.insert(&#123;\"name\": \"lxl\"&#125;) 以上两种方式均可创建一个集合，区别在于第一种方式创建的是空集合， 删除集合 1234collection.drop();--- 命令：db.base_info.drop() 对集合的CRUD 新增数据 单条新增 12Document document = new Document().append(\"name\",\"cc\").append(\"age\",30).append(\"location\",\"SZ\");collection.insertOne(document); 批量新增 123456List&lt;Document&gt; documentList = new ArrayList&lt;&gt;();for (int i = 0; i &lt; 1000; i++) &#123; Document doc = new Document().append(\"name\", \"cc\" + i).append(\"age\", new Random().nextInt(100)).append(\"location\", \"SZ\"); documentList.add(doc);&#125;collection.insertMany(documentList); insertOne(Document)方法每次插入一条数据 insertMany(List)方法批量插入数据，并且可以通过参数InsertManyOptions设置是否排序 删除数据 删除第一条匹配的数据 12345Bson condition = Filters.eq(\"age\", 99);collection.deleteOne(condition);--- 命令：db.base_info.deleteOne(&#123;\"age\": 99&#125;) 删除所有匹配数据 12345Bson condition = Filters.eq(\"age\", 99);collection.deleteMany(condition);--- 命令：db.base_info.deleteMany(&#123;\"ag\": 99&#125;) 查询数据 查询返回第一条匹配的数据 1234Bson condition = Filters.eq(\"age\", 16);FindIterable&lt;Document&gt; vals = collection.find(condition);Document document = vals.first();System.out.println(document.toJson()); 通过调用FindIterable的实例方法first()取第一条数据 查询返回所有匹配数据 12345Bson condition = Filters.eq(\"age\", 16);FindIterable&lt;Document&gt; vals = collection.find(condition);for (Document val : vals) &#123; System.out.println(val.toJson());&#125; find()方法返回所有匹配数据 分页查询 分页查询是我们日常开发中经常用到的功能，尤其是mongo这种量级较大的存储，分页使用limit()和skip()两个方法来实现，limit指定查询的条数，skip进行分页，参数为从第几条开始，需要使用当前页码和分页条数进行计算(pageNo - 1) * pageSize 12345Bson condition = Filters.lt(\"age\", 2);FindIterable&lt;Document&gt; vals = collection.find(condition).limit(10).skip(10).sort(Sorts.descending(\"age\"));for (Document val : vals) &#123; System.out.println(val.toJson());&#125; limit(10): 每页取10条数据 skip(10): 从第11条开始查询，起始位置为0 sort(Sorts.descending(“age”)): 以列age倒序 对应MySQL：select * from base_info where age &lt; 2 order by age desc limit 7, 7 更新数据 更新第一条匹配数据中的某些字段 123Bson condition = Filters.eq(\"age\", 15);Document document = new Document(\"$set\", new Document(\"location\", \"XZ\"));collection.updateOne(condition, document); 注意这里有一个$set，这个指令是必须的，相对应的指令还有$inc 替换第一条匹配数据全部内容 123Bson condition = Filters.eq(\"age\", 15);Document document = new Document(\"location\", \"XZ\");collection.updateOne(condition, document); 没有$set指令,则表示使用参数document替换掉第一条匹配到的数据 第一条匹配的数据中指定字段数量+1 123Bson condition = Filters.eq(\"location\", \"XZ\");Document document = new Document(\"$inc\", new Document(\"age\", 1));collection.updateOne(condition, document); 更新所有匹配数据 123Bson condition = Filters.eq(\"age\", 15);Document document = new Document(\"$set\", new Document(\"location\", \"XZ\"));collection.updateMany(condition, document); 全部代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import org.bson.Document;import org.bson.conversions.Bson;import com.mongodb.MongoClient;import com.mongodb.client.FindIterable;import com.mongodb.client.MongoCollection;import com.mongodb.client.MongoDatabase;import com.mongodb.client.model.Filters;import com.mongodb.client.model.Sorts;/** * @author: cc */public class MongoConnectTest &#123; public static void main(String[] args) &#123; MongoClient client = new MongoClient(\"localhost\", 27017); try &#123; /* // 创建验证信息，根据加密方式选择MongoCredential内对应的加密方式 List&lt;MongoCredential&gt; credentials = new ArrayList&lt;&gt;(); MongoCredential credential = MongoCredential.createCredential(\"admin\", \"test\", \"\".toCharArray()); credentials.add(credential); // 创建mongo服务地址 ServerAddress serverAddress = new ServerAddress(\"localhost\", 27017); MongoClient mongoClient = new MongoClient(serverAddress, credentials);*/ MongoDatabase db = client.getDatabase(\"test1\"); // 创建集合 // 方式1 // db.createCollection(\"base_info2\"); // 方式2 MongoCollection&lt;Document&gt; collection = db.getCollection(\"base_info\"); // collection.insertOne(new Document(\"name\", \"lxl\")); // 删除集合 // collection.drop(); // 新增数据 // 单条新增 /*Document document = new Document().append(\"name\", \"cc\").append(\"age\", 30).append(\"location\", \"SZ\"); collection.insertOne(document); // 批量新增 List&lt;Document&gt; documentList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 1000; i++) &#123; Document doc = new Document().append(\"name\", \"cc\" + i).append(\"age\", new Random().nextInt(100)) .append(\"location\", \"SZ\"); documentList.add(doc); &#125; collection.insertMany(documentList);*/ // 删除数据 // 删除第一条匹配数据 /*Bson condition = Filters.eq(\"age\", 99); collection.deleteOne(condition); // 删除所有匹配数据 collection.deleteMany(condition);*/ // 修改数据 // 修改第一条匹配数据 /*Bson condition = Filters.eq(\"age\", 15); Document document = new Document(\"$set\", new Document(\"location\", \"XZ\")); collection.updateOne(condition, document); collection.updateMany(condition, document);*/ // 年龄+1 /*Bson condition = Filters.eq(\"location\", \"XZ\"); Document document = new Document(\"$inc\", new Document(\"age\", 1)); collection.updateOne(condition, document);*/ // 查询返回第一条匹配数据 Bson condition = Filters.lt(\"age\", 2); FindIterable&lt;Document&gt; vals = collection.find(condition).sort(Sorts.descending(\"age\")).limit(7).skip(7); // Document document = vals.first(); // System.out.println(document.toJson()); for (Document val : vals) &#123; System.out.println(val.toJson()); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://luxiaowan.github.io/categories/MongoDB/"}],"tags":[{"name":"Java与MongoDB","slug":"Java与MongoDB","permalink":"http://luxiaowan.github.io/tags/Java与MongoDB/"}]},{"title":"MongoDB监控、分片及备份恢复","slug":"MongoDB监控、分片及备份恢复","date":"2019-10-23T16:00:00.000Z","updated":"2019-10-24T13:20:12.764Z","comments":true,"path":"2019/10/24/MongoDB监控、分片及备份恢复/","link":"","permalink":"http://luxiaowan.github.io/2019/10/24/MongoDB监控、分片及备份恢复/","excerpt":"","text":"","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://luxiaowan.github.io/categories/MongoDB/"}],"tags":[{"name":"监控","slug":"监控","permalink":"http://luxiaowan.github.io/tags/监控/"}]},{"title":"MongoDB基础应用","slug":"MongoDB基础应用","date":"2019-10-22T16:00:00.000Z","updated":"2019-10-24T13:20:12.764Z","comments":true,"path":"2019/10/23/MongoDB基础应用/","link":"","permalink":"http://luxiaowan.github.io/2019/10/23/MongoDB基础应用/","excerpt":"","text":"索引 说明：索引是为了加快查询速度，可以对集合中的一列或多列设置索引。 – 无索引：扫描整个集合的文档，查找符合条件的文档 – 有索引：查找索引，根据索引取出文档数据 创建索引 db.&lt;collection_name&gt;.createIndex(keys, option) 栗子： 123456789101. 单索引db.user.createIndex(&#123;age:-1&#125;)---age:索引列名-1:倒序索引1:正序索引2. 多索引(复合索引)db.user.createIndex(&#123;age:1, gender:-1&#125;) 可选参数 参数 类型 说明 background Boolean 指定创建索引时是否阻塞集合的其他操作。true:后台执行，不阻塞；false:阻塞，默认 unique Boolean 指定索引是否为唯一索引。true:唯一索引；false:不唯一，默认 name String 索引名称，默认为字段名+索引顺序 v indexversion 索引版本号，默认为当前mongo的版本号 weights Integer 1~99999之间，值越大权重越大 expireAfterSeconds Integer 指定集合生存时间。秒级，TTL sparse Boolean 指定是否忽略不存在的字段。true:不查出不包含查询字段的文档；false:查询所有文档，默认 栗子： 12345&gt; db.user.createIndex(&#123;name:1&#125;, &#123;background: true, unique: true, name: \"idx_user_name\", v: 1, weights: 99, sparse: true&#125;)&gt; &gt; ---&gt; 后台不阻塞集合的方式创建一个name列正序,版本号为1,权重99,忽略无name字段的文档的唯一索引idx_user_name&gt; 查看所有索引 db.&lt;collection_name&gt;.getIndexes() 栗子： db.user.getIndexes() 重建索引 方法1：db.&lt;collection_name&gt;.reIndex() 方法2：先删除原索引，然后再创建 12db.&lt;collection_name&gt;.dropIndex(&lt;idx_name&gt;)db.&lt;collection_name&gt;.createIndex(...) 删除索引 删除指定名称的索引： db.user.dropIndex(&lt;idx_name&gt;) 删除集合中所有索引 db.user.dropIndexes() 说明：只会删除自建的索引，集合中_id列的索引不会被删除 聚合查询 12345678910111213141516171819202122232425创建集合并插入数据：db.agg.save([&#123; title: 'MongoDB Overview', description: 'MongoDB is no sql database', by_user: 'runoob.com', url: 'http://www.runoob.com', tags: ['mongodb', 'database', 'NoSQL'], likes: 100&#125;,&#123; title: 'NoSQL Overview', description: 'No sql database is very fast', by_user: 'runoob.com', url: 'http://www.runoob.com', tags: ['mongodb', 'database', 'NoSQL'], likes: 10&#125;,&#123; title: 'Neo4j Overview', description: 'Neo4j is no sql database', by_user: 'Neo4j', url: 'http://www.neo4j.com', tags: ['neo4j', 'database', 'NoSQL'], likes: 750&#125;]) 格式： 12345678910&gt; db.&lt;collection_name&gt;.aggregate(condition)&gt; ---&gt; condition:&gt; [&#123;$group:&#123;_id:\"$&lt;key&gt;\", num_tutorial:&#123;$&lt;fun_expression&gt;:\"$&lt;key&gt;\"&#125;&#125;&#125;]&gt; $group:一个组&gt; _id:组合列，类同于MySQL的group by后面的字段，默认_id的列会在查询结果中显示&gt; num_tutorial:输出的列名&gt; $&lt;fun_expression&gt;:聚合表达式&gt; $&lt;key&gt;:运算的列名&gt; 关键字：aggregate 说明：聚合查询就是求和、最大、最小、最前、最后、平均数的统称，类似于MySQL的count()、sum()、avg() 栗子： 12345&gt; db.user.aggregate([&#123;$group: $&#123;_id:\"$gender\", num_tutorial:&#123;$sum:1&#125;&#125;&#125;])&gt; &gt; ---&gt; 等同于MySQL：select gender, count(1) from user group by gender&gt; 聚合表达式 表达式 描述 案例 $sum 计算总和 db.agg.aggregate([{$group:{_id:&quot;$by_user&quot;, num_tutorial:{$sum:&quot;$likes&quot;}}}])—等同于MySQL：select by_user, sum(likes) from agg group by by_user $avg 计算平均值 db.agg.aggregate([{$group:{_id:&quot;$by_user&quot;, num_tutorial:{$avg:&quot;$likes&quot;}}}])—等同于MySQL：select by_user, avg(likes) from agg group by by_user $min 获取集合中指定列的最小值记录 db.agg.aggregate([{$group:{_id:&quot;$by_user&quot;, num_tutorial:{$min:&quot;$like&quot;}}}])—等同于MySQL：select by_user, min(likes) from agg group by by_user $max 获取集合中指定列的最大值记录 db.agg.aggregate([{$group:{_id:&quot;$by_user&quot;, num_tutorial:{$max:&quot;$likes&quot;}}}])—等同于MySQL：select by_user, max(likes) from agg group by by_user $push 在结果文档中插入值到数组 db.agg.aggregate([{$group:{_id:&quot;$by_user&quot;, num_tutorial:{$push:&quot;$url&quot;}}}])—等同于MySQL：查出所有数据，然后取字段url写入同一个列表中，url不去重，然后输出 $addToSet 在结果文档中插入值到一个数组中，但不创建副本 db.agg.aggregate([{$group:{_id:&quot;$by_user&quot;, num_tutorial:{$addToSet:&quot;$url&quot;}}}])—等同于MySQL：查出所有数据，然后取字段url写入同一个列表中，url去重，然后输出 $first 根据资源文档的排序获取第一个文档数据 db.agg.aggregate([{$group:{_id:&quot;$by_user&quot;, num_tutorial:{$first:&quot;$title&quot;}}}]) $last 根据资源文档的排序获取最后一个文档数据 db.agg.aggregate([{$group:{_id:&quot;$by_user&quot;, num_tutorial:{$last:&quot;$title&quot;}}}]) 管道函数 说明：管道函数类似于Linux系统中的管道操作，将上一步的运算结果作为下一步的输入值，最终达到理想计算结果的运算方式 函数 说明 $project 指定需要输出的列，默认显示_id，格式：{$project:{by_user:1, title:1, url:1, _id:0不显示id}}, $limit 限制查询返回的文档数，格式：{$limit: 1},只返回一个文档 $skip 跳过指定数量的文档，返回之后的所有文档，格式：{$skip: 1},从第二个文档开始输出 $match 条件筛选，格式：{match: {likes: {gte: 10}}} $group 聚合条件，格式：{group: {_id: &quot;by_user&quot;, count: {$sum: -1}}} $sort 排序，格式：{$sort: {likes: -1}}，-1:倒序; 1:正序 $unwind 将文档中的某一个数组类型字段拆分成多条，每条包含数组中的一个值 $geoNear 输出接近某一地理位置的有序文档。 栗子： 123456&gt; db.agg.aggregate(&#123;$match: &#123;likes: &#123;$gte: 10&#125;&#125;&#125;, &#123; $project: &#123;_id: 0, title: 1, by_user: 1, likes: 1&#125;&#125;, &#123;$limit: 5&#125;, &#123;$skip: 1&#125;, &#123;$sort: &#123;likes: -1&#125;&#125;)&gt; &gt; ---&gt; 等同于MySQL: &gt; select title, by_user, likes from agg where likes &gt;= 10 order by likes desc limit 1,5&gt;","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://luxiaowan.github.io/categories/MongoDB/"}],"tags":[{"name":"基础应用","slug":"基础应用","permalink":"http://luxiaowan.github.io/tags/基础应用/"}]},{"title":"MongoDB基本指令","slug":"MongoDB基本指令","date":"2019-10-22T16:00:00.000Z","updated":"2019-10-24T13:20:12.763Z","comments":true,"path":"2019/10/23/MongoDB基本指令/","link":"","permalink":"http://luxiaowan.github.io/2019/10/23/MongoDB基本指令/","excerpt":"","text":"查看所有的db show dbs 切换db user &lt;db_name&gt; 查看当前所在db名称 db 删除db**(必须在要删除的db中操作)** db.dropDatabase() 查看db下所有的集合 show tables show collections 创建集合 db.createCollection(&quot;abc&quot;) db.createCollection(&quot;def&quot;, {capped: true, autoIndexId: true, size: 1024, max: 100}) 往一张不存在的集合中插入一条数据，会自动创建集合 db.test.insert({title: 123}) 删除集合 db.&lt;collection_name&gt;.drop() 例：db.abc.drop() 插入文档 db.&lt;collection_name&gt;.insert({title: 1234}) doc=({title: 12345, name: &quot;MongoDB指南&quot;}) db.&lt;collection_name&gt;.insert(doc) db.&lt;collection_name&gt;.save({name: &quot;MongoDB简单指令&quot;}}) doc2=({name: &quot;MongoDB从入门到放弃&quot;}) db.&lt;collection_name&gt;.save(doc2) doc3=({_id: &quot;edrftgyhjkgjhfgv2ryuoio&quot;, name: &quot;MongoDB从入门到放弃&quot;}) db.&lt;collection_name&gt;.save(doc3)// 若_id对应值的数据已经存在，则更新这条数据，否则新增一条数据 更新文档 db.&lt;collection_name&gt;.update({title: 1234}, {$set:{title: &quot;4321&quot;}}) 格式： 123456789db.&lt;collection_name&gt;.update( &lt;where&gt;,// 相当于MySQL的where &lt;update&gt;,// 相当于MySQL的update语句的set，需要跟一些指令：$,$inc,$set &#123; upsert: true,// true：如果不存在记录，则新增；false相反，默认 multi: true,// true：只更新第一条匹配的记录；false相反，全部更新， 默认 writeConcern: &lt;document&gt;// 异常级别 &#125;) 案例 12345678910111213&gt; 1. 只更新第一条记录：&gt; db.col.update( &#123; \"count\" : &#123; $gt : 1 &#125; &#125; , &#123; $set : &#123; \"test2\" : \"OK\"&#125; &#125; );&gt; 2. 全部更新：&gt; db.col.update( &#123; \"count\" : &#123; $gt : 3 &#125; &#125; , &#123; $set : &#123; \"test2\" : \"OK\"&#125; &#125;,false,true );&gt; 3. 只添加第一条：&gt; db.col.update( &#123; \"count\" : &#123; $gt : 4 &#125; &#125; , &#123; $set : &#123; \"test5\" : \"OK\"&#125; &#125;,true,false );&gt; 4. 全部添加进去:&gt; db.col.update( &#123; \"count\" : &#123; $gt : 5 &#125; &#125; , &#123; $set : &#123; \"test5\" : \"OK\"&#125; &#125;,true,true );&gt; 5. 全部更新：&gt; db.col.update( &#123; \"count\" : &#123; $gt : 15 &#125; &#125; , &#123; $inc : &#123; \"count\" : 1&#125; &#125;,false,true );&gt; 6. 只更新第一条记录：&gt; db.col.update( &#123; \"count\" : &#123; $gt : 10 &#125; &#125; , &#123; $inc : &#123; \"count\" : 1&#125; &#125;,false,false );&gt; db.&lt;collection_name&gt;.save() 调用save指令一般需要指定_id 删除文档 db.&lt;collection_name&gt;.remove({title: &quot;4321&quot;}, true) 格式 1234db.&lt;collection_name&gt;.remove( &lt;where&gt;,// 相当于MySQL的where &lt;justOne&gt;// true：只删除一条匹配条件的数据；false：匹配条件的数据全部删除，默认) 删除集合中所有数据 db.&lt;collection_name&gt;.remove({}) 新函数： db.&lt;collection_name&gt;.deleteMany({}) db.&lt;collection_name&gt;.deleteOne({title: &quot;12345&quot;}) 查询文档 普通查询 格式： 12345&gt; db.&lt;collection_name&gt;.find(&gt; &lt;where&gt;, // 查询条件&gt; &lt;colName&gt;// 返回字段名称&gt; )&gt; 创建一个集合，插入三条数据 1db.user.insert([&#123;name: \"cc\", age: \"29\", gender: 1&#125;, &#123;name: \"ccc\", age: \"30\", gender: 2&#125;, &#123;name: \"c\", age: \"28\", gender: 1&#125;]) 查询集合中全部数据 普通显示：db.user.find() 格式化显示：db.user.find().pretty() 查询name=&quot;c&quot;的信息 db.user.find({name: &quot;c&quot;}).pretty() 查询只返回第一个匹配到的数据 db.user.findOne({name: &quot;c&quot;}) AND查询 db.user.find({key: value, key: value}) And查询即是在where条件里面用逗号&quot;,&quot;分隔 栗子： 123456&gt; db.user.find(&#123;name:\"cc\", gender:1&#125;)&gt; &gt; ---&gt; 等同于MySQL：&gt; select * from user where name = \"c\" AND gender = 1&gt; OR查询 db.user.find({$or:[{key:value}, {key:value}]}) 栗子： 12345678&gt; db.user.find(&#123;&gt; $or:[&#123;name:\"c\"&#125;, &#123;gender:2&#125;]&gt; &#125;)&gt; &gt; ---&gt; 等同于MySQL：&gt; select * from user where name = \"c\" OR gender = 2&gt; AND和OR组合查询 db.user.find({key:value, $or:[{key:value}, {key:value}]}) 栗子： 123456&gt; db.user.find(&#123;gender:1, $or:[&#123;name: \"c\"&#125;, &#123;age: \"28\"&#125;]&#125;)&gt; &gt; ---&gt; 等同于MySQL：&gt; select * from user where gender = 1 AND (name = \"c\" OR age = \"28\")&gt; 运算符 运算符 格式 案例 MySQL对应语句 等于 {key:value}{key:{$eq:value}} db.user.find({age:“29”})db.user.find({age:{$eq:“29”}}) where age = “29” 大于 {key:{$gt: value}} db.user.find({age:{$gt:“30”}}) where age &gt; “30” 小于 {key:{$lt: value}} db.user.find({age:{$lt: “30”}}) where age &lt; “30” 大于等于 {key:{$gte: value}} db.user.find({age:{$gte:“30”}}) where age &gt;= “30” 小于等于 {key:{$lte: value}} db.user.find({age:{$lte:“30”}}) where age &lt;= “30” 不等于 {key:{ne: value}} db.user.find({age:{$ne:“29”}}) where age != “30” 模糊查询 查询age包含0的：db.user.find({age:/0/}) 查询age以2开头的：db.user.find({age:/^2/}) 查询age以8结束的：db.user.find({age:/8$/}) 分页查询 格式： 123456&gt; db.&lt;collection_name&gt;.find().limit(Number).skip(Number)&gt; &gt; ---&gt; limit(Number)表示查询多少条数据&gt; skip(Number)表示从第几条开始查询&gt; 查询一条数据 第一种方法：db.user.findOne({}) 第二种方法：db.user.find({}).limit(1) 从第二条数据开始查询一条数据 db.user.find().limit(1).skip(2) 查询排序 格式： 1234567&gt; db.&lt;collection_name&gt;.find().sort(&#123;&lt;key_name&gt;:-1/1&#125;)&gt; &gt; ---&gt; &lt;key_name&gt;：排序字段&gt; -1：倒序&gt; 1：正序&gt; 按照年龄倒序 db.user.find().sort({age:-1}) 按照年龄倒序、性别正序 db.user.find().sort({age:-1, gender:1})","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://luxiaowan.github.io/categories/MongoDB/"}],"tags":[{"name":"基本指令","slug":"基本指令","permalink":"http://luxiaowan.github.io/tags/基本指令/"}]},{"title":"MongoDB连接报错java.lang.NoSuchFieldError ACKNOWLEDGED","slug":"MongoDB连接报错java.lang.NoSuchFieldError-ACKNOWLEDGED","date":"2019-10-22T16:00:00.000Z","updated":"2019-10-24T13:20:12.764Z","comments":true,"path":"2019/10/23/MongoDB连接报错java.lang.NoSuchFieldError-ACKNOWLEDGED/","link":"","permalink":"http://luxiaowan.github.io/2019/10/23/MongoDB连接报错java.lang.NoSuchFieldError-ACKNOWLEDGED/","excerpt":"","text":"BUG描述 使用SpringBoot整合MongoDB时，正要运行代码连接mongo，就赤红赤红的报了个错： 12345Exception in thread \"main\" java.lang.NoSuchFieldError: ACKNOWLEDGED at com.mongodb.MongoClientOptions$Builder.&lt;init&gt;(MongoClientOptions.java:960) at com.mongodb.MongoClient.&lt;init&gt;(MongoClient.java:155) at com.mongodb.MongoClient.&lt;init&gt;(MongoClient.java:145) at com.example.demo.mongo.MongoConnectTest.main(MongoConnectTest.java:15) 这一下就傻眼了，对于刚接触mongo的人来说，是很懵圈的，大脑知识库中没有这个异常信息的解决办法，只能谷歌了，没想到有那么多人遇到过这个问题 BUG解决 在Stack Overflow上找到一个帖子： https://stackoverflow.com/questions/13593614/mongodb-java-lang-nosuchfielderror 其中jyemin的回答可以说是直击要害了，顺利的解决了这个问题，我把截图贴上，以防帖子被删 其实就是在工程中引入了多个版本不同的mongo-java-driver，所以导致程序混乱，只要保留自己真正使用的那个版本，其他的都删除即可 我的配置： 将2.7.1版本的依赖删除就可以正常运行了","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://luxiaowan.github.io/categories/MongoDB/"}],"tags":[{"name":"MongoDB错误记录","slug":"MongoDB错误记录","permalink":"http://luxiaowan.github.io/tags/MongoDB错误记录/"}]},{"title":"Git Stash用法","slug":"git stash命令","date":"2019-10-21T16:00:00.000Z","updated":"2019-10-24T13:20:12.765Z","comments":true,"path":"2019/10/22/git stash命令/","link":"","permalink":"http://luxiaowan.github.io/2019/10/22/git stash命令/","excerpt":"","text":"创建仓库 git init echo 123 &gt; test.txt git add . git commit -m “add test file” git remote add origin git@git.xx.xx.xx:xxx/xxx.git git push origin master 以上内容比较简单，就不作详细讲述，如果上面的内容看不懂，下面的请放弃 使用git stash暂存 随意修改test.txt文件的内容，比如：画个心形，你正热火朝天修改中…… 这时领导走过来拍了拍你的肩膀说到：”小伙子啊，创建个朕.txt文件提交上来，内容就写我还想再活五百年，test.txt文件这一版不作修改“。 你面露笑容的回答：“好的，没问题！”，内心却是：“MMP,MMP,MMP……” 辛辛苦苦修改的test.txt文件怎么办？眼瞅着就要完成了，难道复制出去，然后等解决领导需求后再粘贴回来？low不low？肯定不low啊，一个文件而已，这种方法很简单，*但是当你实际项目中修改了几十个文件的时候呢？*挨个儿复制出去？累傻小子呐？！！！！！ git stash命令帮你解决问题，stash是存储的意思，也就是将当前工作区内的所有东西都存储起来，然后工作区所有文件恢复到修改之前的状态(并不是最新状态，不会和仓库中进行自动同步，需要你自己去pull)，然后你就可以继续完成任务了。(在执行git stash之前需要先执行git add命令) stash可以进行多次操作，每次操作都会将当前工作区的文件情况暂存起来，stash是类栈存储，每次stash的序号都为0，此次之前stash的序号会自动+1 使用git stash pop取出 git stash pop取出栈顶元素，也就是序号为0的那个，即最近一次执行git stash保存的内容。pop之后，暂存列表中就会自动将其清除掉，这个时候你再执行git stash list会发现毛都没有 这个时候可能就会纳闷了，我保存了好几次，但是我这次是想使用最开始stash的那份内容，怎么办？一直pop，直到最后一次？当然不行，上面刚讲过pop之后暂存列表中就没有stash的信息了，已经被pop出去的就找不回来了，等于是自杀式攻击，那咋整呢？ git stash apply stash@{序号}可以将指定序号的stash内容弹出到工作区，此时工作区里文件的状态就和stash@{序号}里的一致了，但是这个命令无法将stash记录从暂存列表中删除，仅仅只是将文件恢复而已 git stash drop stash@{序号}来丢弃暂存列表中的记录，可以配合apply使用 查看暂存记录中的信息 查看暂存列表：git stash list查看当前stash的列表 查看暂存内容：git stash show stash@{序号}查看指定序号的stash的内容 git stash save ‘msg’ 等于是在stash的时候打了个标签，妖娆！！！！","categories":[{"name":"Git","slug":"Git","permalink":"http://luxiaowan.github.io/categories/Git/"}],"tags":[]},{"title":"Char和varchar简单介绍","slug":"char与varchar简单介绍","date":"2019-10-12T04:10:43.000Z","updated":"2019-10-21T16:19:50.476Z","comments":true,"path":"2019/10/12/char与varchar简单介绍/","link":"","permalink":"http://luxiaowan.github.io/2019/10/12/char与varchar简单介绍/","excerpt":"","text":"1. 数据长度 1) char(最大长度255个字节) 长度固定（字段存入数据长度始终等于字段长度） 2) varchar(最大长度65535个字节) 可变长度，存入数据长度为N个字节，则实际使用了N+1(255以上长度则+2)个字节的空间，多出来的1字节是用来存储数据实际长度。 存入数据对比 存入&quot;ab&quot; char查询出&quot;ab&quot; varchar查询出&quot;ab&quot; 存入&quot;ab &quot;，ab后面有两个空格， char查询出&quot;ab&quot; varchar查询出&quot;ab &quot; 解析： ​ 在入库时，数据库会自动在ab后面添加两位空格，让入库的数据长度保证等于4 char(4) ，这种操作很容易理解，但是如果你去数据表里面去查这条数据，会发现表中数据并没有空格，所以可以推断出这里是逻辑追加，所以在查询数据的时候会并不会出现引擎自动添加的空格。 2. 实操(技术一定要**实操**) 1）无空格数据 先创建一张表 1create table cv(c char(4), v varchar(4)); 插入数据 1insert into cv values(&quot;ab&quot;, &quot;ab&quot;); 查询数据 1select concat(&apos;(&apos;, c, &apos;)&apos;) AS c, concat(&apos;(&apos;, v, &apos;)&apos;) AS v from cv; 结果 结果中char和varchar均查出来为无空格的ab ####2）有空格数据 插入数据 1insert into cv values(&quot;ab &quot;, &quot;ab &quot;); 查询数据 1select concat(&apos;(&apos;, c, &apos;)&apos;) AS c, concat(&apos;(&apos;, v, &apos;)&apos;) AS v from cv; 结果 结果中可以看出，char类型将数据后面的空格自动去掉了，varchar则保留了所有的空格","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/categories/MySQL/"}],"tags":[]},{"title":"MySQL使用查询缓存","slug":"MySQL使用查询缓存","date":"2019-10-11T05:01:23.000Z","updated":"2019-10-21T15:48:59.282Z","comments":true,"path":"2019/10/11/MySQL使用查询缓存/","link":"","permalink":"http://luxiaowan.github.io/2019/10/11/MySQL使用查询缓存/","excerpt":"","text":"查询服务是否已开启缓存 执行show variables like '%query_cache%';查看缓存状态 Variable_name: query_cache_type为缓存状态，ON表示开启，OFF表示关闭 开启/关闭使用查询缓存 修改my.cnf文件进行开启和关闭 [mysqld]中添加/修改： query_cache_size = 20M query_cache_type = ON/OFF **修改完成后重启MySQL服务：service mysql restart/systemctl mysql restart ** 查询缓存使用情况 执行show status like 'qcache%';查询缓存使用情况 因为本机MySQL未开启查询缓存，所以此处和使用相关的属性均为0 属性解释: 属性 释义 Qcache_free_blocks 缓存中相邻内存块的个数。数目大说明可能有碎片。FLUSH QUERY CACHE会对缓存中的碎片进行整理，从而得到一个空闲块。 Qcache_free_memory 缓存中空闲内存大小 Qcache_hits 缓存命中次数，命中一次就+1 Qcache_inserts 查询次数，命中次数/查询次数=缓存命中率 Qcache_lowmem_prunes 缓存出现内存不足并且必须要进行清理以便为更多查询提供空间的次数，如果数字不断增长，就可能碎片非常严重，或者内存很少，通过Qcache_free_blocks、Qcache_free_memory来分析具体情况 Qcache_not_cached 不适合进行缓存的查询的数量 Qcache_queries_in_cache 当前缓存的查询(和响应)的数量 Qcache_total_blocks 缓存中块的数量","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://luxiaowan.github.io/categories/MySQL/"}],"tags":[]},{"title":"Java基础面试题","slug":"蚂蚁Java基础面试题","date":"2019-10-10T15:21:56.000Z","updated":"2019-10-21T15:49:27.313Z","comments":true,"path":"2019/10/10/蚂蚁Java基础面试题/","link":"","permalink":"http://luxiaowan.github.io/2019/10/10/蚂蚁Java基础面试题/","excerpt":"","text":"一 map怎么实现hashcode和equals,为什么重写equals必须重写hashcode 使用过concurrent包下的哪些类，使用场景等等。 concurrentHashMap怎么实现？concurrenthashmap在1.8和1.7里面有什么区别 CountDownLatch、LinkedHashMap、AQS实现原理 线程池有哪些RejectedExecutionHandler,分别对应的使用场景 多线程的锁？怎么优化的？偏向锁、轻量级锁、重量级锁？ 组合索引？B+树如何存储的？ 为什么缓存更新策略是先更新数据库后删除缓存 OOM说一下？怎么排查？哪些会导致OOM? OSI七层结构，每层结构都是干什么的？ java的线程安全queue需要注意的点 死锁的原因，如何避免 二 jvm虚拟机老年代什么情况下会发生gc，给你一个场景，一台4核8G的服务器，每隔两个小时就要出现一次老年代gc，现在有日志，怎么分析是哪里出了问题 数据库索引有哪些？底层怎么实现的？数据库怎么优化？ 数据库的事务，四个性质说一下，分别有什么用，怎么实现的？ 服务器如何负载均衡，有哪些算法，哪个比较好，一致性哈希原理，怎么避免DDOS攻击请求打到少数机器 volatile讲讲 哪些设计模式？装饰器、代理讲讲？ redis集群会吗？ mysql存储引擎 事务隔离级别 不可重复度和幻读，怎么避免，底层怎么实现（行锁表锁） 三 项目介绍 分布式锁是怎么实现的 MySQL有哪几种join方式，底层原理是什么 Redis有哪些数据结构？底层的编码有哪些？有序链表采用了哪些不同的编码？ Redis扩容，失效key清理策略 Redis的持久化怎么做，aof和rdb，有什么区别，有什么优缺点。 MySQL数据库怎么实现分库分表，以及数据同步？ 单点登录如何是实现？ 谈谈SpringBoot和SpringCloud的理解 未来的技术职业怎么规划？ 为什么选择我们公司？","categories":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/categories/Java/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://luxiaowan.github.io/tags/面试/"},{"name":"阿里","slug":"阿里","permalink":"http://luxiaowan.github.io/tags/阿里/"}]},{"title":"Linux配置JDK环境","slug":"Linux配置JDK环境","date":"2019-01-15T04:12:00.000Z","updated":"2020-03-24T05:09:48.450Z","comments":true,"path":"2019/01/15/Linux配置JDK环境/","link":"","permalink":"http://luxiaowan.github.io/2019/01/15/Linux配置JDK环境/","excerpt":"","text":"1、下载jdk压缩包 https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 2、解压 tar -zxvf jdk-8u191-linux-x64.tar.gz /opt/jdk 3、设置环境变量 vim ~/.bashrc 内容： 12345export JAVA_HOME=/opt/jdkexport CLASSPATH=$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$PATH:$JAVA_HOME/bin 4、立即设置环境变量 source ~/.bashrc","categories":[{"name":"Linux","slug":"Linux","permalink":"http://luxiaowan.github.io/categories/Linux/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://luxiaowan.github.io/tags/Java/"}]},{"title":"Linux配置maven环境","slug":"Linux配置maven环境","date":"2019-01-15T03:00:00.000Z","updated":"2020-03-24T05:06:04.033Z","comments":true,"path":"2019/01/15/Linux配置maven环境/","link":"","permalink":"http://luxiaowan.github.io/2019/01/15/Linux配置maven环境/","excerpt":"","text":"0、先设置JDK环境 &lt; 设置 &gt; 1、下载Maven包 wget -c http://mirror.bit.edu.cn/apache/maven/maven-3/3.6.0/binaries/apache-maven-3.6.0-bin.tar.gz 2、解压 tar -zxvf apache-maven-3.6.0-bin.tar.gz /opt/ 3、设置环境变量 vim ~/.bashrc 内容： 123export M2_HOME=/opt/apache-maven-3.6.0export PATH=$PATH:$M2_HOME/bin 4、立即设置环境变量 source ~/.bashrc","categories":[{"name":"Linux","slug":"Linux","permalink":"http://luxiaowan.github.io/categories/Linux/"}],"tags":[{"name":"Maven","slug":"Maven","permalink":"http://luxiaowan.github.io/tags/Maven/"}]}]}